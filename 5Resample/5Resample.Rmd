---
subtitle: "TMA4268 Statistical Learning V2022"
title: "Module 5: Resampling"
author: "Stefanie Muff, Department of Mathematical Sciences, NTNU"
date: "February 7 and 10, 2022"
fontsize: 10pt
output:
  beamer_presentation:
    keep_tex: yes
    fig_caption: false
    latex_engine: xelatex
    theme: "Singapore"
    colortheme: "default"
    font: "serif"
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
urlcolor: blue
bibliography: refs.bib
header-includes: \usepackage{multirow}

---

```{r setup, include=FALSE}
showsolA<-TRUE
showsolB<-TRUE
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize")
whichformat="latex"
```


---

# Acknowledgements

* A lot of this material stems from Mette Langaas and her TAs. I would like to thank Mette for the permission to use her material!

* Some of the figures and slides in this presentation are taken (or are inspired) from @james.etal.


# Introduction

## Learning material for this module

\vspace{2mm}

* James et al (2013): An Introduction to Statistical Learning, Chapter 5.  

* All the material presented on these module slides.

\vspace{2mm}
Additional material for the interested reader: Chapter 7 (in particular 7.10) in Friedman et al (2001): Elements of Statistical learning.

---

## What will you learn?

$~$

* What is model assessment and model selection?

\vspace{2mm}

* Ideal solution in a data rich situation.

\vspace{2mm}

* Cross-validation and what is best: 
    + validation set 
    + leave-one-out cross-validation (LOOCV) 
    + $k$-fold CV 

\vspace{2mm}

* Bootstrapping - how and why.


---

# Performance of a learning method

* Our models are "good" when they can generalize.

* We want a learning method to perform well on new data (low test error). 

* Inference and understanding of the true pattern (in contrast to overfitting)

\vspace{2mm}

This is important both for 

**Model selection**

Estimate the _performance_ of different models to _choose the best model_.

**Model assessment**

Estimating the performance (prediction error) of the final model, on new data.

---


## Training vs Test Error

$~$

\centering
![Training vs test error](training_test.png){width=80%}


---

## Loss functions 

$~$

In order to define how we measure error, we must first decide for a **loss function**. Here we use:

\vspace{2mm}

* _\textcolor{red}{Mean squared error}_ (quadratic loss) for regression problems (continuous outcomes) $Y_i=f({\boldsymbol x}_i)+\varepsilon_i$, $i=1,\ldots, n$:  

$$\text{MSE}=\frac{1}{n}\sum_{i=1}^n (y_i-\hat{f}({\boldsymbol x}_i))^2 \ .$$

\vspace{2mm}

* _\textcolor{red}{Misclassification rate}_ (0/1 loss) for classification problems where we classify to the class with the highest probability $P(Y=j\mid {\boldsymbol x}_0)$ for $j=1,\ldots,K$:
$$\frac{1}{n}\sum_{i=1}^n \text{I}(y_i \neq \hat{y}_i) \ .$$ 

---

### KNN regression (chapter 3.5 in course book)

\vspace{2mm}

* The KNN regression method provides a prediction at a value $x_0$ by finding the closest $K$ points (Euclidean distance) and calculating the average of the observed $y$ values at the points in the respective neighborhood $\mathcal{N}_0$ 

$$\hat{f}(x_0)=\frac{1}{K}\sum_{i\in \mathcal{N}_0} y_i \ .$$


$~$

Illustration: Linear regression with $K=1$ (left) and $K=9$ (right).

\centering
![](../..//ISLR/Figures/Chapter3/3.17.png){width=55%}

\tiny
(Figure 3.17 from @james.etal).

$~$

\normalsize
\flushleft

What happens for $K=$ number of data points?

---


### Example 

\vspace{2mm}

We aim to do _model selection_ in KNN-regression, where true curve is
$f(x)=-x+x^2+x^3$ with $x \in [-3,3]$. $n=61$ for the training data.

\vspace{4mm}

\centering
![Regression problem: training data and true regression curve](Prob1f1.png){width=50%}



* We have considered $K=1,\ldots,25$, and repeated the experiment $M=1000$ times (that is, $M$ versions of training and test set). 

 


---

### Remember: The bias-variance trade-off
\vspace{2mm}


For KNN: $K$ small = high complexity; $K$ large = low complexity. 

\vspace{2mm}

<!-- <figure><img src="Probl1fig4negK.png" height="250px" width="250px" align="right" /><figcaption>Regression problem: bias-variance traceoff</figure> -->
\centering
![Regression problem: bias-variance traceoff](Prob1f4.png){width=75%}





---

### The challenge

$~$

* In the above examples we knew the truth, so we could assess training and test error.

\vspace{2mm}

* In reality this is of course not the case.

\vspace{2mm}

* We need approaches that work with real data!

---

## The data-rich situation (often unrealistic)

\vspace{2mm}
If we had a large amount of data we could divide our data into three parts:

* **Training set**: to fit the model
* **Validation set**: to select the best model (_model selection_)
* **Test set**: to assess how well the model fits on new independent data (_model assessment_)


\vspace{2mm}

**Q**: Before we had just training and test. Why do we need the additional validation set?

**A**: We have not discussed model selection before.

\vspace{2mm}

**Q**: Why can't we just use the training set for training, and then the test set both for model selection and for model evaluation?

**A**: We will be too optimistic if we report the error on the test set when we have already used the test set to choose the best model.


---

* If you have a lot of data -- great --  then you do not need Module 5. 

* But, this is very seldom the case -- so we will study other solutions based on efficient sample reuse with _resampling_ data.

* An alternative strategy for model selection (using methods penalizing model complexity, e.g. AIC or lasso) is covered in Module 6.

We will look at _cross-validation_ and the _bootstrap_.

---

# Cross-validation (CV)

$~$

"Model selection" situation: We assume that test data is available (and has been put aside), and we want to use the rest of our data to find the model that performs "best", that is, _with lowest test error_.

\vspace{2mm}

This can be done by:

* the validation set approach (not strictly a _cross_-validation approach).
* leave one out cross-validation (LOOCV).
* $k$-fold cross-validation (CV), typically $k=5$ or $10$.




---

## The validation set approach  

\vspace{2mm}

* Consider the case when you have a data set consisting of $n$ observations. 

* To fit a model and to evaluate its predictive performance you randomly divide the data set into two parts ($n/2$ sample size each): 

    + a _training set_ (to fit the model) and 
    + a _validation set_ (to make predictions of the response variable for the observations in the validation set)

\vspace{4mm}

![ISLR Figure 5.1](../../ISLR/Figures/Chapter5/5.1.png){width=70%,centering}




---

### Example of validation set approach
\vspace{2mm}

Auto data set (library `ISLR`): predict `mpg` (miles pr gallon) using polynomial function of `horsepower` (of engine), $n=392$. What do you see?
 

```{r chunkName, echo=FALSE, fig.width=4, fig.height=3,fig.align = "center",out.width='70%'}
library(ISLR)
library(ggplot2)
library(ggpubr)
library(reshape2)
set.seed(123)
n=dim(Auto)[1]

testMSEmat=matrix(ncol=11,nrow=10)
for (newsample in 1:11)
{
  trainid=sample(n,n/2)
  for (polydeg in 1:10)
  {
    lm.fit=lm(mpg~poly(horsepower,polydeg),data=Auto,subset=trainid)
    lm.pred=predict(lm.fit,Auto)[-trainid]
    testMSEmat[polydeg,newsample]=mean((Auto$mpg[-trainid]-lm.pred)^2)
  }
}
yrange=c(15,28)

plotdf=data.frame("testMSE"=testMSEmat[,1],"degree"=1:10)
g0=ggplot(plotdf,aes(x=degree))+geom_line(y=testMSEmat[,1])+scale_y_continuous(limits = yrange)+scale_x_continuous(breaks=1:10)+labs(y="Validation set", x="Degree of polynomial")
g0+theme_minimal()
```

---


But what if we select another split into two parts? Many splits:

```{r auto2, echo=FALSE, fig.width=4, fig.height=3,fig.align = "center",out.width='70%'}
cols=rainbow(10)
g1=g0+geom_line(aes(x=1:10,y=testMSEmat[,2]),colour=cols[1])
g1=g1+geom_line(aes(x=1:10,y=testMSEmat[,3]),colour=cols[2])
g1=g1+geom_line(aes(x=1:10,y=testMSEmat[,4]),colour=cols[3])
g1=g1+geom_line(aes(x=1:10,y=testMSEmat[,5]),colour=cols[4])
g1=g1+geom_line(aes(x=1:10,y=testMSEmat[,6]),colour=cols[5])
g1=g1+geom_line(aes(x=1:10,y=testMSEmat[,7]),colour=cols[6])
g1=g1+geom_line(aes(x=1:10,y=testMSEmat[,8]),colour=cols[7])
g1=g1+geom_line(aes(x=1:10,y=testMSEmat[,9]),colour=cols[8])
g1=g1+geom_line(aes(x=1:10,y=testMSEmat[,10]),colour=cols[9])
g1=g1+geom_line(aes(x=1:10,y=testMSEmat[,11]),colour=cols[10]) +labs(y="Validation set", x="Degree of polynomial") + theme_minimal()
g1
g1valid=g1
#ggarrange(g0,g1)
```

\vspace{2mm}

$\rightarrow$ No consensus which model really gives the lowest validation set MSE.

---

### Drawbacks with the validation set approach

\vspace{2mm}

* _\textcolor{red}{High variability}_ of validation set error due to dependency on the set of observation included in the training and validation set.

\vspace{2mm}

* _\textcolor{red}{Smaller sample size}_ for model fit, as only half of the observations are in the training set. Therefore, the validation set error may tend to overestimate the error rate on new observations for a model that is fit on the full data set (the more data, the lower the error).

---

Better ideas?

---

## Leave-one-out cross-validation (LOOCV)

$~$

Leave-one-out cross-validation (LOOCV) addresses the limitations of the validation set approach.

\vspace{4mm}

**Idea:**

* Only **one observation at a time** is left out (test set size $n=1$). 
* The remaining $n-1$ observations make up the training set. 
* The procedure of model fitting is repeated $n$ times, such that each of the $n$ observations is left out once. In each step, we calculate

$$\text{MSE}_i=(y_i-\hat{y}_i)^2 \ .$$

* The **total prediction error** is the mean across these $n$ models



$$\text{CV}_{n}=\frac{1}{n}\sum_{i=1}^n \text{MSE}_i \ .$$

---

### Regression example: LOOCV

<!-- ![LOOCV](loocv.png) -->

```{r auto_loocv, echo=FALSE, fig.width=5, fig.height=4,fig.align = "center",out.width='70%'}
library(ISLR)
library(boot)
library(ggplot2)
set.seed(123)
n=dim(Auto)[1]
testMSEvec=NULL
start=Sys.time()
for (polydeg in 1:10)
  {
    glm.fit=glm(mpg~poly(horsepower,polydeg),data=Auto)
    glm.cv1=cv.glm(Auto, glm.fit,K=n)
    testMSEvec=c(testMSEvec,glm.cv1$delta[1])
}
stopp=Sys.time()
yrange=c(15,28)
plotdf=data.frame("testMSE"=testMSEvec,"degree"=1:10)
g0=ggplot(plotdf,aes(x=degree,y=testMSE))+geom_line()+geom_point()+scale_y_continuous(limits = yrange)+scale_x_continuous(breaks=1:10)+labs(y="LOOCV")
g0+theme_minimal()
```


\tiny
```{r,echo=FALSE,eval=FALSE}
library(ISLR) #for Auto data set
library(boot) #for cv.glm
library(ggplot2) #for plotting
set.seed(123)
n=dim(Auto)[1]
testMSEvec=NULL
start=Sys.time()
for (polydeg in 1:10)
  {
    glm.fit=glm(mpg~poly(horsepower,polydeg),data=Auto)
    glm.cv1=cv.glm(Auto, glm.fit,K=n)
    testMSEvec=c(testMSEvec,glm.cv1$delta[1])
}
stopp=Sys.time()
yrange=c(15,28)
plotdf=data.frame("testMSE"=testMSEvec,"degree"=1:10)
g0=ggplot(plotdf,aes(x=degree,y=testMSE))+geom_line()+geom_point()+scale_y_continuous(limits = yrange)+scale_x_continuous(breaks=1:10)+labs(y="LOOCV")
g0+theme_minimal()
```
\normalsize

---

### Issues with leave-one-out cross-validation

$~$

* Pros: 
    + No randomness in training/validation splits!
    + Little bias, since nearly the whole data set used for training (compared to half for validation set approach).
  
\vspace{2mm}
    
* Cons: 
    + Expensive to implement -- need to fit $n$ different models.
    + High variance since: two training sets only differ by one observation, thus estimates from each fold highly correlated, which can lead to high variance in their average$^\star$.

\vspace{5mm}
\scriptsize
$^\star$ Recall that
$$\text{Var}(\sum_{i=1}^na_iX_i)=\sum_{i=1}^n\sum_{j=1}^n a_ia_j\text{Cov}(X_i,X_j)$$
$$=\sum_{i=1}^na_i^2\text{Var}(X_i)+2\sum_{i=2}^n \sum_{j=1}^{i-1}
a_ia_j\text{Cov}(X_i,X_j).$$ 

---

### LOOCV for multiple linear regression

\vspace{2mm}

There is a nice shortcut for LOOCV in the case of linear regression:

$$ \text{CV}_{n}=\frac{1}{n}\sum_{i=1}^n \left( \frac{y_i-\hat{y}_i}{1-h_{ii}} \right) ^2 \ ,$$

where $h_i$ is the $i$th diagonal element (leverage) of the hat matrix ${\bf H}={\bf X}({\bf X}^T{\bf X})^{-1}{\bf X}^T$, and $\hat{y}_i$ is the $i$th fitted value from the original least squares fit.

\vspace{2mm}

$\rightarrow$ Need to fit the model only once!

$~$

See Compulsory exercise 1.

---

## $k$-fold cross-validation

$~$

To address the drawbacks of LOOCV, we can leave out not just one single observation in each iteration, but $1/k$-th of all data.

$~$

**Procedure:**

* Split the data into $k$ (more or less) equal parts.

\vspace{2mm}

* Use $k-1$ parts to fit and the $k$th part to validate.

\vspace{2mm}

* Do this $k$ times and leave out another part in each round.


$~$

The MSE is then estimated in each of the $k$ iterations ($\text{MSE}_1,\ldots,\text{MSE}_k$). The $k$-fold CV is then a (weighted) avearge over the $k$  MSEs.

<!-- $$\text{CV}_k = \frac1k \sum_{j=1}^k \text{MSE}_j \ .$$ -->


---

**Comparison of LOOCV and $k$-fold CV**:

\centering
LOOCV: 

![ISLR Figure 5.3](../../ISLR/Figures/Chapter5/5.3.png){width=70%}

$k$-fold: 

![ISLR Figure 5.5](../../ISLR/Figures/Chapter5/5.5.png){width=70%}


<!-- See class notes for drawing. -->

---

### Formally

* Indices of observations - divided into $k$ folds: $C_1, C_2, \ldots, C_k$.
* $n_j$ elements in fold $j$. If $n$ is a multiple of $k$ then $n_j=n/k$ for all folds.

$$\text{MSE}_j=\frac{1}{n_j}\sum_{i\in C_j}(y_i-\hat{y}_i)^2$$
where $\hat{y}_i$ is the fit for observation $i$ obtained from the data with part $j$ removed. 

$$\text{CV}_{k}=\frac{1}{n} \sum_{j=1}^k n_j \text{MSE}_j$$

Observe: setting $k=n$ gives LOOCV.

---

### Regression example: $5$ and $10$-fold cross-validation

$~$

```{r auto_510fold, echo=FALSE, fig.width=5, fig.height=4,fig.align = "center",out.width='70%'}
library(ISLR)
library(boot)
library(ggplot2)
set.seed(123)
n=dim(Auto)[1]
testMSEvec5=NULL
testMSEvec10=NULL
start=Sys.time()
for (polydeg in 1:10)
  {
    glm.fit=glm(mpg~poly(horsepower,polydeg),data=Auto)
    glm.cv5=cv.glm(Auto, glm.fit,K=5)
    glm.cv10=cv.glm(Auto, glm.fit,K=10)
    testMSEvec5=c(testMSEvec5,glm.cv5$delta[1])
    testMSEvec10=c(testMSEvec10,glm.cv10$delta[1])
}
stopp=Sys.time()
yrange=c(15,28)
plotdf=data.frame("testMSE5"=testMSEvec5,"degree"=1:10)
g0=ggplot(plotdf,aes(x=degree,y=testMSE5))+geom_line()+geom_point()+scale_y_continuous(limits = yrange)+scale_x_continuous(breaks=1:10)+labs(y="CV")+ggtitle("5 and 10 fold CV")
g0+geom_line(aes(y=testMSEvec10),colour="red")+geom_point(aes(y=testMSEvec10),colour="red")+ggtitle("5 fold (black), 10 fold (red)")+theme_minimal()
```


```{r,echo=FALSE,eval=FALSE}
library(ISLR)
library(boot)
library(ggplot2)
set.seed(123)
n=dim(Auto)[1]
testMSEvec5=NULL
testMSEvec10=NULL
start=Sys.time()
for (polydeg in 1:10)
  {
    glm.fit=glm(mpg~poly(horsepower,polydeg),data=Auto)
    glm.cv5=cv.glm(Auto, glm.fit,K=5)
    glm.cv10=cv.glm(Auto, glm.fit,K=10)
    testMSEvec5=c(testMSEvec5,glm.cv5$delta[1])
    testMSEvec10=c(testMSEvec10,glm.cv10$delta[1])
}
stopp=Sys.time()
yrange=c(15,28)
plotdf=data.frame("testMSE5"=testMSEvec5,"degree"=1:10)
g0=ggplot(plotdf,aes(x=degree,y=testMSE5))+geom_line()+geom_point()+scale_y_continuous(limits = yrange)+scale_x_continuous(breaks=1:10)+labs(y="CV")+ggtitle("5 and 10 fold CV")
g0+geom_line(aes(y=testMSEvec10),colour="red")+geom_point(aes(y=testMSEvec10),colour="red")+ggtitle("5 fold (black), 10 fold (red)")+theme_minimal()
```
\normalsize

---

10 reruns (different splits) of the 10-CV method - to see variability:

```{r auto_510fold_10, echo=FALSE, fig.width=5, fig.height=4,fig.align = "center",out.width='70%'}
library(ISLR)
library(boot)
library(ggplot2)
set.seed(123)
n=dim(Auto)[1]
start=Sys.time()
testMSEmat=matrix(ncol=11,nrow=10)

for (polydeg in 1:10)
  {
    glm.fit=glm(mpg~poly(horsepower,polydeg),data=Auto)
    for (newsample in 1:11)
    {
        glm.cv10=cv.glm(Auto, glm.fit,K=10)
      testMSEmat[polydeg,newsample]=glm.cv10$delta[1]
    }
}

stopp=Sys.time()
yrange=c(15,28)

plotdf=data.frame("testMSE"=testMSEmat[,1],"degree"=1:10)
g0=ggplot(plotdf,aes(x=degree))+geom_line(y=testMSEmat[,1])+scale_y_continuous(limits = yrange)+scale_x_continuous(breaks=1:10)+labs(y="CV10")
#g0

cols=rainbow(10)
g1=g0+geom_line(aes(x=1:10,y=testMSEmat[,2]),colour=cols[1])
g1=g1+geom_line(aes(x=1:10,y=testMSEmat[,3]),colour=cols[2])
g1=g1+geom_line(aes(x=1:10,y=testMSEmat[,4]),colour=cols[3])
g1=g1+geom_line(aes(x=1:10,y=testMSEmat[,5]),colour=cols[4])
g1=g1+geom_line(aes(x=1:10,y=testMSEmat[,6]),colour=cols[5])
g1=g1+geom_line(aes(x=1:10,y=testMSEmat[,7]),colour=cols[6])
g1=g1+geom_line(aes(x=1:10,y=testMSEmat[,8]),colour=cols[7])
g1=g1+geom_line(aes(x=1:10,y=testMSEmat[,9]),colour=cols[8])
g1=g1+geom_line(aes(x=1:10,y=testMSEmat[,10]),colour=cols[9])
g1=g1+geom_line(aes(x=1:10,y=testMSEmat[,11]),colour=cols[10])
g1+theme_minimal()
#ggarrange(g0,g1)
```

There still _is_ variability, but _much less_ than for validation set approach.

---

### Issues with $k$-fold cross-validation

$~$

1. The _result may vary_ according to how the folds are made, but the variation is in general lower than for the validation set approach.

\vspace{2mm}

2. Computational load lower with $k=5$ or $10$ than LOOCV.

\vspace{2mm}

3. The training set is $(k-1)/k$ times the size of the original data set - the estimate of the prediction error is biased upwards.

\vspace{2mm}

4. This bias is the smallest when $k=n$ (LOOCV), but we know that LOOCV has high variance.

\vspace{2mm}

5. Due to the _bias-variance-trade-off_, $k$-fold CV often gives more accurate estimates of the test error rate than does LOOCV.  
$\rightarrow$ $k=5$ or $k=10$ is used as a compromise. 

---

### Choosing the best model

$~$

* There is a model parameter (maybe $K$ in KNN or the degree of the polynomial), say $\theta$, involved to calculate $\text{CV}_j$, $j=1,\ldots, k$

\vspace{2mm}

* Based on the CV vs $\theta$-plot we can choose the model with _the smallest ${\text{CV}_k}$_ as our best model.

\vspace{2mm}

* We then fit this model using the whole data set (not the test part, that is still kept away), and evaluate the performance on the test set.





---

**One standard error rule:**

Denote by $\text{MSE}_j(\theta)$, $j=1,\ldots, k$ the $k$ parts of the MSE that together give the $\text{CV}_k$.

We can compute the sample standard deviation (standard error) of all $\text{MSE}_j(\theta)$, $j=1,\ldots, k$ 
$$\hat{\text{SE}}(\text{CV}_k(\theta))= \sqrt{\sum_{j=1}^k (\text{MSE}_j(\theta) - \overline{\text{MSE}}(\theta))/(k-1)} \, $$

for each value of the complexity parameter $\theta$.\footnote{Strictly speaking, this estimate is not quite valid. Why?}

<!-- and then use $\text{SE}(\theta)=\text{SD}(\theta)/\sqrt{k}$ as the standard deviation of $\text{CV}_k$. -->


The _one standard error rule_ is to choose the simplest model (_e.g._, with lowest polynomial degree) within one standard error of the minimal error.

<!-- , that is, we move $\theta$ in the direction of simplicity until it ceases to be true that  -->

<!-- $$\text{CV}(\theta)\le \text{CV}(\hat{\theta})+ \hat{\text{SE}}(\text{CV}_k(\hat\theta)) \ .$$ -->

<!-- This will be the simplest model whose error is within one standard error of the minimal error. -->


---

### $k$-fold cross-validation in classification

\vspace{3mm}

What do we need to change from our regression set-up?

\vspace{3mm}

* For LOOCV $\hat{y}_i$ is the fit for observation $i$ obtained from the data with observations $i$ removed, and ${\text{Err}_i}=I(y_i\neq \hat{y}_i)$ . LOOCV is then 
$$\text{CV}_{n}=\frac{1}{n} \sum_{i=1}^n {\text{Err}_i}$$

\vspace{2mm}

* The $k$-fold CV is defined analogously.

\vspace{2mm}

* Read chapter 5.1.5 in the course book.

---

### Can we use CV for model assessment?
\vspace{2mm}

* Assume we already have our model (maybe found by using the methods from Module 6), so we want to perform model assessment based on all our data.

* Then we can use CV with all data (then the validation part is really the test part) and report on the model performance using the validation parts of the data as above.

\vspace{2mm}

### Can we use CV both for model selection and model assessment?
\vspace{2mm}

* Not really: using the test set for both model selection and estimation tends to overfit the test data, and the bias will be underestimated.  

* Solution: you can use two layers of CV - also called _nested CV_. See drawing in class.


---

## The right and the wrong way to do cross-validation

$~$

Example from the online lecture by Hastie and Tibshirain (slide 17 for part 5):

\vspace{2mm}

* Two-class problem, would like to use a simple classification method.
* Many possible predictors (e.g., $p=5000$) and not a big sample size (e.g., $n=50$).

\vspace{2mm}

_Strategy_:

1. Calculate the correlation between the class label and each of the $p$ predictors, and choose the $d=25$ predictors that have the strongest correlation with the class label.

2. Fit our classifier (logistic regression) using only the $d=25$ predictors.

\vspace{2mm}

How can we use cross-validation to produce an estimate of the performance of this classifier? 



---

**Q**: Can we apply cross-validation only to step 2? Why (not)?

**A:** No, **step 1 is part of the training procedure** (the class labels have already been used) and must be part of the CV to give an honest estimate of the performance of the classifier. 

$~$

* Wrong: Apply cross-validation in step 2.
* Right: Apply cross-validation to steps 1 and 2.

$~$

Note: We will see in the Recommended Exercises that doing the wrong thing can give a misclassification error approximately 0 - even if the "true" rate is 50%. 



---

### Selection bias in gene extraction on the basis of microarray gene-expression data

\vspace{2mm}

Article by [Christophe Ambroise and Geoffrey J. McLachlan, PNAS 2002](http://www.pnas.org/content/99/10/6562).



![Training vs test error](pnas.png){width=105%, align=center}




 
<!-- * In the context of cancer diagnosis and treatment, we consider the -->
<!-- problem of constructing an accurate prediction rule on the basis of -->
<!-- a relatively small number of tumor tissue samples of known type -->
<!-- containing the expression data on very many (possibly thousands) -->
<!-- genes.  -->

<!-- * Recently, results have been presented in the literature -->
<!-- suggesting that it is possible to construct a prediction rule from -->
<!-- only a few genes such that it has a negligible prediction error rate. -->

<!-- * However, in these results the test error or the leave-one-out -->
<!-- cross-validated error is calculated without allowance for the selection -->
<!-- bias.  -->

\vspace{10mm}

See also this nice [anecdote at about 7min 15 in the video](https://www.youtube.com/watch?v=r64tRyHFAJ8&list=PLAOUn-KLSAVNz3lv4a957qRpfPWH2EOg4&index=3).


---

We are looking together at Problem 3 of the recommended exercises 5.

---




# The Bootstrap

\vspace{2mm}

* Flexible and powerful statistical tool that
can be used to quantify _uncertainty_ associated with an estimator or statistical learning method.

\vspace{2mm}

* Very popular to obtain standard errors or confidence intervals for a coefficient, when parametric theory does not provide it.

\vspace{2mm}

* We will look at getting an estimate for the standard error of a sample median and of a regression coefficient.


---

* The inventor: Bradley Efron in 1979 - [see interview](https://www.youtube.com/watch?v=6l9V1sINzhE).

* The name? _To pull oneself up by one's bootstraps_ from "The Surprising
Adventures of Baron Munchausen" by Rudolph Erich Raspe:

$~$

> The Baron had fallen to the bottom of a deep lake. Just
when it looked like all was lost, he thought to pick himself
up by his own bootstraps.


$~$

* **Idea: Use the data itself to get more information about a statistic (an estimator).**

<!-- --- -->

<!-- ## Example: The mean $\bar{X}$ -->

<!-- Assume we have univariate random variables  $X_1, X_2, \ldots, X_n$, with some common mean $\mu$ and variance $\sigma^2$. -->

<!-- We can then form the average: $\bar{X}=\frac{1}{n} \sum_{i=1}^n X_i$, which we often use as an estimator for $\mu$. -->

<!-- If we want to estimate $\sigma^2$, a popular estimator is $S^2=\frac{1}{n-1}\sum_{i=1}^n (X_i-\bar{X})^2$, -->

<!-- If the observations are _independent_ (and identically distributed) we know that the variance of $\bar{X}$ is given by $\text{Var}(\bar{X})=\frac{\sigma^2}{n}$, -->
<!-- which we may estimate as $\widehat{\text{Var}}(\bar{X})=\frac{S^2}{n}$. -->

<!-- To form a confidence interval for $\mu$ we need to assume a distribution for the $X_i$s. If we assume that the $X_i$s are independent and normal $N(\mu,\sigma^2)$  and can get a 95% confidence interval from -->

<!-- $$ \bar{X}\pm t_{0.025,n-1}\frac{S}{\sqrt{n}}$$ -->

---

## Example: the standard deviation of the sample median?

$~$

* Assume that we observe a random sample $X_1, X_2, \ldots, X_n$ from an unknown probability distribution $f$. We are interested in saying something about the population median, thus we calculate the sample median $\tilde{X}$.  
$\rightarrow$ Q: How accurate is $\tilde{X}$ as an estimator?

\vspace{2mm}

* If we would know our distribution $F$, we could sample from $F$, and use simulations to answer our question.

\vspace{2mm}

* However, without knowledge of the distribution, we cannot calculate the standard deviation of our estimator, thus $\text{SD}(\tilde{X})$.

\vspace{2mm}

* That's where the bootstrap method comes into play.

---

Let's first assume we would know $f$, for example $X\sim N(0,1)$. Then we can repeatedly take samples and calculate the standard deviation of all medians to obtain an estimate:

$~$

\scriptsize
```{r,eval=TRUE, echo=TRUE}
set.seed(123)
n=101
B=1000
estimator=rep(NA,B)
for (b in 1:B)
{
  xs=rnorm(n)
  estimator[b]=median(xs)
}
sd(estimator)
```

---

```{r boot1, echo=FALSE, fig.width=5, fig.height=4,fig.align = "center",out.width='70%'}
ggplot(data=data.frame(x=estimator),aes(x=x))+
  geom_density()
```

---

## Moving from simulation to bootstrapping ($f$ unknown)

$~$

* The bootstrap method is using the observed data to estimate the _empirical distribution_ $\hat{f}$, that is each observed value of $x$ is given probability $1/n$.

\vspace{2mm}

* A _bootstrap sample_ $X^*_1,X^*_2,\ldots, X^*_n$ is a random sample drawn from $\hat{f}$.

\vspace{2mm}

* A simple way to obtain the bootstrap sample is to _draw with replacement from $X_1, X_2, \ldots, X_n$_.

\vspace{2mm}

* **Note**: Our bootstrap sample consists of $n$ members of $X_1, X_2, \ldots, X_n$ - some appearing more than once, other not appearing at all.

---

Compare the sample median

$~$

\scriptsize
```{r, eval=TRUE, echo=TRUE}
set.seed(123)
n=101
original=rnorm(n)
median(original)
```

\normalsize
to the median from **one bootstrap-sample**: 
\scriptsize
```{r, eval=TRUE, echo=TRUE}
boot1=sample(x=original,size=n,replace=TRUE)
median(boot1)
```

\normalsize
However, drawing only _one_ such sample does not help much.

---

## The bootstrap algorithm for estimating standard errors

$~$

1. Drawing $B$ bootstrap samples: drawn with replacement from the original data.

\vspace{2mm}

2. Evaluate the statistic of interest on _each of the $B$ bootstrap samples_ to get $\tilde{X}^*_b$ for the $b$th bootstrap sample.

\vspace{2mm}

3. Estimate squared standard error by
$$\frac{1}{B-1}\sum_{b=1}^B (\tilde{X}^*_b-\frac{1}{B}\sum_{b=1}^B \tilde{X}^*_b)^2 \ ,$$
which is the empirical standard deviation from the $B$ estimates  $\tilde{X}^*_b$, $b=1,\ldots,B$.


---

### Illustration for the median example (with a for-loop in R)

$~$

Simulate data:

\vspace{2mm}

\scriptsize
```{r, eval=TRUE, echo=TRUE}
set.seed(123)
n=101
original=rnorm(n)
median(original)
```

$~$

\normalsize
Then take 1000 bootstrap samples and repeatedly estimate the median to get a standard deviation:

\vspace{2mm}

\scriptsize
```{r, eval=TRUE, echo=TRUE}
B=1000
estimator=rep(NA,B)
for (b in 1:B)
  {
  thisboot=sample(x=original,size=n,replace=TRUE)
  estimator[b]=median(thisboot)
}
sd(estimator)
```


---

The distribution of the 1000 sampled estimates:

$~$

```{r boot2, echo=FALSE, fig.width=5, fig.height=4,fig.align = "center",out.width='70%'}
ggplot(data=data.frame(x=estimator),aes(x=x))+
  geom_density()

```

---

### Alternative: the built-in `boot` function from library  `boot`

$~$

\scriptsize
```{r, eval=TRUE, echo=TRUE}
library(boot)
boot.median=function(data,index) return(median(data[index]))
B=1000
boot(original,boot.median,R=B)
```

---

## With or without replacement?

\vspace{2mm}

In bootstrapping we sample _with replacement_ from our observations.

\vspace{4mm}

**Q:** What if we instead sample _without replacement_?

\vspace{2mm}

<!-- **A:** Then we would always get the same sample - given that the order of the sample points is not important for our estimator. -->

\vspace{4mm}
 

---

### Example: multiple linear regression 

We assume, for observation $i$:
$$Y_i= \beta_0 + \beta_{1}  x_{i1} + \beta_2 x_{i2} + ... + \beta_p x_{ip} + \varepsilon_i,$$ where $i=1,2,...,n$. The model can be written in matrix form:
$${\bf Y}={\bf X} \boldsymbol{\beta}+{\boldsymbol{\varepsilon}}.$$

The least squares estimator: $\hat{\boldsymbol\beta}=({\bf X}^T{\bf X})^{-1} {\bf X}^T {\bf Y}$
has $\text{Cov}(\boldsymbol\beta)=\sigma^2({\bf X}^T{\bf X})^{-1}$.

In the recommended exercises we will look at how to use bootstrapping to estimate the covariance of the estimator.\footnote{
Why is that "needed" if we already know the mathematical formula for the standard deviation? Answer: not needed - but OK to look at an example where we know the "truth".}

\vspace{2mm}
\normalsize

**Note**: Our bootstrap samples can also be used to make confidence intervals for the regression coefficients or prediction intervals for new observations. 

---

## A related method: Bagging 

\vspace{2mm}

Bagging (_bootstrap aggregation_) is a special case of _ensemble methods_.

\vspace{2mm}

* In Module 8 we will look at bagging, which is built on bootstrapping and the fact that it is possible to reduce the variance of a prediction by taking the average of many model fits.

\vspace{2mm}

* Particularly useful for estimation methods with large variances (like regression trees).

\vspace{2mm}

* Idea:
  + Draw $B$ bootstrap samples from your data and train the method for each sample $b$ in order to get $\hat{f}^{\star b}(x)$.
  + Average over all predictions to obtain 
      $$\hat{f}_{bag}(x) = \frac{1}{B} \sum_{b=1}^B \hat{f}^{\star b}(x) \ .$$




--- 

* Like this, we obtain a new model that has a smaller variance than each of the individual model. If the bootstrap samples were independent (which they are of course not), the variance (thus prediction error) would be reduced by

$$\text{Var}(\bar{X})=\frac{\sigma^2}{B} \ .$$
\vspace{2mm}

* In reality, the variance reduction is less. For a pairwise correlation $\rho$ we would have $\rho\sigma^2 + \frac{1-\rho}{B}\sigma^2$.

\vspace{2mm}

* Models that have poor prediction ability (as we may see can happen with regression and classification trees) might benefit greatly from bagging. More in Module 8.


---

# Further reading

* [Videoes on YouTube by the authors of ISL, Chapter 5](https://www.youtube.com/playlist?list=PL5-da3qGB5IA6E6ZNXu7dp89_uv8yocmf), and corresponding [slides](https://lagunita.stanford.edu/c4x/HumanitiesScience/StatLearning/asset/cv_boot.pdf)
* [Solutions to exercises in the book, chapter 5](https://rstudio-pubs-static.s3.amazonaws.com/65561_43c0eaaa8565414eae333b47038f716c.html)



---

# Excursion: R Markdown and `knitr`


* You can work through the Bonus part about R Markdown in the R course [https://digit.ntnu.no/courses/course-v1:NTNU+IMF001+2020/course/](https://digit.ntnu.no/courses/course-v1:NTNU+IMF001+2020/course/).

* You find a template for the exercise sheets on our course website [https://wiki.math.ntnu.no/tma4268/2022v/subpage6](https://wiki.math.ntnu.no/tma4268/2021v/subpage6). Do not forget to install the packages listed at the beginning of the exercise sheet.

 

---

### How to render a document? 

\vspace{2mm}

You do this by pressing `knit`. 

Knitting is also done by: Ctrl+Shift+K (Windows) or Cmd+Shift+K (MacOS).

\vspace{2mm}

1) Creating documents with R Markdown starts with an .Rmd file that contains a combination of markdown (content with simple text formatting) and R code chunks. 

2) The .Rmd file is fed to `knitr` which executes all of the R code chunks and creates a new markdown (.md) document which includes the R code and it’s output.

3) The markdown file generated by `knitr` is then processed by `pandoc` which is responsible for creating a finished web page, PDF, MS Word document, slide show, handout, book, dashboard, package vignette or other format.

---

* More: [About pandoc - the swiss army knife for file conversion](https://pandoc.org/)


* NB: even if you write tex this is first translated to md and then via pandoc to pdf, so subtle tex stuff may be missed on the way.

* Do you get a separate window popping up, or is your output shown in the Viewer tab of one of the window panes? Go to RStudio-Tools-Global Options-RMarkdown and check what is your value of "show output preview in".

---

### What output type do you want to produce?

\vspace{2mm}

* Just keep track of your own work: html_document

* For TMA4268 Compulsory exercise 1: we ask for a pdf-file (because that is easy to read and grade when you upload that to Blackboard)

* To produce a `pdf_document` RStudio (using pandoc) will call a latex-installation, so you need to have latex installed on your laptop to be able to produce a pdf-file.

* Toggle comment/uncomment with hashtag in YAML header `output` to make different options active, then press `knit`. Alterntively this can be done by calling the function `rmarkdown::render()` from your Console window.

* Optional: check that uncommenting `pdf_document` and commenting out `html_document` and pressing `knit` will give you a pdf-file.


* During rendering we use the location of the .Rmd file as the working directory, and the rendering is done in a _new session_.

 
---

# Formatting your R Markdown file

## Text, mathematics

* formatted with markdown
* mathematics (in latex) with formulas starting and ending with one `$` and equation with `$$`
* boldface with two stars and italic with one, new line with two spaces,
* For more, like sections, bulleted or numbered lists, tables, footnotes, rulers,... see
<https://github.com/rstudio/cheatsheets/raw/master/rmarkdown-2.0.pdf>

---

**Hands-on**: go the the Compulsory exercise 1 template, and just write and press `knitr` to see! 

Check how a nice formula using latex is generated for $Y_i=\beta_0+\beta_1 x_{i1} +\varepsilon_i$.



---

## Code Chunks

$~$

* Chunks of embedded code. Each chunk begins with \`\`\`{r} and ends with \`\`\`

\vspace{2mm}

* Set of code chunk options - but I have mainly used these two:
    + `echo`: display the code in the chunk, TRUE or FALSE or selected lines, or maybe with an R-object (later)
    + `eval`: run code in the chunk, TRUE or FALSE

\vspace{2mm}

* Remember to include packages to be used within the chunk (only needed the first time in a document, if the chung is evaluated, `eval=TRUE`).

\vspace{2mm} 

* Chunks can have (unique) names, may help when debugging.

\vspace{2mm}

* In chunks where figures are generated you can adjust the figure width/height and alignment, for example as

`{r auto, echo=FALSE, fig.width=4, fig.height=3,fig.align = "center",out.width='70%'}`

---

## Set-up chunk

$~$

* The set-up chunk is a code chunk that you add before you actually start to do the work.

\vspace{2mm}

* Smart things to add to the setup-chunk:

$~$

\scriptsize
```{r label, eval=FALSE,echo=TRUE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3)
```

---

## Calling R outside of the code chunks

$~$

Use the \` r before and \` after an R command to integrate into the text. For example,
```{r, echo=TRUE,eval=TRUE}
2+2
```
is equal to `r 2+2`.

$~$

This is what we have done in the YAML-header to include todays date on your submission:
 
\` r format(Sys.time(), '%d %B, %Y')\` gives `r format(Sys.time(), '%d %B, %Y')`.


---

# Problems

## When I `knit` with output: pdf_document no pdf-file is produced. Why?

* html_document is more forgiving than pdf_document wrt tex-errors
* a tex-error is not easy to spot - log is terrible
* many students have problems here, and some just end up handing in html or Rmd for the projects 

My solution

* first I render html_document and look for tex-errors and fix them
* then I render pdf_document, and include `keep_tex: yes` yaml option
* then I compile the tex in my favorite `texshop` and look for sensible log for errors, 
* and then go back to the Rmd and fix the error.

# Handing in Compulsory exercise 1

* I hope you have already joined a group from Bb (front page -- users and groups -- groups).

* Then under Compulsory exercises you will see "Hand in" is possible (this will come).

* Upload **both your Rmd and pdf-version of you R Markdown file** with your solutions to the exercise (based on the template).

* Scores and comments will be given on Bb.




---