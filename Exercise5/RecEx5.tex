% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Module 5: Recommended Exercises},
  pdfauthor={Kenneth Aase, Emma Skarstein, Daesoo Lee, Stefanie Muff; Department of Mathematical Sciences, NTNU},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}

\title{Module 5: Recommended Exercises}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{TMA4268 Statistical Learning V2023}
\author{Kenneth Aase, Emma Skarstein, Daesoo Lee, Stefanie
Muff \and Department of Mathematical Sciences, NTNU}
\date{February 9, 2023}

\begin{document}
\maketitle

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{We strongly recommend you to work through the Section 5.3 in the
course book (Lab: Cross-Validation and the Bootstrap)}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{problem-1}{%
\subsection{Problem 1}\label{problem-1}}

Explain how \(k\)-fold cross-validation is implemented.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  Draw a figure.
\item
  Specify algorithmically what is done, and in particular how the
  ``results'' from each fold are aggregated.
\item
  Relate to one example from regression. Ideas are the complexity w.r.t.
  polynomials of increasing degree in multiple linear regression, or
  \(K\) in KNN-regression.
\item
  Relate to one example from classification. Ideas are the complexity
  w.r.t. polynomials of increasing degree in logistic regression, or
  \(K\) in KNN-classification.
\end{enumerate}

Hint: the words ``loss function,'' ``fold,'' ``training,'' and
``validation'' are central.

\hypertarget{problem-2}{%
\subsection{Problem 2}\label{problem-2}}

What are the advantages and disadvantages of \(k\)-fold cross-validation
relative to

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  The validation set approach
\item
  Leave one out cross-validation (LOOCV)
\item
  What are recommended values for \(k\), and why?
\end{enumerate}

Hint: the words ``bias,'' ``variance'' and ``computational complexity''
should be included.

\hypertarget{problem-3}{%
\subsection{Problem 3}\label{problem-3}}

Topic: Selection bias and the ``wrong way to do CV.''

The task here is to devise an algorithm to ``prove'' that the wrong way
is wrong and that the right way is right.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  What are the steps of such an algorithm? Write down a suggestion.
  Hint: How do you generate data for predictors and class labels, how do
  you do the classification task, where is the CV in the correct way and
  wrong way inserted into your algorithm? Can you make a schematic
  drawing of the right and the wrong way?
\item
  We are now doing a simulation to illustrate the selection bias problem
  in CV, when it is applied the wrong way. Here is what we are
  (conceptually) going to do:
\end{enumerate}

Generate data

\begin{itemize}
\item
  Simulate high dimensional data (\(p = 5000\) predictors) from
  independent or correlated normal variables, but with few samples
  (\(n = 50\)).
\item
  Randomly assign class labels (here only \(2\)). This means that the
  ``truth'' is that the misclassification rate can not get very small.
  What is the expected misclassification rate (for this random set)?
\end{itemize}

Classification task:

\begin{itemize}
\tightlist
\item
  We choose a few (for example \(d = 25\)) of the predictors (those with
  the highest correlation to the outcome).
\item
  Perform a classification rule (here: logistic empirical Bayes) on
  these predictors.
\item
  Then we run CV (\(k = 5\)) on either only the \(d\) (the wrong way),
  or on all \(c + d\) predictors (the right way).
\item
  Report misclassification errors for both situations.
\end{itemize}

One possible version of this is presented in the R-code below. Go
through the code and explain what is done in each step, then run the
code and observe if the results are in agreement with what you expected.
Make changes to the R-code if you want to test out different strategies.

We start by generating data for \(n=50\) observations

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(boot)}
\CommentTok{\# GENERATE DATA; use a seed for reproducibility}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{4268}\NormalTok{)}
\NormalTok{n }\OtherTok{\textless{}{-}} \DecValTok{50}  \CommentTok{\# Number of observations}
\NormalTok{p }\OtherTok{\textless{}{-}} \DecValTok{5000}  \CommentTok{\# Number of predictors}
\NormalTok{d }\OtherTok{\textless{}{-}} \DecValTok{25}  \CommentTok{\# Top correlated predictors chosen}

\CommentTok{\# Generating predictor data}
\NormalTok{xs }\OtherTok{\textless{}{-}} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rnorm}\NormalTok{(n }\SpecialCharTok{*}\NormalTok{ p, }\DecValTok{0}\NormalTok{, }\DecValTok{4}\NormalTok{), ncol }\OtherTok{\textless{}{-}}\NormalTok{ p, nrow }\OtherTok{\textless{}{-}}\NormalTok{ n)  }\CommentTok{\# Independent predictors}
\FunctionTok{dim}\NormalTok{(xs)  }\CommentTok{\# n times p}
\CommentTok{\# Generate class labels independent of predictors {-} so if all}
\CommentTok{\# classifies as class 1 we expect 50\% errors in general.}
\NormalTok{ys }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, n}\SpecialCharTok{/}\DecValTok{2}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, n}\SpecialCharTok{/}\DecValTok{2}\NormalTok{))  }\CommentTok{\# now really 50\% of each}
\FunctionTok{table}\NormalTok{(ys)}
\end{Highlighting}
\end{Shaded}

\textbf{WRONG CV}: Select the 25 most correlated predictors outside the
CV.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{corrs }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(xs, }\DecValTok{2}\NormalTok{, cor, }\AttributeTok{y =}\NormalTok{ ys)}
\FunctionTok{hist}\NormalTok{(corrs)}
\NormalTok{selected }\OtherTok{\textless{}{-}} \FunctionTok{order}\NormalTok{(corrs}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }\AttributeTok{decreasing =} \ConstantTok{TRUE}\NormalTok{)[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{d]  }\CommentTok{\# Top d correlated selected}
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(ys, xs[, selected])}
\end{Highlighting}
\end{Shaded}

Then run CV around the fitting of the classifier - use logistic
regression and built in \texttt{cv.glm()} function

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{logfit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(ys }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{family =} \StringTok{"binomial"}\NormalTok{, }\AttributeTok{data =}\NormalTok{ data)}
\NormalTok{cost }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(r, }\AttributeTok{pi =} \DecValTok{0}\NormalTok{) }\FunctionTok{mean}\NormalTok{(}\FunctionTok{abs}\NormalTok{(r }\SpecialCharTok{{-}}\NormalTok{ pi) }\SpecialCharTok{\textgreater{}} \FloatTok{0.5}\NormalTok{)}
\NormalTok{kfold }\OtherTok{\textless{}{-}} \DecValTok{10}
\NormalTok{cvres }\OtherTok{\textless{}{-}} \FunctionTok{cv.glm}\NormalTok{(}\AttributeTok{data =}\NormalTok{ data, }\AttributeTok{cost =}\NormalTok{ cost, }\AttributeTok{glmfit =}\NormalTok{ logfit, }\AttributeTok{K =}\NormalTok{ kfold)}
\NormalTok{cvres}\SpecialCharTok{$}\NormalTok{delta}
\end{Highlighting}
\end{Shaded}

Observe a zero misclassification rate!

\textbf{CORRECT CV}: Do not pre-select predictors outside the CV, but as
part of the CV. In order words, the entire analysis is done within the
CV. We need to code this ourselves:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reorder }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, }\AttributeTok{replace =} \ConstantTok{FALSE}\NormalTok{)}
\NormalTok{validclass }\OtherTok{\textless{}{-}} \ConstantTok{NULL}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{kfold) \{}
\NormalTok{    neach }\OtherTok{\textless{}{-}}\NormalTok{ n}\SpecialCharTok{/}\NormalTok{kfold}
\NormalTok{    trainids }\OtherTok{\textless{}{-}} \FunctionTok{setdiff}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, (((i }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ neach }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{(i }\SpecialCharTok{*}\NormalTok{ neach)))}
\NormalTok{    traindata }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(xs[reorder[trainids], ], ys[reorder[trainids]])}
\NormalTok{    validdata }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(xs[reorder[}\SpecialCharTok{{-}}\NormalTok{trainids], ], ys[reorder[}\SpecialCharTok{{-}}\NormalTok{trainids]])}
    \FunctionTok{colnames}\NormalTok{(traindata) }\OtherTok{\textless{}{-}} \FunctionTok{colnames}\NormalTok{(validdata) }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FunctionTok{paste}\NormalTok{(}\StringTok{"X"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{p),}
        \StringTok{"y"}\NormalTok{)}
\NormalTok{    foldcorrs }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(traindata[, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{p], }\DecValTok{2}\NormalTok{, cor, }\AttributeTok{y =}\NormalTok{ traindata[, p }\SpecialCharTok{+}
        \DecValTok{1}\NormalTok{])}
\NormalTok{    selected }\OtherTok{\textless{}{-}} \FunctionTok{order}\NormalTok{(foldcorrs}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{, }\AttributeTok{decreasing =} \ConstantTok{TRUE}\NormalTok{)[}\DecValTok{1}\SpecialCharTok{:}\NormalTok{d]  }\CommentTok{\# Select top d correlated }
\NormalTok{    data }\OtherTok{\textless{}{-}}\NormalTok{ traindata[, }\FunctionTok{c}\NormalTok{(selected, p }\SpecialCharTok{+} \DecValTok{1}\NormalTok{)]}
\NormalTok{    trainlogfit }\OtherTok{\textless{}{-}} \FunctionTok{glm}\NormalTok{(y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{family =} \StringTok{"binomial"}\NormalTok{, }\AttributeTok{data =}\NormalTok{ data)}
\NormalTok{    pred }\OtherTok{\textless{}{-}} \FunctionTok{plogis}\NormalTok{(}\FunctionTok{predict.glm}\NormalTok{(trainlogfit, }\AttributeTok{newdata =}\NormalTok{ validdata[, selected]))}
\NormalTok{    validclass }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(validclass, }\FunctionTok{ifelse}\NormalTok{(pred }\SpecialCharTok{\textgreater{}} \FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{0}\NormalTok{))}
\NormalTok{\}}
\FunctionTok{table}\NormalTok{(ys[reorder], validclass)}
\DecValTok{1} \SpecialCharTok{{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{diag}\NormalTok{(}\FunctionTok{table}\NormalTok{(ys[reorder], validclass)))}\SpecialCharTok{/}\NormalTok{n}
\end{Highlighting}
\end{Shaded}

\hypertarget{problem-4}{%
\subsection{Problem 4}\label{problem-4}}

We will calculate the probability that a given observation in our
original sample is part of a bootstrap sample. This is useful for us to
know in Module 8.

Our sample size is \(n\).

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  We draw one observation from our sample. What is the probability of
  drawing observation \(i\) (i.e., \(x_i\))? And of not drawing
  observation \(i\)?
\item
  We make \(n\) independent draws (with replacement). What is the
  probability of not drawing observation \(i\) in any of the \(n\)
  drawings? What is then the probability that data point \(i\) is in our
  bootstrap sample (that is, more than \(0\) times)?
\item
  When \(n\) is large \((1-\frac{1}{n})^n \approx \frac{1}{e}\). Use
  this to give a numerical value for the probability that a specific
  observation \(i\) is in our bootstrap sample.
\item
  Write a short R code chunk to check your result. (Hint: An example on
  how to this is on page 198 in our ISLR book.) You may also study the
  result in c.~How good is the approximation as a function of \(n\)?
\end{enumerate}

\hypertarget{problem-5}{%
\subsection{Problem 5}\label{problem-5}}

Explain with words and an algorithm how you would proceed to use
bootstrapping to estimate the standard deviation and the \(95\%\)
confidence interval of one of the regression parameters in multiple
linear regression. Comment on which assumptions you make for your
regression model.

\hypertarget{problem-6}{%
\subsection{Problem 6}\label{problem-6}}

Implement your algorithm from 5 both using for-loop and using the
\texttt{boot} function. Hint: see page 195 of our ISLR book. Use our
SLID data set and provide standard errors for the coefficient for age.
Compare with the theoretical value
\(({\bf X}^\top{\bf X})^{-1}\hat{\sigma}^2\) that you find in the output
from the regression model.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(car)}
\FunctionTok{library}\NormalTok{(boot)}
\NormalTok{SLID }\OtherTok{\textless{}{-}} \FunctionTok{na.omit}\NormalTok{(SLID)}
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{dim}\NormalTok{(SLID)[}\DecValTok{1}\NormalTok{]}
\NormalTok{SLID.lm }\OtherTok{\textless{}{-}} \FunctionTok{lm}\NormalTok{(wages }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ SLID)}
\FunctionTok{summary}\NormalTok{(SLID.lm)}\SpecialCharTok{$}\NormalTok{coeff[}\StringTok{"age"}\NormalTok{, ]}
\end{Highlighting}
\end{Shaded}

Now go ahead and use bootstrap to estimate the \(95\%\) CI. Compare your
result to

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{confint}\NormalTok{(SLID.lm)}
\end{Highlighting}
\end{Shaded}


\end{document}
