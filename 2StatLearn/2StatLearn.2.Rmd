---
subtitle: "TMA4268 Statistical Learning V2022"
title: "Module 2, Part 2: Random vectors, covariance, multivariate Normal distribution"
author: "Stefanie Muff, Department of Mathematical Sciences, NTNU"
date: "January 20, 2022"
output:
  # beamer_presentation:
  #   keep_tex: yes
  #   fig_caption: false
  #   latex_engine: xelatex
  #   theme: "Singapore"
  #   colortheme: "default"
  #   font: "serif"
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
urlcolor: blue
header-includes: \usepackage{xcolor,bm}
---

```{r setup, include=FALSE}
showsolA<-TRUE
showsolB<-FALSE
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize")
```


# Overview

\normalsize


* Random vectors
\vspace{2mm}

* The covariance and correlation matrix 
\vspace{2mm}

* The multivariate normal distribution



---

# Random vector

* A random vector $\boldsymbol{X}_{(p\times 1)}$ is a $p$-dimensional vector of random variables. For example
    + Weight of cork deposits in $p=4$ directions (N, E, S, W).
    + Factors to predict body fat: bmi, age, weight, hip circumference,....
    
* Joint distribution function: $f(\boldsymbol{x})$.
* From joint distribution function to marginal (and conditional distributions).
$$f_1(x_1)=\int_{-\infty}^{\infty}\cdots \int_{-\infty}^{\infty} f(x_1,x_2,\ldots,x_p)dx_2 \cdots dx_p$$
* Cumulative distribution (definite integrals!) used to calculate probabilites.

---

## Moments 

\vspace{2mm}

The moments are important properties of the distribution of $\boldsymbol{X}$. We will look at:

\vspace{2mm}

* E: Mean of random vector and random matrices.
* Cov: Covariance matrix.
* Corr: Correlation matrix.
* E and Cov of multiple linear combinations.

---

### The Cork deposit data

$~$

* Classical multivariate data set from Rao (1948).
* Weigth of bark deposits of $n=28$ cork trees in $p=4$ directions (N, E, S, W). 

$~$

\tiny
```{r, echo=TRUE, eval=TRUE,size="tiny"}
corkds=as.matrix(
  read.table("https://www.math.ntnu.no/emner/TMA4268/2019v/data/corkMKB.txt")
  )
dimnames(corkds)[[2]]=c("N","E","S","W")
head(corkds)

dim(corkds)
```

---

Look at the data (always the first thing to do):

\tiny
```{r, echo=TRUE, eval=TRUE,size="tiny",fig.width=5.0,fig.height=3.3}
library(GGally)
corkds <- as.data.frame(corkds)
ggpairs(corkds)
```


---

 
* Here we have a random sample of $n=28$  cork trees from the population and observe a $p=4$ dimensional random vector for each tree.

* This leads us to the definition of random vectors and a random matrix for cork trees:

$$\boldsymbol{X}_{(28 \times 4)}=\left[ \begin{array}{cccc}X_{11} & X_{12} & X_{13}& X_{14}\\ X_{21} & X_{22} & X_{23}& X_{24}\\ X_{31} & X_{32} & X_{33}& X_{34}\\ \vdots & \vdots & \ddots & \vdots\\ X_{28,1} & X_{28,2} & X_{28,3}& X_{28,4}\\ \end{array} \right]$$

---

## Rules for means

$~$

* Random vector $\boldsymbol{X}_{(p\times 1)}$ with mean vector $\boldsymbol{\mu}_{(p\times 1)}$:\footnote{Observe that $\text{E}(X_j)$ is calculated from the marginal distribution of $X_j$ and contains no information about dependencies between $X_{j}$ and $X_k$, $k\neq j$.}

$$\boldsymbol{X}_{(p\times 1)}=\left[ \begin{array}{c}X_1\\ X_2\\ \vdots\\ X_p\\ \end{array}\right], \text{ and }\boldsymbol{\mu}_{(p \times 1)}=\text{E}(\boldsymbol{X})=\left[ \begin{array}{c}\text{E}(X_1)\\ \text{E}(X_2)\\ \vdots\\ \text{E}(X_p)\\ \end{array}\right] \ .$$

  
  
* Same rule for random matrices.

\vspace{2mm}

* Random matrix $\boldsymbol{X}_{(n\times p)}$ and random matrix $\boldsymbol{Y}_{(n\times p)}$:
$$\text{E}(\boldsymbol{X}+\boldsymbol{Y})=\text{E}(\boldsymbol{X})+\text{E}(\boldsymbol{Y}) \ .$$

(Rules of vector addition)

---

* Random matrix $\boldsymbol{X}_{(n\times p)}$ and conformable constant matrices $\boldsymbol{A}$ and $\boldsymbol{B}$:
$$\text{E}(\boldsymbol{A}\boldsymbol{X}\boldsymbol{B})=\boldsymbol{A}\text{E}(\boldsymbol{X})\boldsymbol{B}$$

Proof: Board

\vspace{60mm}
<!-- Look at element $(i,j)$ of $\boldsymbol{A}\boldsymbol{X}\boldsymbol{B}$ -->
<!-- $$e_{ij}=\sum_{k=1}^n a_{ik} \sum_{l=1}^p X_{kl}b_{lj}$$ -->
<!-- (where $a_{ik}$ and $b_{lj}$ are elements of $\boldsymbol{A}$ and $\boldsymbol{B}$ respectively), and see that $\text{E}(e_{ij})$ is the element $(i,j)$ if $\boldsymbol{A}\text{E}(\boldsymbol{X})\boldsymbol{B}$. -->


---

**Q**: 

* What are the univariate analogue to the formulas on the previous two slides (which you studied in your first introductory course in statistics)? 



```{r, echo=FALSE, results='asis', eval=showsolB}
cat("
**A**: $$\\text{E}(aX+b)=a \\text{E}(X)+b$$
")
```

\vspace{80mm}

---

## The covariance
\vspace{2mm}

In the introductory statistics course we defined the covariance 
\begin{align*}
\rho_{ij} & =\text{Cov}(X_i,X_j)  =\text{E}[(X_i-\mu_i)(X_j-\mu_j)] \\
& =\text{E}(X_i \cdot X_j)-\mu_i\mu_j \ . 
\end{align*}

* What is the covariance called when $i=j$? 

\vspace{2mm}

* What does it mean when the covariance is
    + negative
    + zero
    + positive?
    
---    
    
Make a scatter plot for negative, zero and positive correlation (see also R example).

\vspace{6cm}

---

## Variance-covariance matrix

\vspace{2mm}

* Consider random vector $\boldsymbol{X}_{(p\times 1)}$ with mean vector $\boldsymbol{\mu}_{(p\times 1)}$:
$$\boldsymbol{X}_{(p\times 1)} =\left[ \begin{array}{c} X_1\\ X_2\\ \vdots\\ X_p\\ \end{array} \right], \text{ and }\boldsymbol{\mu}_{(p\times 1)} =\text{E}(\boldsymbol{X})=\left[ \begin{array}{c} \text{E}(X_1)\\ \text{E}(X_2)\\ \vdots\\ \text{E}(X_p)\\ \end{array}\right]$$

* Variance-covariance matrix $\boldsymbol\Sigma$ (real and symmetric)
\begin{align*}
\boldsymbol\Sigma & =\text{Cov}(\boldsymbol{X})  
=\text{E}[(\boldsymbol{X}-\boldsymbol{\mu})(\boldsymbol{X}-\boldsymbol{\mu})^T] \\
& = \left[ \begin{array}{cccc} \sigma_{1}^2 & \sigma_{12} & \cdots & \sigma_{1p}\\ \sigma_{12} & \sigma_{2}^2 & \cdots & \sigma_{2p}\\ \vdots & \vdots & \ddots & \vdots\\ \sigma_{1p} & \sigma_{2p} & \cdots & \sigma_{p}^2\\ \end{array}  \right] 
 = \text{E}(\boldsymbol{X}\boldsymbol{X}^T)-\boldsymbol{\mu}\boldsymbol{\mu}^T
\end{align*}



---



* The diagonal elements in $\boldsymbol\Sigma$, $\sigma_{ii}=\sigma_i^2$, are variances.

* The off-diagonal elements are covariances $\sigma_{ij}=\text{E}[(X_i-\mu_i)(X_j-\mu_j)]=\sigma_{ji}$.

* $\boldsymbol\Sigma$ is called variance, covariance and variance-covariance matrix and denoted both $\text{Var}(\boldsymbol{X})$ and $\text{Cov}(\boldsymbol{X})$.

---

### Exercise: the variance-covariance matrix

\vspace{2mm}
    
Let $\boldsymbol{X}_{4\times 1}$ have variance-covariance matrix
$$\boldsymbol\Sigma= \left[ \begin{array}{cccc} 2&1&0&0\\
      1&2&0&1\\
      0&0&2&1\\
      0&1&1&2\\
          \end{array}
          \right].$$
          
Explain what this means. 

---

### Correlation matrix
$~$

Correlation matrix $\boldsymbol{\rho}$ (real and symmetric)
$$\boldsymbol{\rho}=\left[ \begin{array}{cccc}
    \frac{\sigma_{1}^2}{\sqrt{\sigma_{1}^2\sigma_{1}^2}} &
    \frac{\sigma_{12}}{\sqrt{\sigma_{1}^2\sigma_{2}^2}} &
    \cdots &
    \frac{\sigma_{1p}}{\sqrt{\sigma_{1}^2\sigma_{p}^2}}\\
    \frac{\sigma_{12}}{\sqrt{\sigma_{1}^2\sigma_{2}^2}} &
    \frac{\sigma_{2}^2}{\sqrt{\sigma_{2}^2\sigma_{2}^2}} &
    \cdots &
    \frac{\sigma_{2p}}{\sqrt{\sigma_{2}^2\sigma_{p}^2}}\\
    \vdots & \vdots & \ddots & \vdots\\
      \frac{\sigma_{1p}}{\sqrt{\sigma_{1}^2\sigma_{p}^2}} &
    \frac{\sigma_{2p}}{\sqrt{\sigma_{2}^2\sigma_{p}^2}} &
    \cdots &
    \frac{\sigma_{p}^2}{\sqrt{\sigma_{p}^2\sigma_{p}^2}}\\ \end{array}\right]=
 \left[ \begin{array}{cccc}
    1 & \rho_{12} & \cdots & \rho_{1p}\\
    \rho_{12} & 1 & \cdots & \rho_{2p}\\
    \vdots & \vdots & \ddots & \vdots\\
    \rho_{1p} & \rho_{2p} & \cdots & 1\\
\end{array}\right]$$

$$\boldsymbol{\rho}=(\boldsymbol{V}^{\frac{1}{2}})^{-1}
    \boldsymbol\Sigma(\boldsymbol{V}^{\frac{1}{2}})^{-1}, \text{   where    }
   \boldsymbol{V}^{\frac{1}{2}}=
 \left[ \begin{array}{cccc}
    \sqrt{\sigma_{1}^2} & 0& \cdots & 0\\
    0 &  \sqrt{\sigma_{2}^2}  & \cdots & 0\\
    \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & \cdots &  \sqrt{\sigma_{p}^2} \\
\end{array} \right]$$

---

### Exercise: the correlation matrix

$~$

Let $\boldsymbol{X}_{4\times 1}$ have variance-covariance matrix
$$\boldsymbol\Sigma= \left[ \begin{array}{cccc} 2&1&0&0\\
      1&2&0&1\\
      0&0&2&1\\
      0&1&1&2\\
          \end{array}
          \right].$$
          Find the correlation matrix.
          
---


**A**: 

<!-- $$\rho=\left[ \begin{array}{cccc} 1&0.5&0&0\\ -->
<!--       0.5&1&0&0.5\\ -->
<!--       0&0&1&0.5\\ -->
<!--       0&0.5&0.5&1\\ -->
<!--           \end{array} -->
<!--           \right]$$ -->


          
---

### Linear combinations

$~$ 

Consider a random vector $\boldsymbol{X}_{(p\times 1)}$ with mean vector
    $\boldsymbol{\mu}=\text{E}(\boldsymbol{X})$ and
   variance-covariance matrix $\boldsymbol\Sigma=\text{Cov}(\boldsymbol{X})$.

The linear combinations 
$$\boldsymbol{Z}=\boldsymbol{C}\boldsymbol{X}=\left[ \begin{array}{c} \sum_{j=1}^p c_{1j}X_j\\ \sum_{j=1}^p c_{2j}X_j\\ \vdots \\ \sum_{j=1}^p c_{kj}X_j \end{array} \right]$$ 
have
$$\text{E}(\boldsymbol{Z})=\text{E}(\boldsymbol{C}\boldsymbol{X})=\boldsymbol{C}\boldsymbol{\mu}$$
$$\text{Cov}(\boldsymbol{Z})=\text{Cov}(\boldsymbol{C}\boldsymbol{X})=
   \boldsymbol{C}\boldsymbol\Sigma\boldsymbol{C}^T$$

<!-- [Proof](https://www.math.ntnu.no/emner/TMA4268/2018v/notes/CXproof.pdf) -->

**Exercise:** Follow the proof (board) - what are the most important transitions? 


---

### Exercise: Linear combinations

$$\boldsymbol{X}=\left[ \begin{array}{c} X_N\\
          X_E\\
X_S\\
          X_W\\
          \end{array}
          \right],
          \boldsymbol{\mu}=\left[
      \begin{array}{c} \mu_N\\
          \mu_E\\
\mu_S\\
          \mu_W\\
          \end{array}
          \right], \text{ and } \boldsymbol\Sigma=\left[ \begin{array}{cccc}
    \sigma_{N}^2 & \sigma_{NE} & \sigma_{NS} & \sigma_{NW}\\
    \sigma_{NE} & \sigma_{E}^2 & \sigma_{ES}& \sigma_{EW}\\
        \sigma_{NS} & \sigma_{SE} & \sigma_{S}^2 & \sigma_{SW}\\
    \sigma_{NW} & \sigma_{EW} & \sigma_{SW} & \sigma_{W}^2\\
\end{array} \right]$$

Scientists would like to compare the following three _contrasts_: N-S, E+W and (E+W)-(N+S),
and define a new random vector $\boldsymbol{Y}_{(3\times 1)}=\boldsymbol{C}_{(3\times 4)} \boldsymbol{X}_{(4\times 1)}$ giving the three contrasts.

\vspace{2mm}

* Write down $\boldsymbol{C}$.
* Explain how to find $\text{E}(Y_1)$ and $\text{Cov}(Y_1,Y_3)$.
* Use R to find the mean vector, covariance matrix and correlations matrix of $\boldsymbol{Y}$, when the mean vector and covariance matrix for $\boldsymbol{X}$ given below.

---

\tiny

```{r,eval=TRUE,echo=TRUE,results="hold"}
corkds <- as.matrix(read.table("https://www.math.ntnu.no/emner/TMA4268/2019v/data/corkMKB.txt"))
dimnames(corkds)[[2]] <- c("N","E","S","W")
mu=apply(corkds,2,mean)
mu
Sigma=var(corkds)
Sigma
```

```{r,eval=TRUE,echo=TRUE,results="hold"}

(C <- matrix(c(1,0,-1,0,0,1,0,1,-1,1,-1,1),byrow=T,nrow=3))
```

```{r,eval=TRUE,echo=TRUE,results="hold"}
C %*% Sigma %*% t(C)
```

---

### The covariance matrix - more requirements?
$~$

Random vector $\boldsymbol{X}_{(p\times 1)}$ with mean vector $\boldsymbol{\mu}_{(p\times 1)}$ and covariance matrix
$$\boldsymbol\Sigma=\text{Cov}(\boldsymbol{X})=\text{E}[(\boldsymbol{X}-\boldsymbol{\mu})(\boldsymbol{X}-\boldsymbol{\mu})^T]=
\left[ \begin{array}{cccc}
    \sigma_{1}^2 & \sigma_{12} & \cdots & \sigma_{1p}\\
    \sigma_{12} & \sigma_{2}^2 & \cdots & \sigma_{2p}\\
    \vdots & \vdots & \ddots & \vdots\\
    \sigma_{1p} & \sigma_{2p} & \cdots & \sigma_{p}^2\\
\end{array} \right]$$

---


* The covariance matrix is by construction symmetric, and it is common to require that the covariance matrix is positive semidefinite. This means that, for every vector $\boldsymbol{b}\neq \boldsymbol{0}$

$$\boldsymbol{b}^T \boldsymbol{\Sigma} \boldsymbol{b} \geq 0 \ .$$


* Why do you think that is?

$~$

Hint: Is it possible that the variance of the linear combination $\boldsymbol{Y}=\boldsymbol{b}^T\boldsymbol{X}$ is negative? 


---

## Random vectors - Single-choice exercise

\vspace{4mm}

Quizz on www.menti.com, code 3967 5773

<!-- In class - go to [https://forms.gle/HCaAFxpY2b7JoVim9](https://forms.gle/HCaAFxpY2b7JoVim9) to answer the questions on the following slides -->


 

---

### Question 1: Mean of sum
\vspace{2mm}

$\boldsymbol{X}$ and $\boldsymbol{Y}$ are two bivariate random vectors with $\text{E}(\boldsymbol{X})=(1,2)^T$
and $\text{E}(\boldsymbol{Y})=(2,0)^T$. What is $\text{E}(\boldsymbol{X}+\boldsymbol{Y})$?


$~$

* A: $(1.5,1)^T$
* B: $(3,2)^T$
* C: $(-1,2)^ T$
* D: $(1,-2)^T$

---

### Question 2: Mean of linear combination
\vspace{2mm}

$\boldsymbol{X}$ is a 2-dimensional random vector with $\text{E}(\boldsymbol{X})=(2,5)^T$ , and $\boldsymbol{b}=(0.5, 0.5)^T$ is a constant vector. What is $\text{E}(\boldsymbol{b}^T\boldsymbol{X})$?

$~$

* A: 3.5
* B: 7
* C: 2
* D: 5 

---

### Question 3: Covariance
\vspace{2mm}

$\boldsymbol{X}$ is a $p$-dimensional random vector with mean $\boldsymbol{\mu}$. Which of the following defines the covariance matrix?

$~$

* A: $E[(\boldsymbol{X}-\boldsymbol{\mu})^T(\boldsymbol{X}-\boldsymbol{\mu})]$ 
* B: $E[(\boldsymbol{X}-\boldsymbol{\mu})(\boldsymbol{X}-\boldsymbol{\mu})^T]$ 
* C: $E[(\boldsymbol{X}-\boldsymbol{\mu})(\boldsymbol{X}-\boldsymbol{\mu})]$  
* D: $E[(\boldsymbol{X}-\boldsymbol{\mu})^T(\boldsymbol{X}-\boldsymbol{\mu})^T]$

---

### Question 4: Mean of linear combinations
\vspace{2mm}

$\boldsymbol{X}$ is a $p$-dimensional random vector with mean $\boldsymbol{\mu}$
and covariance matrix $\boldsymbol\Sigma$. $\boldsymbol{C}$ is a constant matrix. 
What is then the mean of the $k$-dimensional random vector $\boldsymbol{Y}=\boldsymbol{C}\boldsymbol{X}$?

$~$

* A: $\boldsymbol{C}\boldsymbol{\mu}$
* B: $\boldsymbol{C}\boldsymbol\Sigma$
* C: $\boldsymbol{C}\boldsymbol{\mu}\boldsymbol{C}^T$
* D: $\boldsymbol{C}\boldsymbol\Sigma\boldsymbol{C}^T$

---

### Question 5: Covariance of linear combinations
\vspace{2mm}

$\boldsymbol{X}$ is a $p$-dimensional random vector with mean $\boldsymbol{\mu}$
and covariance matrix $\boldsymbol\Sigma$. $\boldsymbol{C}$ is a constant matrix. 
What is then the covariance of the $k$-dimensional random vector $\boldsymbol{Y}=\boldsymbol{C}\boldsymbol{X}$?

$~$

* A: $\boldsymbol{C}\boldsymbol{\mu}$
* B: $\boldsymbol{C}\boldsymbol\Sigma$
* C: $\boldsymbol{C}\boldsymbol{\mu}\boldsymbol{C}^T$
* D: $\boldsymbol{C}\boldsymbol\Sigma\boldsymbol{C}^T$

---

### Question 6: Correlation
\vspace{2mm}

$\boldsymbol{X}$ is a $2$-dimensional random vector with
covariance matrix 
\[ \boldsymbol\Sigma= \left[\begin{array}{cc}
          4 & 0.8 \\
          0.8 & 1\\
      \end{array}
    \right]\]
Then the correlation between the two elements of $\boldsymbol{X}$ are:

$~$

* A: 0.10
* B: 0.25
* C: 0.40
* D: 0.80

---

```{r, echo=FALSE, results='asis', eval=showsolB}
cat("
## Answers: 
1B, 2A, 3B, 4A, 5D, 6C 
")
```

---

# The multivariate normal distribution
\vspace{2mm}

Why is the mvN so popular?

* Many natural phenomena may be modelled using this distribution (just as in the univariate case).
* Multivariate version of the central limit theorem- the sample mean will be approximately multivariate normal for large samples.
* Good interpretability of the covariance.
* Mathematically tractable.
* Building block in many models and methods.



---

\centering
![3D multivariate Normal distributions](mvN.jpeg){width=80%}


3D multivariate Normal distributions

---

## The multivariate normal (mvN) pdf
\vspace{2mm}

The random vector $\boldsymbol{X}_{p\times 1}$ is multivariate normal $N_p$ with mean $\boldsymbol{\mu}$ and (positive definite) covariate matrix $\boldsymbol\Sigma$. The pdf is:

$$f(\boldsymbol{x})=\frac{1}{(2\pi)^\frac{p}{2}|\boldsymbol\Sigma|^\frac{1}{2}} \exp\{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^T\boldsymbol\Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu})\}$$
$~$


**Questions**: 



* How does this compare to the univariate version?
$$f(x)=\frac{1}{\sqrt{2\pi}\sigma}\exp\{- \frac{1}{2\sigma^2}(x-\mu)^2\}$$

* Why do we need the constant in front of the $\exp$?
* What is the dimension of the part in $\exp$? 

* What happens if the determinant $|\boldsymbol\Sigma| = 0$ (degenerate case)?
<!-- Then the MVN distribution does not have a density! -->

---

## Four useful properties of the mvN
\vspace{2mm}

Let $\boldsymbol{X}_{(p\times 1)}$ be a random vector from $N_p(\boldsymbol{\mu},\boldsymbol\Sigma)$.

\vspace{2mm}

1. The grapical contours of the mvN are ellipsoids (can be shown using spectral decomposition).

2. Linear combinations of components of $\boldsymbol{X}$ are (multivariate) normal.

3. All subsets of the components of $\boldsymbol{X}$ are (multivariate) normal (special case of the above).

4. Zero covariance implies that the corresponding components are independently distributed (in contrast to general distributions).

$~$

\small
If you need a refresh, you might find that video useful: [https://www.youtube.com/watch?v=eho8xH3E6mE](https://www.youtube.com/watch?v=eho8xH3E6mE)

---

All of these are proven in TMA4267 Linear Statistical Models. 

The result 4 is rather useful! If you have a bivariate normal and observed covariance 0, then your variables are independent.

---
  
## Contours of multivariate normal distribution
\vspace{2mm}

* Contours of constant density for the $p$-dimensional normal distribution are ellipsoids defined by $\boldsymbol{x}$ such that
$$ (\boldsymbol{x}-\boldsymbol{\mu})^T\boldsymbol\Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu})=b $$
where $b>0$ is a constant.

* These ellipsoids are centered at $\boldsymbol{\mu}$ and have axes $\pm \sqrt{b \lambda_i}\boldsymbol{e}_i$, where $\boldsymbol\Sigma\boldsymbol{e}_i=\lambda_i \boldsymbol{e}_i$ (eigenvector for $\lambda_i$), for $i=1,...,p$.

* To see this the spectral decomposition of the covariance matrix is useful.

<!-- * $(\boldsymbol{x}-\boldsymbol{\mu})^T\boldsymbol\Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu})$ is distributed as $\chi^2_p$. -->

<!-- * The volume inside the ellipsoid of $\boldsymbol{x}$ values satisfying -->
<!-- $$ (\boldsymbol{x}-\boldsymbol{\mu})^T\boldsymbol\Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu}) \le \chi^2_p(\alpha) $$ -->
<!-- has probability $1-\alpha$. -->

---

Note: 

_In M4: Classification the mvN is very important and we will often draw contours of the mvN as ellipses (in 2D space). This is the reason why we do that. _


---

## Identify the mvNs from their contours
\vspace{4mm}

Let $\boldsymbol\Sigma=\left[\begin{array}{cc} \sigma_x^2 & \rho\sigma_{x}\sigma_{y}\\\rho\sigma_{x}\sigma_{y}&\sigma_y^2\\ \end{array} \right]$.

\vspace{4mm}
The following four figure contours have been generated:

* 1: $\sigma_x=1$, $\sigma_y=2$, $\rho=-0.3$
* 2: $\sigma_x=1$, $\sigma_y=1$, $\rho=0$
* 3: $\sigma_x=1$, $\sigma_y=1$, $\rho=0.5$
* 4: $\sigma_x=1$, $\sigma_y=2$, $\rho=0$

\vspace{8mm}

**Match the distributions to the figures on the next slide.** 


---

```{r cont,  echo=FALSE, fig.width=6, fig.height=6,fig.align = "center",out.width='65%'}
library(mvtnorm)
library(ggpubr)
x.points <- seq(-4,4,length.out=100)
y.points <- x.points
z <- matrix(0,nrow=100,ncol=100)
mu <- c(0,0)
sigmax <- 1
sigmay <- 2
rho <- -0.3

plot_cont <- function(sigmax,sigmay,rho){

sigma <- matrix(c(sigmax^2,sigmax*sigmay*rho,sigmax*sigmay*rho,sigmay^2),nrow=2)

for (i in 1:100) {
for (j in 1:100) {
  z[i,j] <- dmvnorm(c(x.points[i],y.points[j]),
  mean=mu,sigma=sigma)
 }
}
contour(x.points,y.points,z,xlim=c(-4.5,4.5),ylim=c(-4.5,4.5))
}

par(mfrow=c(2,2),oma=c(0,0,0,0),mar=c(0,0,0,0))
 
plot_cont(1,1,0)
text(-4,4,labels="A",cex=2)

plot_cont(1,1,0.5)
text(-4,4,labels="B",cex=2)

plot_cont(1,2,-0.3)
text(-4,4,labels="C",cex=2)

plot_cont(1,2,0)
text(-4,4,labels="D",cex=2)
```

Take a look at the contour plots - when are the contours circles, when ellipses?



---


## Multiple choice - multivariate normal
\vspace{2mm}

A second quizz on www.menti.com, code 3967 5773

Choose the correct answer. Let's go!

<!-- [https://forms.gle/G6eJNp2Pr3trjL8T7](https://forms.gle/G6eJNp2Pr3trjL8T7) -->

---

### Question 1: Multivariate normal pdf
\vspace{2mm}

The probability density function is 
$(\frac{1}{2\pi})^\frac{p}{2}\det(\boldsymbol\Sigma)^{-\frac{1}{2}}\exp\{-\frac{1}{2}Q\}$ where $Q$ is

\vspace{2mm}

* A: $(\boldsymbol{x}-\boldsymbol{\mu})^T\boldsymbol\Sigma^{-1}(\boldsymbol{x}-\boldsymbol{\mu})$
* B: $(\boldsymbol{x}-\boldsymbol{\mu})\boldsymbol\Sigma(\boldsymbol{x}-\boldsymbol{\mu})^T$
* C: $\boldsymbol\Sigma-\boldsymbol{\mu}$

---

### Question 2:  Trivariate normal pdf
\vspace{2mm}

What graphical form has the solution to $f(\boldsymbol{x})=$ constant?

\vspace{2mm}

* A: Circle
* B: Parabola
* C: Ellipsoid
* D: Bell shape

---

### Question 3: Multivariate normal distribution
\vspace{2mm}

$\boldsymbol{X}_p \sim N_p(\boldsymbol{\mu},\boldsymbol\Sigma)$, and 
$\boldsymbol{C}$ is a $k \times p$ constant matrix. $\boldsymbol{Y}=\boldsymbol{C}\boldsymbol{X}$ is

\vspace{2mm}

* A: Chi-squared with $k$ degrees of freedom
* B: Multivariate normal with mean $k\boldsymbol{\mu}$
* C: Chi-squared with $p$ degrees of freedom
* D: Multivariate normal with mean $\boldsymbol{C}\boldsymbol{\mu}$

---

### Question 4: Independence
\vspace{2mm}

 Let $\boldsymbol{X}\sim N_3(\boldsymbol{\mu},\boldsymbol\Sigma)$, with
$$\boldsymbol\Sigma= \left[     
\begin{array}{ccc} 1&1&0\\
      1&3&2\\
      0&2&5\\
          \end{array}
          \right].$$
Which two variables are independent?

\vspace{2mm}

* A: $X_1$ and $X_2$
* B: $X_1$ and $X_3$
* C: $X_2$ and $X_3$
* D: None -- but two are uncorrelated.

---

### Question 5: Constructing independent variables?
\vspace{2mm}

Let $\boldsymbol{X}\sim N_p(\boldsymbol{\mu},\boldsymbol\Sigma)$. How can I construct a vector of independent standard normal variables from $\boldsymbol{X}$?

\vspace{2mm}

* A: $\boldsymbol\Sigma(\boldsymbol{X}-\boldsymbol{\mu})$
* B: $\boldsymbol\Sigma^{-1}(\boldsymbol{X}+\boldsymbol{\mu})$
* C: $\boldsymbol\Sigma^{-\frac{1}{2}}(\boldsymbol{X}-\boldsymbol{\mu})$
* D: $\boldsymbol\Sigma^{\frac{1}{2}}(\boldsymbol{X}+\boldsymbol{\mu})$


---

```{r, echo=FALSE, results='asis', eval=showsolB}
cat("
## Answers: 
1A 2C 3D 4B 5C 
")
```


---


# <a id="further"> Further reading/resources</a>

* [Videoes on YouTube by the authors of ISL, Chapter 2](https://www.youtube.com/playlist?list=PL5-da3qGB5IDvuFPNoSqheihPOQNJpzyy)




---

# Acknowledgements

Thanks to Mette Langaas, who developed the first slide set in 2018 and 2019, and to Julia Debik for contributing to this module.

