% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Module 8: Recommended Exercises},
  pdfauthor={Daesoo Lee, Emma Skarstein, Kenneth Aase, Stefanie Muff; Department of Mathematical Sciences, NTNU},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}

\title{Module 8: Recommended Exercises}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{TMA4268 Statistical Learning V2023}
\author{Daesoo Lee, Emma Skarstein, Kenneth Aase, Stefanie
Muff \and Department of Mathematical Sciences, NTNU}
\date{March 9, 2023}

\begin{document}
\maketitle

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{problem-1-theoretical}{%
\subsection{Problem 1 -- Theoretical}\label{problem-1-theoretical}}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  Provide a detailed explanation of the algorithm that is used to fit a
  regression tree. What is different for a classification tree?
\item
  What are the advantages and disadvantages of regression and
  classification trees?
\item
  What is the idea behind bagging and what is the role of bootstap? How
  do random forests improve that idea?
\item
  What is an out-of bag (OOB) error estimator and what percentage of
  observations are included in an OOB sample? (Hint: The result from
  RecEx5-Problem 4c can be used)
\item
  Bagging and Random Forests typically improve the prediction accuracy
  of a single tree, but it can be difficult to interpret, for example in
  terms of understanding which predictors are how relevant. How can we
  evaluate the importance of the different predictors for these methods?
\end{enumerate}

\hypertarget{problem-2-regression-book-ex.-8}{%
\subsection{Problem 2 -- Regression (Book Ex.
8)}\label{problem-2-regression-book-ex.-8}}

In the lab, a classification tree was applied to the Carseats data set
after converting the variable \texttt{Sales} into a qualitative response
variable. Now we will seek to predict \texttt{Sales} using regression
trees and related approaches, treating the response as a quantitative
variable.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  Split the data set into a training set and a test set. (Hint: Use 70\%
  of the data as training set and the rest 30\% as testing set)
\end{enumerate}

\textbf{R-hints}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ISLR)}
\FunctionTok{data}\NormalTok{(}\StringTok{"Carseats"}\NormalTok{)}
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{4268}\NormalTok{)}
\NormalTok{n }\OtherTok{=} \FunctionTok{nrow}\NormalTok{(Carseats)}
\NormalTok{train }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\NormalTok{n, }\FloatTok{0.7}\SpecialCharTok{*}\FunctionTok{nrow}\NormalTok{(Carseats), }\AttributeTok{replace =}\NormalTok{ F)}
\NormalTok{test }\OtherTok{=}\NormalTok{ ...}
\NormalTok{Carseats.train }\OtherTok{=}\NormalTok{ ...}
\NormalTok{Carseats.test }\OtherTok{=}\NormalTok{ ...}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Fit a regression tree to the training set. Plot the tree, and
  interpret the results. What test MSE do you obtain?
\end{enumerate}

\textbf{R-hints}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tree)}
\NormalTok{tree.mod }\OtherTok{=} \FunctionTok{tree}\NormalTok{(Sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{., ..., }\AttributeTok{data =}\NormalTok{ ...)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{2}
\tightlist
\item
  Use cross-validation in order to determine the optimal level of tree
  complexity. Does pruning the tree improve the test MSE?
\end{enumerate}

\textbf{R-hints}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{4268}\NormalTok{)}
\NormalTok{cv.Carseats  }\OtherTok{=} \FunctionTok{cv.tree}\NormalTok{(...) }
\NormalTok{tree.min }\OtherTok{=}  \FunctionTok{which.min}\NormalTok{(...)}
\NormalTok{best }\OtherTok{=}\NormalTok{ cv.Carseats}\SpecialCharTok{$}\NormalTok{size[...]}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  Use the bagging approach with 500 trees in order to analyze the data.
  What test MSE do you obtain? Use the \texttt{importance()} function to
  determine which variables are most important.
\end{enumerate}

\textbf{R-hints}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(randomForest)}
\FunctionTok{dim}\NormalTok{(Carseats)}
\NormalTok{bag.Carseats }\OtherTok{=} \FunctionTok{randomForest}\NormalTok{(Sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{., ... , }\AttributeTok{ntree =} \DecValTok{500}\NormalTok{, }\AttributeTok{importance =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{4}
\tightlist
\item
  Use random forests and to analyze the data. Include 500 trees and
  select 3 variables for each split. What test MSE do you obtain? Use
  the \texttt{importance()} function to determine which variables are
  most important. Describe the effect of m, the number of variables
  considered at each split, on the error rate obtained.
\end{enumerate}

\textbf{R-hints}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{rf.Carseats }\OtherTok{=} \FunctionTok{randomForest}\NormalTok{(Sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{., ... , }\AttributeTok{mtry =} \DecValTok{3}\NormalTok{, }\AttributeTok{ntree =} \DecValTok{500}\NormalTok{, }\AttributeTok{importance =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{5}
\tightlist
\item
  Finally use boosting with 500 trees, an interaction depth \(d=4\) and
  a shrinkage factor \(\lambda=0.1\) (default in the \texttt{gbm()}
  function) on our data. Compare the MSE to all other methods.
\end{enumerate}

\textbf{R-hints}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(gbm)}
\NormalTok{r.boost}\OtherTok{=}\FunctionTok{gbm}\NormalTok{(Sales}\SpecialCharTok{\textasciitilde{}}\NormalTok{., Carseats.train,}
                 \AttributeTok{distribution=}\NormalTok{...,}
                 \AttributeTok{n.trees=}\NormalTok{ ... ,}\AttributeTok{interaction.depth=}\NormalTok{ ..., }\AttributeTok{shrinkage =}\NormalTok{ ...)}
\end{Highlighting}
\end{Shaded}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{6}
\tightlist
\item
  What is the effect of the number of trees (\texttt{ntree}) on the test
  error? Plot the test MSE as a function of \texttt{ntree} for both the
  bagging and the random forest method.
\end{enumerate}

\hypertarget{problem-3-classification}{%
\subsection{Problem 3 --
Classification}\label{problem-3-classification}}

In this exercise you are going to implement a spam filter for e-mails by
using tree-based methods. Data from 4601 e-mails are collected and can
be uploaded from the kernlab library as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(kernlab)}
\FunctionTok{data}\NormalTok{(spam)}
\end{Highlighting}
\end{Shaded}

Each e-mail is classified by \texttt{type} ( \texttt{spam} or
\texttt{nonspam}), and this will be the response in our model. In
addition there are 57 predictors in the dataset. The predictors describe
the frequency of different words in the e-mails and orthography
(capitalization, spelling, punctuation and so on).

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\item
  Study the dataset by writing \texttt{?spam} in R.
\item
  Create a training set and a test set for the dataset. (Hint: Use 70\%
  of the data as training set and the rest 30\% as testing set)
\item
  Fit a tree to the training data with \texttt{type} as the response and
  the rest of the variables as predictors. Study the results by using
  the \texttt{summary()} function. Also create a plot of the tree. How
  many terminal nodes does it have?
\item
  Predict the response on the test data. What is the misclassification
  rate?
\item
  Use the \texttt{cv.tree()} function to find the optimal tree size.
  Prune the tree according to the optimal tree size by using the
  \texttt{prune.misclass()} function and plot the result. Predict the
  response on the test data by using the pruned tree. What is the
  misclassification rate in this case?
\item
  Create a decision tree by using the bagging approach with \(B=500\).
  Use the function \texttt{randomForest()} and consider all of the
  predictors in each split. Predict the response on the test data and
  report the misclassification rate.
\item
  Apply the \texttt{randomForest()} function again with 500 trees, but
  this time consider only a subset of the predictors in each split. This
  corresponds to the random forest-algorithm. Study the importance of
  each variable by using the function \texttt{importance()}. Are the
  results as expected based on earlier results? Again, predict the
  response for the test data and report the misclassification rate.
\item
  Use \texttt{gbm()} to construct a boosted classification tree using
  5000 trees, an interaction depth of \(d=3\) and a shrinkage parameter
  of \(\lambda=0.001\). Predict the response for the test data and
  report the misclassification rate.
\item
  Compare the misclassification rates in d-h. Which method gives the
  lowest misclassification rate for the test data? Are the results as
  expected?
\end{enumerate}

\end{document}
