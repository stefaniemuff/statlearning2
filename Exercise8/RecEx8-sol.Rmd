---
title: 'Module 8: Solutions to Recommended Exercises'
author:
- Daesoo Lee, Emma Skarstein, Kenneth Aase, Stefanie Muff
- Department of Mathematical Sciences, NTNU
date: "March 9, 2023"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    toc: no
    toc_depth: 2
  html_document:
    toc: no
    toc_depth: '2'
    df_print: paged
subtitle: TMA4268 Statistical Learning V2023
urlcolor: blue
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize")
```


```{r, eval=FALSE,echo=FALSE}
install.packages("gamlss.data")
install.packages("tidyverse")
install.packages("GGally")
install.packages("Matrix")
```

---


## Problem 1 -- Theoretical

a)

1. _Recursive binary splitting_: We find the best single partitioning of the data such that the reduction of RSS is the greatest. This process is applied sequencially to each of the split parts until a predefined minimum number of leave observation is reached.

2. _Cost complexity pruning_ of the large tree from previoius step, in order to obtain a sequence of best trees as a function of a parameter $\alpha$. Each value of $\alpha$ corresponds to a subtree that minimize the following equation (several $\alpha$s for the same tree):

$$\sum_{m=1}^{|T|}\sum_{i:x_i\in R_m}(y_i - \hat y_{R_m})^2 + \alpha |T|,$$

where $|T|$ is the numb er of terminal nodes.

3. _$K$-fold cross-validation_ to choose $\alpha$. For each fold:

* Repeat Steps 1 and 2 on all but the kth folds of the training data.
* Evaluate the mean squared prediction on the data in the left-out kth fold, as a function of $\alpha$.
* Average the results for each value of $\alpha$ and choose $\alpha$ to minimize the average error.

4. Return the subtree from Step 2 that corresponds to the chosen value of $\alpha.$

For a **classification** tree we replace RSS with Gini index or entropy.

b) 

$\color{green}{\text{Advantages}}$

* Very easy to explain

* Can be displayed graphically

* Can handle both quantitative and qualitative predictors without the need to create dummy variables

$\color{red}{\text{Disadvantages}}$

* The predictive accuracy is usualy not very high

* They are non-robust. That is a small change in the data can cause a large change in the estimated tree


c) 

Decision trees suffer from high variance. Recall that if we have $B$ $i.i.d$ observations of a random variable $X$ with the same mean and variance $\sigma^2.$
We calculate the mean $\bar{X} = \frac{1}{B} \sum_{b=1}^B X_b,$ and the variance of the mean is $\text{Var}(\bar{X}) = \frac{\sigma^2}{B}.$
That is by averaging we get reduced variance. 

For decision trees, if we have $B$ training  sets, we could estimate $\hat{f}_1({\boldsymbol x}),\hat{f}_2({\boldsymbol x}),\ldots, \hat{f}_B({\boldsymbol x})$ and average them as
$$\hat{f}_{avg}({\boldsymbol x})=\frac{1}{B}\sum_{b=1}^B \hat{f}_b({\boldsymbol x}) \ .$$
However we do not have many data independent data sets, and we bootstraping to create $B$ datasets. These datasets are however not completely independent and the reduction in variance is therefore not as large as for independent training sets.

To make the different trees that are built from each bootstrapped dataset more different, random forests use a random subset of the predictors to split the tree into new branches at each step. This decorrelates the different trees that are built from the $B$ bootstrapped datasets, and consequently reduces variance.

d) An OOB is the set of observations that were not chosen to be in a specific bootstrap sample. From RecEx5-Problem 4c we have that on average $1-0.632 = 0.368$ are included in the OOB sample.

e) 

**Variable importance based on node impurity**

**Regression Trees**: The total amount that the RSS is decreased due to splits of each predictor, averaged over the B trees.

**Classification Trees**: The importance is the mean decrease (over all B trees) in the Gini index by splits of a predictor.

**Variable importance based on randomization**

This measure is based on how much the predictive accuracy (MSE or gini indiex) is decreased when the variable is replaced by a permuted version of it. You find a drawing [here](https://github.com/stefaniemuff/statlearning/blob/master/8Trees/M8_variableImportanceRandomization.pdf). 

## Problem 2 -- Regression (Book Ex. 8)


a) 


```{r}
library(ISLR)
data("Carseats")
set.seed(4268)
n = nrow(Carseats)
train = sample(1:n, 0.7*nrow(Carseats), replace = F)
test = (1:n)[-train]
Carseats.train = Carseats[train,]
Carseats.test = Carseats[-train,]
```

b) 

```{r,fig.width=10, fig.height=8,out.width='70%'}
library(tree)
tree.mod = tree(Sales~., Carseats, subset = train)
summary(tree.mod)
plot(tree.mod)
text(tree.mod, pretty = 0)

yhat = predict(tree.mod, newdata = Carseats.test)
mse = mean((yhat - Carseats.test$Sales)^2)
mse
```

c) 

```{r}
set.seed(4268)
cv.Carseats  = cv.tree(tree.mod) 
tree.min =  which.min(cv.Carseats$dev)
best = cv.Carseats$size[tree.min]
plot(cv.Carseats$size, cv.Carseats$dev, type = "b")
points(cv.Carseats$size[tree.min], cv.Carseats$dev[tree.min], col = "red", pch = 20)
```

We see that trees with sizes 11, 12, 16 and 17 have similar deviance values. We might choose the tree of size 11 as it gives the simpler tree.


```{r,fig.width=10, fig.height=8,out.width='70%'}
pr.tree = prune.tree(tree.mod, best = 11)
plot(pr.tree)
text(pr.tree, pretty = 0)
```



```{r}
yhat = predict(pr.tree, newdata = Carseats.test)
mse = mean((yhat - Carseats.test$Sales)^2) 
mse
```

There is a slight reduction in MSE for the pruned tree with 11 leaves.

d) 

```{r}
library(randomForest)
dim(Carseats)
bag.Carseats = randomForest(Sales~., Carseats.train, mtry=ncol(Carseats)-1, ntree = 500, importance = TRUE)
yhat.bag = predict(bag.Carseats, newdata = Carseats.test)
mse.bag = mean((yhat.bag - Carseats.test$Sales)^2)
mse.bag
```

Bagging decreases the test MSE significantly to `r round(mse.bag,2)`. From the importance plots we might conclude that `Price`and `ShelveLoc` are the most important Variables.

```{r,fig.width=8, fig.height=5,out.width='80%'}
importance(bag.Carseats)
varImpPlot(bag.Carseats)
```



e) 

```{r}
rf.Carseats = randomForest(Sales~., data = Carseats.train, mtry = 3, ntree = 500, importance = TRUE)
yhat.rf = predict(rf.Carseats, newdata = Carseats.test)
mse_forest <- mean((yhat.rf - Carseats.test$Sales)^2)
mse_forest
```

We use $p/3 = 10/3 \approx  3$ trees, and we obtain an MSE of `r round(mse_forest,2)` which is slightly larger than Bagging MSE. The two most important Variables are again `Price`and `ShelveLoc`.

```{r,fig.width=8, fig.height=5,out.width='80%'}
importance(rf.Carseats)
varImpPlot(rf.Carseats)
```

f)

```{r}
library(gbm)
r.boost=gbm(Sales~., Carseats.train,
                 distribution="gaussian",
                 n.trees=500,interaction.depth=4, shrinkage =0.1)
yhat.boost = predict(r.boost, newdata = Carseats.test,n.trees=500)
mse_boost <- mean((yhat.boost - Carseats.test$Sales)^2)
mse_boost
```

We see a further decrease in MSE by boosting our trees.


g) 

```{r,fig.width=4.5, fig.height=4,out.width='70%'}
train.predictors = Carseats.train[,-1]
test.predictors = Carseats.test[,-1]
Y.train = Carseats.train[,1]
Y.test = Carseats.test[,1]

bag.Car = randomForest(train.predictors, y = Y.train, xtest = test.predictors, ytest = Y.test, mtry = 10, ntree = 500)
rf.Car = randomForest(train.predictors, y = Y.train, xtest = test.predictors, ytest = Y.test, mtry = 3, ntree = 500)
plot(1:500, bag.Car$test$mse, col = "blue", type = "l", xlab = "Number of Trees", ylab = "Test MSE",ylim=c(2,2.8))
lines(1:500, rf.Car$test$mse, col = "green")

legend("topright", c("m = p", "m = p/3"), col = c("blue", "green"), cex = 1, lty = 1)
```




## Problem 3 -- Classification

```{r}
library(kernlab)
data(spam)
```


a) 

```{r}
?spam
```

b) 

```{r}
library(ISLR)
set.seed(4268)
n = nrow(spam)
train = sample(1:n, 0.7*n, replace = F)
test = (1:n)[-train]
spam.train = spam[train,]
spam.test = spam[-train,]
```

c) 

```{r,fig.width=10, fig.height=8,out.width='70%'}
spam.tree=tree(type~.,spam,subset=train)

plot(spam.tree)
text(spam.tree,pretty=1)

summary(spam.tree)
```
  
d) 

```{r}
yhat=predict(spam.tree,spam[test,],type="class")
response.test=spam$type[test]

misclass=table(yhat,response.test)
misclass

1-sum(diag(misclass))/sum(misclass)
```
  
e) 

```{r,fig.width=6, fig.height=5,out.width='70%'}
set.seed(4268)

cv.spam=cv.tree(spam.tree,FUN=prune.misclass)

plot(cv.spam$size,cv.spam$dev,type="b")
```

According to the plot the optimal number of terminal nodes is 6 (or larger). We choose 6 as this gives the simplest tree, and prune the tree according to this value.

```{r,fig.width=8, fig.height=6,out.width='70%'}
prune.spam=prune.misclass(spam.tree,best=6)

plot(prune.spam)
text(prune.spam,pretty=1)
```

We predict the response for the test data:

```{r}
yhat.prune=predict(prune.spam,spam[test,],type="class")

misclass.prune=table(yhat.prune,response.test)
misclass.prune
```

The misclassification rate is

```{r}
1-sum(diag(misclass.prune))/sum(misclass.prune)
```

f) 

```{r}
library(randomForest)
bag.spam=randomForest(type~.,data=spam,subset=train,mtry=ncol(spam)-1,ntree=500,importance=TRUE)
```


We predict the response for the test data as before:

```{r}
yhat.bag=predict(bag.spam,newdata=spam[test,])

misclass.bag=table(yhat.bag,response.test)
misclass.bag
```

The misclassification rate is

```{r}
1-sum(diag(misclass.bag))/sum(misclass.bag)
```

g) 

We now use the random forest-algorithm and consider only $\sqrt{57}\approx 8$ of the predictors at each split. This is specified in `mtry`. 

```{r}
set.seed(4268)
rf.spam=randomForest(type~.,data=spam,subset=train,mtry=round(sqrt(ncol(spam)-1)),ntree=500,importance=TRUE)
```

We study the importance of each variable

```{r,eval=F}
importance(rf.spam)
```

If `MeanDecreaseAccuracy` and `MeanDecreaseGini` are large, the corresponding covariate is important.

```{r,fig.width=9, fig.height=6,out.width='90%'}
varImpPlot(rf.spam)
```

In this plot we see that `charExclamation` is the most important covariate, followed by `remove` and `charDollar`. This is as expected as these variables are used in the top splits in the classification trees we have seen so far.

We now predict the response for the test data.

```{r}
yhat.rf=predict(rf.spam,newdata=spam[test,])

misclass.rf=table(yhat.rf,response.test)
1-sum(diag(misclass.rf))/sum(misclass.rf)
```

The misclassification rate is given by

```{r}
misclass.rf
```

h) 

The `gbm()` function does not allow factors, so we have to use '1' and '0' instead of `spam` and `nonspam`

```{r}
library(gbm)
set.seed(4268)

spamboost = spam
spamboost$type = c()
spamboost$type[spam$type == "spam"] = 1
spamboost$type[spam$type == "nonspam"] = 0

boost.spam = gbm(type ~ ., data = spamboost[train, ], distribution = "bernoulli", 
    n.trees = 5000, interaction.depth = 3, shrinkage = 0.001)
```

We predict the response for the test data

```{r}
yhat.boost = predict(boost.spam, newdata = spamboost[-train, ], n.trees = 5000, 
    distribution = "bernoulli", type = "response")

yhat.boost = ifelse(yhat.boost > 0.5, 1, 0)  #Transform to 0 and 1 (nonspam and spam).

misclass.boost = table(yhat.boost, spamboost$type[test])

misclass.boost

```

and the misclassification rate is

```{r}
1 - sum(diag(misclass.boost))/sum(misclass.boost)
```

i) 

We get lower missclassification rates for bagging, random forest and boosting than for a simple tree, which is expected.
