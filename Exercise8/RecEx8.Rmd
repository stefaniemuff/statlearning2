---
title: 'Module 8: Recommended Exercises'
author:
- Daesoo Lee, Kenneth Aase, Sara Martino, Stefanie Muff
- Department of Mathematical Sciences, NTNU
date: "March 7, 2024"
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    toc: no
    toc_depth: 2
  html_document:
    toc: no
    toc_depth: '2'
    df_print: paged
subtitle: TMA4268 Statistical Learning V2023
urlcolor: blue
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=5.5, fig.height=3.5)
```

---

## Problem 1 -- Theoretical

a) Provide a detailed explanation of the algorithm that is used to fit a regression tree. What is different for a classification tree? 

b) What are the advantages and disadvantages of regression and classification trees?

c) What is the idea behind bagging and what is the role of bootstap? How do random forests improve that idea?

d) What is an out-of bag (OOB) error estimator and what percentage of observations are included in an OOB sample? (Hint: The result from RecEx5-Problem 4c can be used)

e) Bagging and Random Forests typically improve the prediction accuracy of a single tree, but it can be difficult to interpret, for example in terms of understanding which predictors are how relevant. 
How can we evaluate the importance of the different predictors for these methods? 



## Problem 2 -- Regression (Book Ex. 8)

In the lab (8.3 Lab: Decision Trees), the Carseats dataset is used. 
This dataset comprises sales data for child car seats at 400 different stores. The dataset includes a mix of quantitative and qualitative predictors.

Key attributes of the dataset include:

- Sales: The number of car seats sold at each location, serving as the response variable for regression analyses.
- CompPrice: The price charged by competitors at each location.
- Income: The community income level where the store is located.
- Advertising: The budget for advertising in each location.
- Population: The population size in the region around the store.
- Price: The price of the car seats at each store.
- ShelveLoc: A qualitative variable indicating the quality of the shelving location at the store (Good, Medium, Bad).
- Age: The average age of the local population.
- Education: The education level at each location.
- Urban: A qualitative variable indicating whether the store is in an urban or rural location.
- US: A qualitative variable indicating whether the store is in the US or not.

In the lab, a classification tree was applied to the Carseats dataset after converting the variable `Sales` into a qualitative response variable. 
Now we will seek to predict `Sales` using regression trees and related approaches, treating the response as a quantitative variable.

a) Split the data set into a training set and a test set. (Hint: Use 70% of the data as training set and the rest 30% as testing set)

**R-hints**
```{r,eval=F,echo=T}
library(ISLR)
data("Carseats")
set.seed(4268)
n = nrow(Carseats)
train = sample(1:n, 0.7*nrow(Carseats), replace = F)
test = ...
Carseats.train = ...
Carseats.test = ...
```

b) Fit a regression tree to the training set. Plot the tree, and interpret the results. What test MSE do you obtain?

**R-hints**

```{r,eval=F,echo=T}
library(tree)
tree.mod = tree(Sales~., ..., data = ...)
```

c) Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?

**R-hints**

```{r, eval=F, echo=T}
set.seed(4268)
cv.Carseats  = cv.tree(...) 
tree.min =  which.min(...)
best = cv.Carseats$size[...]
```

d) Use the bagging approach with 500 trees in order to analyze the data. What test MSE do you obtain? Use the `importance()` function to determine which variables are most important.

**R-hints**

```{r, eval=F, echo=T}
library(randomForest)
dim(Carseats)
bag.Carseats = randomForest(Sales~., ... , ntree = 500, importance = TRUE)
```

e) Use random forests and to analyze the data. Include 500 trees and select 3 variables for each split. What test MSE do you obtain? Use the `importance()` function to determine which variables are most important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained.

**R-hints**

```{r, eval=F, echo=T}
rf.Carseats = randomForest(Sales~., ... , mtry = 3, ntree = 500, importance = TRUE)
```


f) Finally use boosting with 500 trees, an interaction depth $d=4$ and a shrinkage factor $\lambda=0.1$ (default in the `gbm()` function) on our data. Compare the MSE to all other methods.

**R-hints**

```{r, eval=F, echo=T}
library(gbm)
r.boost=gbm(Sales~., Carseats.train,
                 distribution=...,
                 n.trees= ... ,interaction.depth= ..., shrinkage = ...)
```

g) What is the effect of the number of trees (`ntree`) on the test error? Plot the test MSE as a function of `ntree` for both the bagging and the random forest method.


## Problem 3 -- Classification

In this exercise you are going to implement a spam filter for e-mails by using tree-based methods. Data from 4601 e-mails are collected and can be uploaded from the kernlab library as follows:
```{r}
library(kernlab)
data(spam)
```
Each e-mail is classified by `type` ( `spam` or `nonspam`), and this will be the response in our model. In addition there are 57 predictors in the dataset. The predictors describe the frequency of different words in the e-mails and orthography (capitalization, spelling, punctuation and so on).

a) Study the dataset by writing `?spam` in R.

b) Create a training set and a test set for the dataset. (Hint: Use 70% of the data as training set and the rest 30% as testing set)

c) Fit a tree to the training data with `type` as the response and the rest of the variables as predictors. Study the results by using the `summary()` function. Also create a plot of the tree. How many terminal nodes does it have?
  
d) Predict the response on the test data. What is the misclassification rate?
  
e) Use the `cv.tree()` function to find the optimal tree size. Prune the tree according to the optimal tree size by using the `prune.misclass()` function and plot the result. Predict the response on the test data by using the pruned tree. What is the misclassification rate in this case?
  
f) Create a decision tree by using the bagging approach with $B=500$. Use the function `randomForest()` and consider all of the predictors in each split. Predict the response on the test data and report the misclassification rate.

g) Apply the `randomForest()` function again with 500 trees, but this time consider only a subset of the predictors in each split. This corresponds to the random forest-algorithm. Study the importance of each variable by using the function `importance()`. Are the results as expected based on earlier results? Again, predict the response for the test data and report the misclassification rate.

h) Use `gbm()` to construct a boosted classification tree using 5000 trees, an interaction depth of $d=3$ and a shrinkage parameter of $\lambda=0.001$. Predict the response for the test data and report the misclassification rate.

i) Compare the misclassification rates in d-h. Which method gives the lowest misclassification rate for the test data? Are the results as expected?
