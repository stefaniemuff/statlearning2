% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Module 9: Recommended Exercises},
  pdfauthor={Kenneth Aase, Daesoo Lee, Sara Martino, Stefanie Muff; Department of Mathematical Sciences, NTNU},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}

\title{Module 9: Recommended Exercises}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{TMA4268 Statistical Learning V2024}
\author{Kenneth Aase, Daesoo Lee, Sara Martino, Stefanie
Muff \and Department of Mathematical Sciences, NTNU}
\date{March 14, 2024}

\begin{document}
\maketitle

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{We recommend you to work through the Section 8.3.4 in the course
book (Lab: Boosting)}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\hypertarget{problem-1}{%
\subsection{Problem 1}\label{problem-1}}

Both \textbf{bagging} and \textbf{boosting} are \emph{ensemble} methods.

\begin{itemize}
\tightlist
\item
  What is an ensemble method?
\item
  What are the main conceptual differences between bagging and boosting?
\end{itemize}

\hypertarget{problem-2}{%
\subsection{Problem 2}\label{problem-2}}

Consider the Gradient Tree Boosting Algorithm (given on slide 35 in this
week's lectures).

Explain the meaning behind each of the following symbols. If relevant,
also explain what the impact of changing them will be, and explain how
one might find or choose them.

\begin{itemize}
\tightlist
\item
  \(L(.)\)
\item
  \(f_m(.)\)
\item
  \(M\)
\item
  \(J_m\)
\end{itemize}

\hypertarget{problem-3---learning-rate}{%
\subsection{Problem 3 - Learning rate}\label{problem-3---learning-rate}}

The algorithm mentioned in Problem 2 does not include the so-called
\emph{learning rate} parameter \(\nu\). In the context of boosting
methods, what is the interpretation behind this parameter, and how would
one choose it? If you were to include the parameter in the Gradient Tree
Boosting Algorithm, which equation(s) in the algorithm would you change
and how?

\hypertarget{problem-4---boosting-methods}{%
\subsection{Problem 4 - Boosting
methods}\label{problem-4---boosting-methods}}

In the code chunk below we simulate data meant to mimic a genomic data
set for a set of markers (locations in the human genome) that underlie a
complex trait (such as height, or whether you have cancer or not). You
don't need to understand the details of the simulation, but the main
takeaways are

\begin{itemize}
\tightlist
\item
  \(y\) is a continuous response, representing some biological trait
  (for example height) depending on both genetic (the predictors) and
  environmental (here included as random noise) factors.
\item
  We have many more predictors (genetic markers) \(M\) than observations
  (individuals) \(N\).
\item
  The predictors have a complicated structure: most of them have a very
  small effect on \(y\), but a few of them have a larger effect.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("DirichletReg")}
\CommentTok{\# install.packages("magrittr")}
\FunctionTok{library}\NormalTok{(DirichletReg)}
\FunctionTok{library}\NormalTok{(magrittr)}

\CommentTok{\# Simulate }
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{1000} \CommentTok{\# number of individuals}
\NormalTok{M }\OtherTok{\textless{}{-}} \DecValTok{10000} \CommentTok{\# number of genotyped markers}
\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{rbeta}\NormalTok{(}\AttributeTok{n =}\NormalTok{ M, }\AttributeTok{shape1 =} \FloatTok{2.3}\NormalTok{, }\AttributeTok{shape2 =} \FloatTok{7.4}\NormalTok{) }\CommentTok{\# Allele frequencies}
\NormalTok{p }\SpecialCharTok{\%\textless{}\textgreater{}\%} \FunctionTok{ifelse}\NormalTok{(. }\SpecialCharTok{\textless{}} \FloatTok{0.5}\NormalTok{, ., }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ .)}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(p, rbinom, }\AttributeTok{n =}\NormalTok{ N, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\CommentTok{\# genotype matrix}

\NormalTok{effect\_weights }\OtherTok{\textless{}{-}} \FunctionTok{rdirichlet}\NormalTok{(M, }\FunctionTok{c}\NormalTok{(}\FloatTok{0.9}\NormalTok{, }\FloatTok{0.08}\NormalTok{, }\FloatTok{0.015}\NormalTok{, }\FloatTok{0.005}\NormalTok{))}

\NormalTok{marker\_effects }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(effect\_weights, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(w) \{}
\NormalTok{  effects }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{sapply}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{1e{-}4}\NormalTok{, }\FloatTok{1e{-}3}\NormalTok{, }\FloatTok{1e{-}2}\NormalTok{)), rnorm, }\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{))}
  \FunctionTok{sum}\NormalTok{(w }\SpecialCharTok{*}\NormalTok{ effects)}
\NormalTok{\})}

\NormalTok{genetics }\OtherTok{\textless{}{-}} \FunctionTok{as.vector}\NormalTok{(X }\SpecialCharTok{\%*\%}\NormalTok{ marker\_effects)}
\NormalTok{environment }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(N, }\AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{var}\NormalTok{(genetics) }\SpecialCharTok{/} \DecValTok{2}\NormalTok{))}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ genetics }\SpecialCharTok{+}\NormalTok{ environment}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(y, }\FunctionTok{as.data.frame}\NormalTok{(X))}
\end{Highlighting}
\end{Shaded}

We will now try to predict \(y\) using various tree boosting methods,
adapted from code provided in
\url{https://bradleyboehmke.github.io/HOML/gbm.html}.

\hypertarget{a-a-basic-gbm}{%
\subsubsection{a) A basic GBM}\label{a-a-basic-gbm}}

We first implement a standard gradient boosting tree using the package
\texttt{gbm}. To make sure everything works, we run a very small
(\(M=3\)) model below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("dplyr")}
\FunctionTok{library}\NormalTok{(dplyr)    }\CommentTok{\# for general data wrangling needs}

\CommentTok{\# Modeling packages}
\CommentTok{\# install.packages("gbm")}
\CommentTok{\# install.packages("h2o")}
\CommentTok{\# install.packages("xgboost")}
\FunctionTok{library}\NormalTok{(gbm)      }\CommentTok{\# for original implementation of regular and stochastic GBMs}
\FunctionTok{library}\NormalTok{(h2o)      }\CommentTok{\# for a java{-}based implementation of GBM variants}
\FunctionTok{library}\NormalTok{(xgboost)  }\CommentTok{\# for fitting extreme gradient boosting}

\NormalTok{gbm1 }\OtherTok{\textless{}{-}} \FunctionTok{gbm}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{data =}\NormalTok{ dat,}
  \AttributeTok{distribution =} \StringTok{"gaussian"}\NormalTok{,  }\CommentTok{\# SSE loss function}
  \AttributeTok{n.trees =} \DecValTok{3}\NormalTok{,}
  \AttributeTok{shrinkage =} \FloatTok{0.1}\NormalTok{,}
  \AttributeTok{interaction.depth =} \DecValTok{7}\NormalTok{,}
  \AttributeTok{n.minobsinnode =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{cv.folds =} \DecValTok{10}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We note that running the above code block is very slow - even for a
\emph{very} low number of trees \(M=3\) (\(M\) is typically in the
thousands). How could we modify the algorithm to be less computationally
intensive?

\hypertarget{b-stochastic-gbms}{%
\subsubsection{b) Stochastic GBMs}\label{b-stochastic-gbms}}

We now move on to stochastic gradient boosting tree models for the same
data set, as implemented in the \texttt{h2o} package. Explain what is
done in the below code (\texttt{?h2o.grid}, \texttt{h2o.gbm} and
\url{https://bradleyboehmke.github.io/HOML/gbm.html} might be helpful).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set maximum memory usage}
\FunctionTok{h2o.init}\NormalTok{(}\AttributeTok{max\_mem\_size =} \StringTok{"14g"}\NormalTok{)}

\CommentTok{\# refined hyperparameter grid}
\NormalTok{hyper\_grid }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \AttributeTok{sample\_rate =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.5}\NormalTok{),              }\CommentTok{\# row subsampling}
  \AttributeTok{col\_sample\_rate =} \FloatTok{0.1}\NormalTok{,          }\CommentTok{\# col subsampling for each split}
  \AttributeTok{col\_sample\_rate\_per\_tree =} \FloatTok{0.1}\NormalTok{,  }\CommentTok{\# col subsampling for each tree}
  \AttributeTok{min\_rows =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{learn\_rate =} \FloatTok{0.05}\NormalTok{,}
  \AttributeTok{max\_depth =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{),}
  \AttributeTok{ntrees =} \DecValTok{10000}
\NormalTok{)}

\CommentTok{\# random grid search strategy}
\NormalTok{search\_criteria }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \AttributeTok{strategy =} \StringTok{"RandomDiscrete"}\NormalTok{,}
  \AttributeTok{stopping\_metric =} \StringTok{"mse"}\NormalTok{,}
  \AttributeTok{stopping\_tolerance =} \FloatTok{0.001}\NormalTok{,}
  \AttributeTok{stopping\_rounds =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{max\_runtime\_secs =} \DecValTok{20}\SpecialCharTok{*}\DecValTok{60}
\NormalTok{)}

\CommentTok{\# perform grid search }
\NormalTok{grid }\OtherTok{\textless{}{-}} \FunctionTok{h2o.grid}\NormalTok{(}
  \AttributeTok{algorithm =} \StringTok{"gbm"}\NormalTok{,}
  \AttributeTok{y =} \StringTok{"y"}\NormalTok{,}
  \AttributeTok{training\_frame =} \FunctionTok{as.h2o}\NormalTok{(dat),}
  \AttributeTok{hyper\_params =}\NormalTok{ hyper\_grid,}
  \AttributeTok{nfolds =} \DecValTok{5}\NormalTok{,}
  \AttributeTok{search\_criteria =}\NormalTok{ search\_criteria,}
  \AttributeTok{seed =} \DecValTok{123}\NormalTok{,}
  \AttributeTok{parallelism =} \DecValTok{0}
\NormalTok{)}

\CommentTok{\# collect the results and sort by our model performance metric of choice}
\NormalTok{grid\_perf }\OtherTok{\textless{}{-}} \FunctionTok{h2o.getGrid}\NormalTok{(}
  \AttributeTok{grid\_id =}\NormalTok{ grid}\SpecialCharTok{@}\NormalTok{grid\_id,}
  \AttributeTok{sort\_by =} \StringTok{"mse"}\NormalTok{,}
  \AttributeTok{decreasing =} \ConstantTok{FALSE}
\NormalTok{)}

\NormalTok{grid\_perf}
\end{Highlighting}
\end{Shaded}

\hypertarget{c-xgboost}{%
\subsubsection{c) XGboost}\label{c-xgboost}}

Below we also provide code for applying cross-validated XGBoost to the
same data set. Expand this code to perform a search of the
hyperparameter space, similar to b). Finally, experimenting with the
code from a), b) and c), what is the best model you are able to find?
Fit that model using all available training data (no cross-validation).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{xgb }\OtherTok{\textless{}{-}} \FunctionTok{xgb.cv}\NormalTok{(}
  \AttributeTok{data =}\NormalTok{ X,}
  \AttributeTok{label =}\NormalTok{ y,}
  \AttributeTok{nrounds =} \DecValTok{6000}\NormalTok{,}
  \AttributeTok{early\_stopping\_rounds =} \DecValTok{50}\NormalTok{, }
  \AttributeTok{nfold =} \DecValTok{5}\NormalTok{,}
  \AttributeTok{params =} \FunctionTok{list}\NormalTok{(}
    \AttributeTok{eta =} \FloatTok{0.05}\NormalTok{,}
    \AttributeTok{max\_depth =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{min\_child\_weight =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{subsample =} \FloatTok{0.2}\NormalTok{,}
    \AttributeTok{colsample\_bytree =} \FloatTok{0.1}\NormalTok{),}
  \AttributeTok{verbose =} \ConstantTok{TRUE}
\NormalTok{)}

\CommentTok{\# minimum test CV RMSE}
\FunctionTok{min}\NormalTok{(xgb}\SpecialCharTok{$}\NormalTok{evaluation\_log}\SpecialCharTok{$}\NormalTok{test\_rmse\_mean)}
\end{Highlighting}
\end{Shaded}


\end{document}
