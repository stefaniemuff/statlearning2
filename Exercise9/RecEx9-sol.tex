% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Module 9: Recommended Exercises (Solution)},
  pdfauthor={Kenneth Aase, Sara Martino, Stefanie Muff; Department of Mathematical Sciences, NTNU},
  colorlinks=true,
  linkcolor={Maroon},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={blue},
  pdfcreator={LaTeX via pandoc}}

\title{Module 9: Recommended Exercises (Solution)}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{TMA4268 Statistical Learning V2025}
\author{Kenneth Aase, Sara Martino, Stefanie Muff \and Department of
Mathematical Sciences, NTNU}
\date{March 19, 2025}

\begin{document}
\maketitle

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\textbf{We recommend you to work through the Section 8.3.4 in the course
book (Lab: Boosting)}

\begin{center}\rule{0.5\linewidth}{0.5pt}\end{center}

\subsection{Problem 1}\label{problem-1}

Both \textbf{bagging} and \textbf{boosting} are \emph{ensemble} methods.

\emph{Solution}:

\begin{itemize}
\tightlist
\item
  What is an ensemble method?
\end{itemize}

A statistical learning model that combines the output from many
statistical learning models, to perform better than any of the models
individually.

\begin{itemize}
\tightlist
\item
  What are the main conceptual differences between bagging and boosting?
\end{itemize}

Bagging: Bootstrap aggregation. Based on building many models
\emph{independently of each other}, and combining the results. Boosting:
Based on building many models \emph{sequentially}, where we let each
model improve the fit for the parts of the problem that the model in the
previous iteration struggled with.

\subsection{Problem 2}\label{problem-2}

Consider the Gradient Tree Boosting Algorithm (given on slide 34 in this
week's lectures).

Explain the meaning behind each of the following symbols. If relevant,
also explain what the impact of changing them will be, and explain how
one might find or choose them.

\emph{Solution}:

\begin{itemize}
\tightlist
\item
  \(L(.)\)
\end{itemize}

This is the loss function we are trying to minimize when fitting the
base learners in each iteration. Our choice of loss function will of
course depend on the type of data we are modelling (i.e., regression
vs.~classification), but the choice can also be highly impactful on the
robustness of our method - for example, the quadratic loss function
might place too much weight on extreme observations.

\begin{itemize}
\tightlist
\item
  \(f_m(.)\)
\end{itemize}

This is our gradient boosted tree at the \(m^{\text{th}}\) iteration of
the algorithm. It is found by fitting a tree to the pseudo-residuals
\(r_{im}\), and within the terminal regions of that tree, adding a
``correction'' (\(\gamma_{jm}\) for terminal region \(j\)) to the
current \(f_{m-1}\) in a way that minimizes the loss function \(L\). For
details on how to fit the individual trees, see the previous module.

\begin{itemize}
\tightlist
\item
  \(M\)
\end{itemize}

This is the number of trees (base learners) that we fit. This is a
hyperparameter that should be tuned with e.g.~cross-validation, or
chosen adaptively with some stopping criterion. Too low \(M\) leads to
underfitting (we don't have enough base learners to capture the
structure in the data), while too high \(M\) leads to overfitting.

\begin{itemize}
\tightlist
\item
  \(J_m\)
\end{itemize}

This is the number of terminal regions of the tree (base learner) we fit
in iteration \(m\). \(J_m\) will be determined by the procedure we use
to fit each base learner, but we commonly constrain what the maximum
value of \(J_m\) can be. This constraint (the maximum value of \(J_m\))
is a hyperparameter that should be tuned with e.g.~cross-validation. The
lower we set the maximum allowed \(J_m\) to be, the less complex each
tree will be - i.e.~the simpler the base learners in the ensemble will
be. And vice versa. More complicated base learners will also mean a
higher computational cost per iteration.

\subsection{Problem 3 - Learning rate}\label{problem-3---learning-rate}

The algorithm mentioned in Problem 2 does not include the so-called
\emph{learning rate} parameter \(\nu\). In the context of boosting
methods, what is the interpretation behind this parameter, and how would
one choose it? If you were to include the parameter in the Gradient Tree
Boosting Algorithm, which equation(s) in the algorithm would you change
and how?

\emph{Solution}:

The learning rate \(0<\nu<1\) is a factor that we use to scale the base
learner at each iteration of the boosting algorithm. Thus, the lower its
value, the less impact each base learner will have on the ensemble.
Empirically, this parameter is often found to be crucial in determining
the accuracy of the model. The smaller the value of this parameter, the
higher we need \(M\) to be, since we slow down the learning. \(\nu\) can
be chosen by cross-validation, or set to a fixed (small) value if \(M\)
is chosen adaptively (see above).

The learning rate would be added to step 2d) on lecture slide 35
(Algorithm 10.3 in ``The Elements of Statistical Learning''), so that it
would read

\[
\text{Update} \quad f_m(x)=f_{m-1}(x) + \nu \sum_{j=1}^{J_m} \gamma_{jm}I(x\in R_{jm}).
\]

\subsection{Problem 4 - Boosting
methods}\label{problem-4---boosting-methods}

In the code chunk below we simulate data meant to mimic a genomic data
set for a set of markers (locations in the human genome) that underlie a
complex trait (such as height, or whether you have cancer or not). You
don't need to understand the details of the simulation, but the main
takeaways are

\begin{itemize}
\tightlist
\item
  \(y\) is a continuous response, representing some biological trait
  (for example height) depending on both genetic (the predictors) and
  environmental (here included as random noise) factors.
\item
  We have many more predictors (genetic markers) \(M\) than observations
  (individuals) \(N\).
\item
  The predictors have a complicated structure: most of them have a very
  small effect on \(y\), but a few of them have a larger effect.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("DirichletReg")}
\CommentTok{\# install.packages("magrittr")}
\FunctionTok{library}\NormalTok{(DirichletReg)}
\FunctionTok{library}\NormalTok{(magrittr)}

\CommentTok{\# Simulate }
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\NormalTok{N }\OtherTok{\textless{}{-}} \DecValTok{1000} \CommentTok{\# number of individuals}
\NormalTok{M }\OtherTok{\textless{}{-}} \DecValTok{10000} \CommentTok{\# number of genotyped markers}
\NormalTok{p }\OtherTok{\textless{}{-}} \FunctionTok{rbeta}\NormalTok{(}\AttributeTok{n =}\NormalTok{ M, }\AttributeTok{shape1 =} \FloatTok{2.3}\NormalTok{, }\AttributeTok{shape2 =} \FloatTok{7.4}\NormalTok{) }\CommentTok{\# Allele frequencies}
\NormalTok{p }\SpecialCharTok{\%\textless{}\textgreater{}\%} \FunctionTok{ifelse}\NormalTok{(. }\SpecialCharTok{\textless{}} \FloatTok{0.5}\NormalTok{, ., }\DecValTok{1} \SpecialCharTok{{-}}\NormalTok{ .)}
\NormalTok{X }\OtherTok{\textless{}{-}} \FunctionTok{sapply}\NormalTok{(p, rbinom, }\AttributeTok{n =}\NormalTok{ N, }\AttributeTok{size =} \DecValTok{2}\NormalTok{) }\CommentTok{\# genotype matrix}

\NormalTok{effect\_weights }\OtherTok{\textless{}{-}} \FunctionTok{rdirichlet}\NormalTok{(M, }\FunctionTok{c}\NormalTok{(}\FloatTok{0.9}\NormalTok{, }\FloatTok{0.08}\NormalTok{, }\FloatTok{0.015}\NormalTok{, }\FloatTok{0.005}\NormalTok{))}

\NormalTok{marker\_effects }\OtherTok{\textless{}{-}} \FunctionTok{apply}\NormalTok{(effect\_weights, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(w) \{}
\NormalTok{  effects }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FunctionTok{sapply}\NormalTok{(}\FunctionTok{sqrt}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\FloatTok{1e{-}4}\NormalTok{, }\FloatTok{1e{-}3}\NormalTok{, }\FloatTok{1e{-}2}\NormalTok{)), rnorm, }\AttributeTok{n =} \DecValTok{1}\NormalTok{, }\AttributeTok{mean =} \DecValTok{0}\NormalTok{))}
  \FunctionTok{sum}\NormalTok{(w }\SpecialCharTok{*}\NormalTok{ effects)}
\NormalTok{\})}

\NormalTok{genetics }\OtherTok{\textless{}{-}} \FunctionTok{as.vector}\NormalTok{(X }\SpecialCharTok{\%*\%}\NormalTok{ marker\_effects)}
\NormalTok{environment }\OtherTok{\textless{}{-}} \FunctionTok{rnorm}\NormalTok{(N, }\AttributeTok{sd =} \FunctionTok{sqrt}\NormalTok{(}\FunctionTok{var}\NormalTok{(genetics) }\SpecialCharTok{/} \DecValTok{2}\NormalTok{))}
\NormalTok{y }\OtherTok{\textless{}{-}}\NormalTok{ genetics }\SpecialCharTok{+}\NormalTok{ environment}
\NormalTok{dat }\OtherTok{\textless{}{-}} \FunctionTok{cbind}\NormalTok{(y, }\FunctionTok{as.data.frame}\NormalTok{(X))}

\CommentTok{\# Looking at a few entries in the data:}
\NormalTok{dat[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{6}\NormalTok{,}\DecValTok{1}\SpecialCharTok{:}\DecValTok{20}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            y V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 V18 V19
## 1 -0.6553370  0  1  0  0  0  0  1  0  0   0   0   0   1   1   1   1   1   2   2
## 2 -0.6780554  1  0  0  2  0  2  1  0  2   0   0   0   0   0   0   0   0   1   1
## 3 -0.2311866  0  1  0  0  1  0  0  0  1   1   0   0   1   0   1   0   0   1   1
## 4 -1.0386650  2  0  0  1  0  1  0  0  1   0   1   0   1   0   0   0   0   0   0
## 5 -0.4776310  0  1  1  1  0  0  0  2  2   2   2   0   2   1   1   0   1   1   0
## 6 -0.4600658  0  0  1  1  1  0  0  1  1   0   0   0   0   0   0   0   0   1   1
\end{verbatim}

We will now try to predict \(y\) using various tree boosting methods,
adapted from code provided in
\url{https://bradleyboehmke.github.io/HOML/gbm.html}.

\subsubsection{a) A basic GBM}\label{a-a-basic-gbm}

We first implement a standard gradient boosting tree using the package
\texttt{gbm}. To make sure everything works, we run a very small
(\(M=3\)) model below.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("dplyr")}
\FunctionTok{library}\NormalTok{(dplyr)    }\CommentTok{\# for general data wrangling needs}

\CommentTok{\# Modeling packages}
\CommentTok{\# install.packages("gbm")}
\CommentTok{\# install.packages("h2o")}
\CommentTok{\# install.packages("xgboost")}
\FunctionTok{library}\NormalTok{(gbm)      }\CommentTok{\# for original implementation of regular and stochastic GBMs}
\FunctionTok{library}\NormalTok{(h2o)      }\CommentTok{\# for a java{-}based implementation of GBM variants}
\FunctionTok{library}\NormalTok{(xgboost)  }\CommentTok{\# for fitting extreme gradient boosting}

\NormalTok{gbm1 }\OtherTok{\textless{}{-}} \FunctionTok{gbm}\NormalTok{(}
  \AttributeTok{formula =}\NormalTok{ y }\SpecialCharTok{\textasciitilde{}}\NormalTok{ .,}
  \AttributeTok{data =}\NormalTok{ dat,}
  \AttributeTok{distribution =} \StringTok{"gaussian"}\NormalTok{,  }\CommentTok{\# SSE loss function}
  \AttributeTok{n.trees =} \DecValTok{3}\NormalTok{,}
  \AttributeTok{shrinkage =} \FloatTok{0.1}\NormalTok{,}
  \AttributeTok{interaction.depth =} \DecValTok{7}\NormalTok{,}
  \AttributeTok{n.minobsinnode =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{cv.folds =} \DecValTok{10}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We note that running the above code block is very slow - even for a
\emph{very} low number of trees \(M=3\) (\(M\) is typically in the
thousands). How could we modify the algorithm to be less computationally
intensive?

\emph{Solution}:

We could for example:

\begin{itemize}
\tightlist
\item
  We could reduce the allowed complexity of each base learner (reduce
  the interaction depth).
\item
  We could subsample observations (like in bagging) and/or predictors
  (like in random forest). In other words, go for a stochastic gradient
  boosting approach.
\item
  Change our validation approach. The above code does a 10-fold cross
  validation for each \(M\) up to \(3\). To save on computations, we
  could go to 5-fold CV, or even a training/test set approach.
\end{itemize}

\subsubsection{b) Stochastic GBMs}\label{b-stochastic-gbms}

We now move on to stochastic gradient boosting tree models for the same
data set, as implemented in the \texttt{h2o} package. Explain what is
done in the below code (\texttt{?h2o.grid}, \texttt{h2o.gbm} and
\url{https://bradleyboehmke.github.io/HOML/gbm.html} might be helpful).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Set maximum memory usage}
\FunctionTok{h2o.init}\NormalTok{(}\AttributeTok{max\_mem\_size =} \StringTok{"14g"}\NormalTok{)}

\CommentTok{\# refined hyperparameter grid}
\NormalTok{hyper\_grid }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \AttributeTok{sample\_rate =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.5}\NormalTok{),              }\CommentTok{\# row subsampling}
  \AttributeTok{col\_sample\_rate =} \FloatTok{0.1}\NormalTok{,          }\CommentTok{\# col subsampling for each split}
  \AttributeTok{col\_sample\_rate\_per\_tree =} \FloatTok{0.1}\NormalTok{,  }\CommentTok{\# col subsampling for each tree}
  \AttributeTok{min\_rows =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{learn\_rate =} \FloatTok{0.05}\NormalTok{,}
  \AttributeTok{max\_depth =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{),}
  \AttributeTok{ntrees =} \DecValTok{10000}
\NormalTok{)}

\CommentTok{\# random grid search strategy}
\NormalTok{search\_criteria }\OtherTok{\textless{}{-}} \FunctionTok{list}\NormalTok{(}
  \AttributeTok{strategy =} \StringTok{"RandomDiscrete"}\NormalTok{,}
  \AttributeTok{stopping\_metric =} \StringTok{"mse"}\NormalTok{,}
  \AttributeTok{stopping\_tolerance =} \FloatTok{0.001}\NormalTok{,}
  \AttributeTok{stopping\_rounds =} \DecValTok{10}\NormalTok{,}
  \AttributeTok{max\_runtime\_secs =} \DecValTok{20}\SpecialCharTok{*}\DecValTok{60}
\NormalTok{)}

\CommentTok{\# perform grid search }
\NormalTok{grid }\OtherTok{\textless{}{-}} \FunctionTok{h2o.grid}\NormalTok{(}
  \AttributeTok{algorithm =} \StringTok{"gbm"}\NormalTok{,}
  \AttributeTok{y =} \StringTok{"y"}\NormalTok{,}
  \AttributeTok{training\_frame =} \FunctionTok{as.h2o}\NormalTok{(dat),}
  \AttributeTok{hyper\_params =}\NormalTok{ hyper\_grid,}
  \AttributeTok{nfolds =} \DecValTok{5}\NormalTok{,}
  \AttributeTok{search\_criteria =}\NormalTok{ search\_criteria,}
  \AttributeTok{seed =} \DecValTok{123}\NormalTok{,}
  \AttributeTok{parallelism =} \DecValTok{0}
\NormalTok{)}

\CommentTok{\# collect the results and sort by our model performance metric of choice}
\NormalTok{grid\_perf }\OtherTok{\textless{}{-}} \FunctionTok{h2o.getGrid}\NormalTok{(}
  \AttributeTok{grid\_id =}\NormalTok{ grid}\SpecialCharTok{@}\NormalTok{grid\_id,}
  \AttributeTok{sort\_by =} \StringTok{"mse"}\NormalTok{,}
  \AttributeTok{decreasing =} \ConstantTok{FALSE}
\NormalTok{)}

\NormalTok{grid\_perf}
\end{Highlighting}
\end{Shaded}

\emph{Solution}:

We run a Stochastic gradient boosting tree with subsampling of both rows
(observations) and columns (predictors). In each tree we subsample
\(10\)\% of the predictors, and \(10\)\% of these are subsampled when
creating each split within a tree. So,
\(10 000 \cdot 0.1\cdot 0.1 = 100\) predictors are considered at each
split in the trees. The number of observations to sample for each tree
(\(20\)\% or \(50\)\%) is chosen by 5-fold cross validation, as is the
allowed complexity of each tree (\texttt{max\_depth}).

We fix the learning rate to a low value of \(\nu = 0.05\), and choose
\(M\) adaptively: we either stop the model if the (cross-validated) MSE
has not improved by \(0.001\) over the last \(10\) iterations, if we
reach \(10000\) iterations or if the runtime exceeds twenty minutes.

\subsubsection{c) XGboost}\label{c-xgboost}

Below we also provide code for applying cross-validated XGBoost to the
same data set. Expand this code to perform a search of the
hyperparameter space, similar to b).

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{xgb }\OtherTok{\textless{}{-}} \FunctionTok{xgb.cv}\NormalTok{(}
  \AttributeTok{data =}\NormalTok{ X,}
  \AttributeTok{label =}\NormalTok{ y,}
  \AttributeTok{nrounds =} \DecValTok{6000}\NormalTok{,}
  \AttributeTok{early\_stopping\_rounds =} \DecValTok{50}\NormalTok{, }
  \AttributeTok{nfold =} \DecValTok{5}\NormalTok{,}
  \AttributeTok{params =} \FunctionTok{list}\NormalTok{(}
    \AttributeTok{eta =} \FloatTok{0.05}\NormalTok{,}
    \AttributeTok{max\_depth =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{min\_child\_weight =} \DecValTok{3}\NormalTok{,}
    \AttributeTok{subsample =} \FloatTok{0.2}\NormalTok{,}
    \AttributeTok{colsample\_bytree =} \FloatTok{0.1}\NormalTok{),}
  \AttributeTok{verbose =} \ConstantTok{TRUE}
\NormalTok{)}

\CommentTok{\# minimum test CV RMSE}
\FunctionTok{min}\NormalTok{(xgb}\SpecialCharTok{$}\NormalTok{evaluation\_log}\SpecialCharTok{$}\NormalTok{test\_rmse\_mean)}
\end{Highlighting}
\end{Shaded}

\emph{Solution}:

Below we do a hyperparameter tuning, again adapted from
\url{https://bradleyboehmke.github.io/HOML/gbm.html}. Note that XGBoost
has \emph{many} parameters that can be tuned on a much finer grid than
what is done here.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# hyperparameter grid}
\NormalTok{hyper\_grid }\OtherTok{\textless{}{-}} \FunctionTok{expand.grid}\NormalTok{(}
  \AttributeTok{eta =} \FloatTok{0.05}\NormalTok{,}
  \AttributeTok{max\_depth =} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{7}\NormalTok{), }
  \AttributeTok{min\_child\_weight =} \DecValTok{3}\NormalTok{,}
  \AttributeTok{subsample =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.5}\NormalTok{), }
  \AttributeTok{colsample\_bytree =} \FloatTok{0.1}\NormalTok{,}
  \AttributeTok{rmse =} \DecValTok{0}\NormalTok{,          }\CommentTok{\# a place to dump RMSE results}
  \AttributeTok{trees =} \DecValTok{0}          \CommentTok{\# a place to dump required number of trees}
\NormalTok{)}

\CommentTok{\# grid search}
\ControlFlowTok{for}\NormalTok{(i }\ControlFlowTok{in} \FunctionTok{seq\_len}\NormalTok{(}\FunctionTok{nrow}\NormalTok{(hyper\_grid))) \{}
  \FunctionTok{set.seed}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\NormalTok{  m }\OtherTok{\textless{}{-}} \FunctionTok{xgb.cv}\NormalTok{(}
    \AttributeTok{data =}\NormalTok{ X,}
    \AttributeTok{label =}\NormalTok{ y,}
    \AttributeTok{nrounds =} \DecValTok{6000}\NormalTok{,}
    \AttributeTok{early\_stopping\_rounds =} \DecValTok{50}\NormalTok{, }
    \AttributeTok{nfold =} \DecValTok{5}\NormalTok{,}
    \AttributeTok{verbose =} \ConstantTok{TRUE}\NormalTok{,}
    \AttributeTok{params =} \FunctionTok{list}\NormalTok{( }
      \AttributeTok{eta =}\NormalTok{ hyper\_grid}\SpecialCharTok{$}\NormalTok{eta[i], }
      \AttributeTok{max\_depth =}\NormalTok{ hyper\_grid}\SpecialCharTok{$}\NormalTok{max\_depth[i],}
      \AttributeTok{min\_child\_weight =}\NormalTok{ hyper\_grid}\SpecialCharTok{$}\NormalTok{min\_child\_weight[i],}
      \AttributeTok{subsample =}\NormalTok{ hyper\_grid}\SpecialCharTok{$}\NormalTok{subsample[i],}
      \AttributeTok{colsample\_bytree =}\NormalTok{ hyper\_grid}\SpecialCharTok{$}\NormalTok{colsample\_bytree[i]}
\NormalTok{    ) }
\NormalTok{  )}
\NormalTok{  hyper\_grid}\SpecialCharTok{$}\NormalTok{rmse[i] }\OtherTok{\textless{}{-}} \FunctionTok{min}\NormalTok{(m}\SpecialCharTok{$}\NormalTok{evaluation\_log}\SpecialCharTok{$}\NormalTok{test\_rmse\_mean)}
\NormalTok{  hyper\_grid}\SpecialCharTok{$}\NormalTok{trees[i] }\OtherTok{\textless{}{-}}\NormalTok{ m}\SpecialCharTok{$}\NormalTok{best\_iteration}
\NormalTok{\}}

\NormalTok{hyper\_grid }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(rmse }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(rmse) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{glimpse}\NormalTok{()}

\CommentTok{\# minimum test CV RMSE}
\FunctionTok{min}\NormalTok{(xgb}\SpecialCharTok{$}\NormalTok{evaluation\_log}\SpecialCharTok{$}\NormalTok{test\_rmse\_mean)}
\end{Highlighting}
\end{Shaded}

\subsubsection{d) Model evaluation}\label{d-model-evaluation}

Finally, experimenting with the code from a), b) and c), what is the
best model you are able to find? Fit that model using all available
training data (no cross-validation).

\emph{Solution}:

The answer will depend on what models you decided to investigate, and
how you decided to tune the hyperparameters. The final fit should
involve a call to \texttt{gbm()}, \texttt{h2o.gbm()} or
\texttt{xgb.train()}.

Notice that here we have skipped a model assessment of the final model,
as we used all the available data to train the final model. If we were
in a data-rich situation we might have held out a independent test set
until now to check the accuracy of the final model. Or, if we had a lot
of computing power, we might have done a nested cross-validation to
perform the model assessment.

\end{document}
