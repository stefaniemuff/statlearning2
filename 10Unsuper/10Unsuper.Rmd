---
subtitle: "TMA4268 Statistical Learning V2023"
title: "Module 10: Unsupervised learning (Overview/quizz lecture)"
author: "Stefanie Muff, Department of Mathematical Sciences, NTNU"
date: "March 23, 2023"
fontsize: 10pt
output:
  beamer_presentation:
    keep_tex: yes
    fig_caption: false
    latex_engine: xelatex
    theme: "Singapore"
    colortheme: "default"
    font: "serif"
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
urlcolor: blue
bibliography: ../refs.bib
header-includes: \usepackage{multirow}

---


```{r, echo = FALSE}

library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = FALSE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize")

```

```{r}
library(tidyverse)
library(ISLR)
library(ggplot2)
library(GGally)
library(patchwork)
```

### US Arrest Example

$~$


```{r}

data(USArrests)

head(USArrests)

```

$~$

Scales:

- Number of occurrence per 100 000 people
- Percentage 

---

### US Arrest Example

$~$


```{r}

USArrests %>% mutate(Murder = Murder/1000, Assault = Assault/1000, Rape = Rape/1000) %>% head()

```
$~$

Scales:

- Number of occurrence per 100 people
- Percentage 

$~$

Scales:

- Number of occurrence per 100  people
- Percentage 

---



### PC loadings vectors $\Phi$

$~$

```{r}
pca1 = prcomp(USArrests, scale = FALSE)
pca2 = prcomp(USArrests%>% mutate(Murder = Murder/1000, Assault = Assault/1000, Rape = Rape/1000), scale = FALSE)
knitr::kable(
  list(pca1$rotation, pca2$rotation),
  digits = 4
)

```


$~$



---

### The biplot

$~$

\centering
```{r,  fig.show="hold", out.width="40%"}
biplot(pca1, scale = 0)
biplot(pca2, scale = 0)
```


--- 

### Example from Compulsory 3, 2020

$~$

* We study the `decathlon2` dataset from the `factoextra` package in R, where Athletes' performance during a sporting meeting was recorded. 

* We look at 23 athletes and the results from the 10 disciplines in two competitions.

```{r,eval=T,echo=F}
library(factoextra)
library(FactoMineR)
data("decathlon2")
decathlon2.active <- decathlon2[1:23, 1:10]
names(decathlon2.active) <- c("100m","long_jump","shot_put","high_jump","400m","110.hurdle","discus","pole_vault","javeline","1500m")
```

$~$

\scriptsize
```{r}
decathlon2.active[c(1,3,4),]
```

---

```{r biplot,eval=T,echo=F,fig.width=7,fig.height=7,out.width="55%",fig.cap=""}
r.prcomp <- prcomp(decathlon2.active, scale=T)

biplot(r.prcomp)
```

---

### Scree plot

$~$

A graphical description of the **proportion of variance explained (PVE)** by a certain number of PCs (see Fig 12.3 from @ISL):

\centering
<!-- ![](123.png){width=90%} -->

---

### Proportion of varianced explained (PVE)

$~$

**Recap:** The PVE by PC $m$ is given by 

$$
\frac{\sum_{i=1}^m z_{im}^2} {\sum_{j=1}^p\sum_{i=1}^n x_{ij}^2}
$$

---

# Clustering

$~$

* The aim is to find _clusters_ or _subgroups_.

* Clustering looks for homogeneous subgroups in the data.



$~$

Difference to PCA?

\pause

$\rightarrow$ PCA looks for low-dimensional representation of the data.

---

### K-means vs. hierarchical clustering

$~$

See menti.com



---

### K-means clustering

$~$

* Fix the number of clusters $K$.

$~$

* Find groups such that the sum of the within-cluster variation is minimized.

$~$

* Algorithm?

---

<!-- ![](kmeans_alg.png) -->

---

\centering
<!-- ![](fig12_8.png){width=75%} -->
\flushleft
\small
(Fig 12.8 from course book)

---

### Hierarchical clustering

$~$

Bottom-up agglomerative clustering that results in a _\textcolor{red}{dendogram}_.

$~$

<!-- ![](hierclust.png){width=90%} -->

---

### Important in hierarchical clustering

$~$

* _\textcolor{red}{Linkage:}_ Complete, single, average centroid.

$~$

* _\textcolor{red}{Dissimilarity measure:}_ Euclidian distance, correlation. _Other similarity/distance measures?_ \footnote{ Note: Correlation is actually a similarity measure, not a distance measure. Implication?}


---

### Hierarchical clustering -- example

$~$

<!-- ![](fig12_12.png) -->

Note: The representation on the right is not possible in high-dimensional space (i.e., if we have $X_1, X_2, X_3, ...., X_p$).

---

### Hierarchical clustering -- example

$~$

An exam question from 2022:

<!-- ![](exam_question3b_2023.png){with=110%} -->

---

## Pros and cons of clusterization methods / practical issues

$~$

$~$

$~$

$~$

$~$

$~$

$~$

$~$

$~$

$~$




---

# References