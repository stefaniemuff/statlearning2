---
subtitle: "TMA4268 Statistical Learning V2023"
title: "Module 12: Summing up and some cautionary notes"
author: "Stefanie Muff, Department of Mathematical Sciences, NTNU"
date: "May 04, 2023"
fontsize: 10pt
output:
  beamer_presentation:
    keep_tex: yes
    fig_caption: false
    theme: "Singapore"
    colortheme: "default"
    font: "serif"
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
urlcolor: blue
bibliography: refs.bib
header-includes: \usepackage{multirow}

---

```{r setup, include=FALSE}
showsol<-TRUE
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize")
whichformat="latex"
```


--- 

# Overview

$~$

* Course content and learning outcome
\vspace{2mm}

* Overview of modules and core course topics
\vspace{2mm}

* Some cautionary notes

---

Some of the figures and slides in this presentation are taken (or are inspired) from @james.etal2 .


<!-- --- -->

<!-- # Course content -->
<!-- Statistical learning, multiple linear regression, classification, resampling methods, model selection/regularization, non-linearity, support vector machines, tree-based methods, unsupervised methods, neural nets. -->

---

# Learning outcomes of TMA4268

1. **Knowledge.** The student has knowledge about the most popular statistical learning models and methods that are used for _prediction_ and _inference_ in science and technology. Emphasis is on regression- and classification-type statistical models.

2. **Skills.** The student can, based on an existing data set, _choose a suitable statistical model, apply sound statistical methods_, and _perform the analyses using statistical software_. The student can present, interpret and communicate the results from the statistical analyses, and knows which conclusions can be drawn from the analyses, and what are the caveats.  

$~$

**And**: you got to be an expert in using the R language and writing R Markdown reports.

---


# The exam

$~$

* **School exam** June 1st, 15-19h.

$~$

Tentative plan:

* Warm up section.
* 1-2 examples with real data.
* A conceptual/theoretical section testing your understanding.
* A section with single/multiple choice questions. 

$~$

**Check out the exam from 2022 and additional information on the course website:**

https://wiki.math.ntnu.no/tma4268/2023v/subpage16



---

# The course book

$~$

Attention: 

* We have now worked with the _second edition_ this year:
https://www.statlearning.com/

* This edition included a new chapter on Deep Learning, which we discussed in module 11.

\centering
![](deep_learning.png){width=50%}



---

# Core of the course

Supervised and unsupervised learning:

* _\textcolor{red}{Supervised}_: regression and classification
    + examples of regression and classification type problems
    + how complex a model to get the best fit?  
    $\rightarrow$ flexiblity/overfitting/underfitting.
    + the bias-variance trade-off
    + how to find a good fit - validation and cross-validation (or AIC-type solutions)
    + how to compare different solutions 
    + how to evaluate the fit - on new unseen data

\vspace{2mm}    
 * _\textcolor{red}{Unsupervised}_: how to find structure or groupings in data?

\vspace{2mm}  

and of course all **the methods** (with underlying models) to perform regression, classification and unsupervised learning. 


---

\centering
![](../../ISLR/Figures/Chapter2/2.7.png)
Figure 2.7 from @ISL

---

# The modules


## 1. Introduction 

$~$

* Examples, the modules, required background in statistics
 \vspace{2mm}
* Introduction to R and RStudio via the online R-course


---

## 2. Statistical learning 

$~$

* Model complexity
    + Prediction vs. interpretation (inference).  
    + Parametric vs. nonparametric. 
    + Flexible vs.\ inflexible.
    + Overfitting vs. underfitting
  \vspace{2mm}
* Supervised vs. unsupervised. 
 \vspace{2mm}
* Regression and classification.
 \vspace{2mm}
* Loss functions: quadratic and 0/1 loss.
 \vspace{2mm}
* Bias-variance trade-off (polynomial example): mean squared error, training and test set.
 \vspace{2mm}
* Vectors and matrices, rules for mean and covariances, the multivariate normal distribution.



---

## 3. Linear regression 

$~$

* The classical normal linear regression model on vector/matrix form.
 \vspace{2mm}

* Parameter estimators and distribution thereof. Model fit.
 \vspace{2mm}

* Confidence intervals, hypothesis tests, and interpreting R-output from regression.
 \vspace{2mm}

* Qualitative covariates, interactions.
 \vspace{2mm}

* This module is a stepping stone for all subsequent uses of regression in Modules 6, 7, 8, and 11.


---

## 4. Classification (Mainly two-class problems)
 
$~$

* Bayes classifier: classify to the most probable class to minimize the expected 0/1 loss. We usually do not know the probability of each class for each input. The Bayes optimal boundary is the boundary for the Bayes classifier and the error rate (on a test set) for the Bayes classifier is the Bayes error rate. 

$~$

* Two paradigms (not in textbook): 
    * _\textcolor{red}{Diagnostic}_ (directly estimating the posterior distribution for the classes). Example: KNN classifier, logistic regression.
    * _\textcolor{red}{Sampling}_ (estimating class prior probabilities and class conditional distribution and then putting together with Bayes rule). Examples: LDA, QDA with linear or quadratic class boundaries.
      
$~$

* ROC curves, AUC, sensitivity and specificity of classification methods.


---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(class)
library(dplyr)
library(ggpubr)
library(mvtnorm)
set.seed(9)

Sigma = matrix(c(2, 0, 0, 2), 2, 2)

mu1 = c(1, 1)
mu2 = c(3, 3)

X1 = rmvnorm(100, mu1,  Sigma)
X2 = rmvnorm(100, mu2,  Sigma)

class = c(rep("A",100), rep("B", 100))
class = as.factor(class)

df = data.frame(rbind(X1, X2), class)

test = expand.grid(x = seq(min(df[,1]-1), max(df[,1]+1), by=0.2), y=seq(min(df[,2]-1), max(df[,2]+1), by=0.2))


## k = 1
classif = knn(df[,1:2], test=test, cl=df[,3], k=1, prob=TRUE)
prob = attr(classif, "prob")

dataf = bind_rows(mutate(test, prob=prob, class="A", prob_cls=ifelse(classif==class, 1, 0)),
                  mutate(test, prob=prob, class="B", prob_cls=ifelse(classif==class, 1, 0)))

gg = ggplot(dataf)+geom_point(aes(x=x, y=y, colour=class), data=mutate(test, class=classif), size=0.01) 
gg = gg + geom_contour(aes(x=x, y=y, z=prob_cls, group=class, color=class), data=dataf, bins=2,size=0.5)
gg = gg + geom_point(aes(x=x, y=y, col=class), size=2, data=data.frame(x=df[,1], y=df[,2], class=df[,3]))
gg = gg + ggtitle("k = 1")+xlab("X1")+ylab("X2")

# k = 3
classif3 = knn(df[,1:2], test=test, cl=df[,3], k=3, prob=TRUE)
prob3 = attr(classif3, "prob")

dataf3 = bind_rows(mutate(test, prob=prob3, class="A", prob_cls=ifelse(classif3==class, 1, 0)),
                    mutate(test, prob=prob3, class="B", prob_cls=ifelse(classif3==class, 1, 0)))

gg3 = ggplot(dataf3)+geom_point(aes(x=x, y=y, colour=class), data=mutate(test, class=classif3), size=0.01)
gg3 = gg3 + geom_contour(aes(x=x, y=y, z=prob_cls, group=class, color=class), data=dataf3, bins=2, size=0.5)
gg3 = gg3 + geom_point(aes(x=x, y=y, col=class), size=2, data=data.frame(x=df[,1], y=df[,2], class=df[,3]))
gg3 = gg3 + ggtitle("k = 3")+xlab("X1")+ylab("X2")

## k = 10

classif10 = knn(df[,1:2], test=test, cl=df[,3], k=10, prob=TRUE)
prob10 = attr(classif10, "prob")

dataf10 = bind_rows(mutate(test, prob=prob10, class="A", prob_cls=ifelse(classif10==class, 1, 0)),
                  mutate(test, prob=prob10, class="B", prob_cls=ifelse(classif10==class, 1, 0)))

gg10 = ggplot(dataf10)+geom_point(aes(x=x, y=y, colour=class), data=mutate(test, class=classif10), size=0.05)
gg10 = gg10 + geom_contour(aes(x=x, y=y, z=prob_cls, group=class, color=class), data=dataf10, bins=2, size=0.5)
gg10 = gg10 + geom_point(aes(x=x, y=y, col=class), size=2, data=data.frame(x=df[,1], y=df[,2], class=df[,3]))
gg10 = gg10 + ggtitle("k = 10")+xlab("X1")+ylab("X2")

## k = 150

classif150 = knn(df[,1:2], test=test, cl=df[,3], k=150, prob=TRUE)
prob150 = attr(classif150, "prob")

dataf150 = bind_rows(mutate(test, prob=prob150, class="A", prob_cls=ifelse(classif150==class, 1, 0)),
                  mutate(test, prob=prob150, class="B", prob_cls=ifelse(classif150==class, 1, 0)))

gg150 = ggplot(dataf150)+geom_point(aes(x=x, y=y, colour=class), data=mutate(test, class=classif150), size=0.05)
gg150 = gg150 + geom_contour(aes(x=x, y=y, z=prob_cls, group=class, color=class), data=dataf150, bins=2, size=0.5)
gg150 = gg150 + geom_point(aes(x=x, y=y, col=class), size=2, data=data.frame(x=df[,1], y=df[,2], class=df[,3]))
gg150 = gg150 + ggtitle("k = 150")+xlab("X1")+ylab("X2")

ggarrange(gg,gg3,gg10,gg150)
```

--- 

Logistic regression gives a probability, given a certain value of the covariats $P(Y=1 \, | \, \boldsymbol{x})$.

![](beetles.png){width=90%}

---

```{r,echo=FALSE,eval=TRUE,fig.width=6,fig.height=4.5}
attach(iris)
library(class)
library(MASS)
library(dplyr)
library(ggpubr)

testgrid = expand.grid(Sepal.Length = seq(min(iris[,1]-0.2), max(iris[,1]+0.2), 
              by=0.05), Sepal.Width = seq(min(iris[,2]-0.2), max(iris[,2]+0.2), 
              by=0.05))
iris_lda = lda(Species~Sepal.Length+Sepal.Width, data=iris, prior=c(1,1,1)/3)
res = predict(object = iris_lda, newdata = testgrid)
Species_lda = res$class
postprobs=res$posterior
iris_lda_df = bind_rows(mutate(testgrid, Species_lda))
iris_lda_df$Species_lda = as.factor(iris_lda_df$Species_lda)
iris0_plot = ggplot(iris, aes(x=Sepal.Width, y=Sepal.Length, 
                              color=Species))+geom_point(size=2.5)
irislda_plot = iris0_plot + geom_point(aes(x = Sepal.Width, y=Sepal.Length, 
                            colour=Species_lda), data=iris_lda_df, size=0.8)
iris_qda = qda(Species~Sepal.Length + Sepal.Width, data=iris, prior=c(1,1,1)/3)
Species_qda = predict(object = iris_qda, newdata = testgrid)$class

iris_qda_df = bind_rows(mutate(testgrid, Species_qda))
iris_qda_df$Species_qda = as.factor(iris_qda_df$Species_qda)

gridprobs=

irisqda_plot = iris0_plot + geom_point(aes(x = Sepal.Width, y=Sepal.Length, 
                            colour=Species_qda), data=iris_qda_df, size=0.8)
```

```{r,echo=FALSE, fig.width=9,fig.height=4}
ggarrange(irislda_plot,irisqda_plot)
```



---

## 5. Resampling methods 

$~$

**Cross-validation**

\vspace{2mm}

* Data rich situation: Training, validation and test set.
\vspace{2mm}
* Validation set approach.
\vspace{2mm}
* Cross-validation for regression and for classification.
\vspace{2mm}
* LOOCV, 5 and 10 fold CV.
\vspace{2mm}
* Good and bad issues with validation set, LOOCV, 10-fold CV.
\vspace{2mm}
* Bias and variance for $k$-fold cross-validation.
\vspace{2mm}
* Selection bias -- the right and wrong way to do cross-validation.
\vspace{2mm}

* Distinction between model selection and model assessment.

$~$

**The Bootstrap**

* Idea: Re-use the same data to estimate a statistic of interest by _sampling with replacement_.  

---

## 6. Linear model selection and regularization:

$~$

Subset-selection. Discriminate:
\vspace{2mm}

* _\textcolor{red}{Model selection}_: estimate performance of different models to choose the best one. 
* _\textcolor{red}{Model assessment}_: having chosen a final model, estimate its performance on new data.

$~$

How?

\vspace{2mm}  

* Model selection by 
    * Subset selection (best subset selection or stepwise model selection)
    * Penalizing the training error: AIC, BIC, $C_p$, Adjusted $R^2$.
    * Cross-validation.

\vspace{2mm}  
* Model assessment by 
    * Cross-validation.


---

### Model selection

$~$

* Shrinkage methods
    + ridge regression: quadratic L2 penalty added to RSS 
    + lasso regression: absolute L1 penalty added to RSS
    + no penalty on intercept, not scale invariant: center and scale covariates
    
    $~$
    
* Dimension reduction methods:
    + principal component analysis: eigenvectors, proportion of variance explained, scree plot
    + principal component regression
    + partial least squares 
    
    $~$
    
* High dimensionality issues: multicollinearity, interpretation.

---


![ISL 6.7](../../ISLR/Figures/Chapter6/6.7.png)

---

![ISL 6.4](../../ISLR/Figures/Chapter6/6.4.png) 

---

![ISL 6.6](../../ISLR/Figures/Chapter6/6.6.png)

 

---

## 7. Moving beyond linearity 

$~$

* Modifications to the multiple linear regression model - when a linear model is not the best choice. First look at one covariate, combine in "additive model".
\vspace{2mm}

* Basis functions: fixed functions of the covariates.
\vspace{2mm}

* Polynomial regression: multiple linear regression with polynomial components as basis functions.
\vspace{2mm}

* Step functions - piecewise constants. Like our dummy variable coding of factors.
\vspace{2mm}

* Regression splines: regional polynomials joined smoothly - neat use of basis functions. Cubic splines very popular.

---

* Smoothing splines: smooth functions - minimizing the RSS with an additional penalty on the second derivative of the curve. Results in a natural cubic spline with knots in the unique values of the covariate. 
\vspace{2mm}

* Local regressions: smoothed $K$-nearest neighbour with local regression and weighting. In applied areas `loess` is very popular.
\vspace{2mm}

* (Generalized) additive models (GAMs): combine the above. Sum of (possibly) non-linear instead of linear functions.


---

![ISL 7.3](../../ISLR/Figures/Chapter7/7.3.png)

 
---

## 8. Tree-based methods

$~$

* Method applicable both to regression and classification ($K$ classes) and will give non-linear covariate effects and include interactions between covariates. 
\vspace{2mm}

* A tree can also be seen as a division of the covariate space into non-overlapping regions.
\vspace{2mm}

* Binary splits using only at the current best split:  _greedy strategy_.
\vspace{2mm}

* Minimization criterion: residual sums of squares (RSS), Gini index or cross-entropy.
\vspace{2mm}
      
* Stopping criterion: When to stop: decided stopping criterion - like minimal decrease in RSS or less than 10 observations in terminal node.
\vspace{2mm}

* Prediction:
    * Regression: Mean in box $R_j$
    * Classification: Majority vote or cut-off on probabiity. 

---

* _Pruning_: Grow full tree, and then prune back using pruning strategy: cost complexity pruning.


To improve prediction (but worse interpretation):

* _Bagging_ (bootstrap aggregation): draw $B$ bootstrap samples and fit one full tree to each, used the average over all trees for prediction. 

* _Random forest_: as bagging but only $m$ (randomly) chosen covariates (out of the $p$) are available for selection at each possible split. 

* Out-of-bag estimation can be used for model selection - no need for cross-validation.

* Variable importance plots.

* _Boosting_: fit one tree with $d$ splits, make residuals and fit a new tree, adjust residuals partly with new tree - repeat. 

---


## 9. Support vector machines

$~$

* SVM can be used both classification and regression, but we have only studied two-class classification.

\vspace{2mm}

* Aim: find high dimensional hyperplane that separates two classes $f({\bf x})=\beta_0+{\bf x}^T \boldsymbol\beta=0$. If $y_if({\bf x}_i)>0$ observation ${\bf x}_i$ is correctly classified.

\vspace{2mm}

---

* Maximizing the distance (on both sides) from the class boundary to the closes observations (the margin $M$). 

* Relaxed with slack variables (support vector classifiers), and to allow nonlinear functions of ${\bf x}$ (inner products).

\vspace{2mm}

* Support vectors: observations that lie on the margin or on the wrong side of the margin. 

\centering
![casi Figure 19.3](../../casi/193.png){width=80%}

---

* Kernels: generalization of an inner product to allow for non-linear boundaries 

* Most popular kernel is radial $$K(x_i,x_i')=\exp(-\gamma\sum_{j=1}^p (x_{ij}-x_{i'j})^2) \ . $$


* Tuning parameters: cost and parameters in kernels - chosen by CV.

\vspace{2mm}

* Unfortunately not able to present details since then a course in optimization is needed.

\vspace{2mm}

* _\textcolor{red}{SVMs have been overtaken largely by neural networks and tree-based methods.}_

---


## 10. Unsupervised learning 

$~$

* Principal component analysis: 
     + Mathematical details (eigenvectors corresponding to covariance or correlation matrix) also in TMA4267. 
     + Understanding loadings, scores and the biplot, choosing the number of principal components from proportion of variance explained or scree-type plots (elbow).
     
\vspace{2mm}

* Clustering:
     + $k$-means: number of clusters given, iterative algorithm to classify to nearest centroid and recalculate centroid
     + Hierarchical clustering: choice of distance measure, choice of linkage method (single, average, complete), 

---

\centering
```{r,echo=FALSE,out.width="70%"}
# reading data on consumption of different beverages for countries
drink <- read.csv("https://www.math.ntnu.no/emner/TMA4267/2017v/drikke.TXT",sep=",",header=TRUE)
drink <- na.omit(drink)
# looking at correlation between consumptions
drinkcorr=cor(drink)
pcaS <- prcomp(drink,scale=TRUE) # scale: variables are scaled 
biplot(pcaS,scale=0,cex=0.6) # scale=0: arrows scaled to represent the loadings
```

 

---

Hierarchical clustering for visualization

\centering
![](../1Intro/heatmap.png){width=70%}



---

## 11. Neural networks
 
 $~$

* Feedforward network architecture: mathematical formula - layers of multivariate transformed (`relu`, `linear`, `sigmoid`) inner products - sequentially connected. 
 \vspace{2mm}
 
 
\centering
![](../11Nnet/fig10_4.png){width=70%} 
 
--- 
 
* Loss function to minimize (on output layer): regression (mean squared), classification binary (binary crossentropy), classification multiple classes (categorical crossentropy).
\vspace{2mm}

* Remember the correct choice of output activiation function: mean squared loss goes with linear activation, binary crossentropy with sigmoid, categorical crossentropy with softmax.
\vspace{2mm}

* Gradient based (chain rule) back-propagation.
\vspace{2mm}

* Over-fitting $\rightarrow$ Regularization.
\vspace{2mm}

* Convolutional neural networks (CNNs), recursive neural networks (RNNs) and transformers (Daesoo's lecture).
\vspace{2mm}

* `keras` in R. Use of tensors: Piping sequential layers, piping to estimation and then to evaluation (metrics).

---

# Mentimeter poll

$~$

\centering

www.menti.com

Code 16 18 55 1


---

# Some cautionary words

$~$

* In most of the problems we looked at we could (or had to) choose a set of variables to explain or predict an outcome ($y$).

* Model selection was the topic of Module 6, but there is more to say about it, in particular in the regression context.

* Importantly, the approach to find a model **heavily depends on the aim** for which the model is built. 

$~$

It is important to make the following distinction:

* The aim is to _\textcolor{red}{predict}_ future values of $y$ from known regressors.
* The aim is to _\textcolor{red}{explain}_ $y$ using known regressors. In this case, the ultimate aim is to find _causal relationships_.

---

$\rightarrow$ Even among statisticians there is no real consensus about how, if, or when to select a model:

\vspace{2mm}

![](graphics/brewer_title.png){width=90%}
![](graphics/brewer.png){width=90%}


Note: The first sentence of a paper in _Methods in Ecology and Evolution_ from 2016 is: ``Model selection is difficult.''

---

## Why is finding a model so hard?

$~$

A model is an approximation of the reality. The aim of statistics and data analysis is to find connections (explanations or predictions) thanks to simplifications of the real world.

$~$

Box (1979): _\textcolor{red}{``All models are wrong, but some are useful.''}_

$~$
$~$

$\rightarrow$ There is often not a "right" or a "wrong" model -- but there are more and less useful ones.

$~$

$\rightarrow$ Finding a model or the appropriate method with good properties is sometimes an art...


---


## Predictive and explanatory models 

$~$

When choosing a method or a model, you need to be clear about the scope:

$~$

* **\textcolor{red}{Predictive models}**: These are models that aim to predict the outcome of future subjects.  \vspace{2mm}  
**Example:** In the bodyfat example (module 3) the aim is to predict people's bodyfat from factors that are easy to measure (age, BMI, weight,..).

$~$

* **\textcolor{red}{Explanatory models}**: These are models that aim at understanding the (causal) relationship between covariates and the response. \vspace{2mm}     
**Example:** The South African heart disease data (module 4) aims to identify important risk factors for coronary heart disease.
 

$~$
 
$\rightarrow$ **The model selection strategy depends on this distinction.**
 
---

## Prediction vs explanation

$~$

When the aim is **_prediction_**, the best model is the one that best predicts the fate of a future subject (smallest test error rate). This is a well defined task and ``objective'' variable selection strategies to find the model which is best in this sense are potentially useful.

$~$

However, when used for **_explanation_** the best model will depend on the scientific question being asked, **and automatic variable selection strategies have no place**. 
 

$~$


\scriptsize
Chapters 27.1 and 27.2 in @clayton.hills1993

---

## Model selection with AIC, AIC$_c$, BIC, $C_p$, adjusted $R^2$

$~$

Given $m$ potential variables to be included in a model. Remember from Module 6: 
\vspace{2mm}

* Subset selection using forward, backward or best subset selection method.

* Use an "objective" criterion to find the "best" model.

$~$

**Cautionary Note**:

The coefficients of such an optimized ``best'' model should _not be interpreted_ in a causal sense! Why?

$~$

* Subset selection may lead to **biased parameter estimates**, thus **do not draw (biological, medical,..) conclusions** from models that were optimized for prediction, for example by AIC/AICc/BIC minimization! 
See, e.g., @freedman1983, @copas1983.

\vspace{2mm}
\normalsize



<!-- --- -->

<!-- ### AIC$_c$: The AIC for low sample sizes -->

<!-- $~$ -->

<!-- When the number of data points $n$ is small with respect to the number of parameters $p$ in a model, the use of a _\textcolor{red}{corrected AIC}_, the AIC$_c$ is recommended. For a model with $n$ data points, likelihood $L$ and $p$ parameters, it is given as -->
<!-- \begin{equation*} -->
<!-- \text{AIC}_c = -2\log(L) + 2p\cdot\frac{n}{n-p-1} \ . -->
<!-- \end{equation*} -->

<!-- \vspace{2mm} -->

<!-- Burnham and Anderson **recommend to use AIC$_c$ in general, but for sure when the ratio $n/p<40$.** -->







<!-- --- -->

<!-- ## Explanatory: Confirmatory vs exploratory -->



<!-- $~$ -->

<!-- $\rightarrow$ We can further split \textcolor{red}{explanatory analyses} into: -->

<!-- $~$ -->

<!-- * \textcolor{red}{Confirmatory}:  -->
<!--     * Clear hypothesis and \emph{\bf a priori} selection of regressors for $y$. -->
<!--     * **No subset selection!** -->
<!--     * Allowed to interpret the results and draw quantitative conclusions.  -->

<!-- $~$ -->


<!-- * \textcolor{red}{Exploratory}:  -->
<!--     * Build whatever model you want, but the results should only be used to generate new hypotheses, a.k.a. ``speculations''. -->
<!--     * Clearly label the results as ``exploratory''. -->


<!-- $~$ -->

<!-- $~$ -->

<!-- Therefore: Any **additional analyses** that you potentially do with your data have the character of **exploratory models**. -->

<!-- --- -->

<!-- ## Example from a clinical study -->



<!-- --- -->

<!-- ## Interpretation of exploratory models? -->

<!-- $~$  -->

<!-- Results from exploratory models can be used to generate new hypotheses, but it is then _not allowed to draw causal conclusions from them_, or to over-interpret effect-sizes. -->

<!-- $~$ -->


<!-- * In biological publications it is (unfortunately) still common practice that exploratory models, which were optimized with model selection criteria (like AIC), are used to draw conclusions as if the models were confirmatory. -->

<!-- $~$ -->


<!-- * We illustrate why this is a problem on the next slides. -->


---


## Illustration: Model selection bias
 
$~$

**Aim of the example:** To illustrate how model selection purely based on AIC can lead to biased parameters and overestimated effects.

$~$

_Procedure:_
\vspace{2mm}

1. Randomly generate 100 data points for 50 covariables $x^{(1)},\ldots, x^{(50)}$ and a response $y$: 

$~$
\scriptsize
```{r,echo=T,eval=T}
set.seed(123456)
data_aic <- data.frame(matrix(rnorm(51*100),ncol=51))
names(data_aic)[51] <- "Y"
```
 
$~$

\normalsize
\texttt{data} is a 100$\times$ 51 matrix, where the last column is the response. The **data were generated completely independently**, the covariates do not have any explanatory power for the response! 

---

2. Fit a linear regression model of $y$ against all the 50 variables
\begin{equation*}
y_i = \beta_0 + \beta_1 x_i^{(1)} + \ldots + \beta_{50}x_i^{(50)} + \epsilon_i \ .
\end{equation*}

\scriptsize
```{r}
r.lm.aic <- lm(Y~.,data_aic)
```

\normalsize
As expected, the distribution of the $p$-values is (more or less) uniform between 0 and 1, with none below 0.05: 

\scriptsize

\centering

```{r, fig.width=4, fig.height=4.5,out.width="4cm"}
hist(summary(r.lm.aic)$coef[-1,4],freq=T,main="50 variables",xlab="p-values")
```
 
---

3. Then use AICc minimization to obtain the objectively "best" model:

\vspace{2mm}

\scriptsize
```{r, echo=T}
library(MASS)
r.AICmin <- stepAIC(r.lm.aic, direction = c("both"), trace = FALSE,AICc=TRUE)
```

\centering
```{r, fig.width=4, fig.height=4.5,out.width="4cm"}
hist(summary(r.AICmin)$coef[-1,4],freq=T,main="18 variables of minimal AICc model",xlab="p-values")
```
 
\flushleft
\normalsize
The distribution of the $p$-values is now skewed: many of them reach rather small values (`r sum(summary(r.AICmin)$coef[-1,4]<0.05)` have $p<0.05$). This happened _although none of the variables has any explanatory power!_


---

<!-- ### Your aim is prediction? -->

<!-- $~$ -->

<!-- Then you are basically free to do what you want. Your only aim is to _minimize some sort of prediction error_ or _loss function_. -->

<!-- $~$ -->

<!-- **Examples**: -->

<!-- --- -->

<!-- ### Your aim is explanation? -->

<!-- $~$ -->

<!-- _Explanation_ means that you will want to interpret the regression coefficients, 95% CIs and $p$-values. It is then often assumed that some sort of causality ($x\rightarrow y$) exists. -->

<!-- $~$ -->

<!-- In such a situation: -->

<!-- $~$ -->

<!-- * Start with a _\textcolor{red}{clear hypothesis}_.  -->
<!-- \vspace{2mm} -->

<!-- * Select your covariates according to _\textcolor{red}{a priori knowledge}_.  -->
<!-- \vspace{2mm} -->

<!-- * Ideally, formulate _\textcolor{red}{only one}_ or a few model(s) _\textcolor{red}{before you start analysing your data}_. Ideally even before you start collecting your data. -->



<!-- --- -->


## Summary: Main problem with model selection 

$~$

\colorbox{lightgray}{\begin{minipage}{10cm}
When model selection is carried out based on objective criteria, the effect sizes will to be too large and the uncertainty too small. So you end up being too sure about a too large effect. 
\end{minipage}}

$~$

$~$

**Exception**: Shrinkage methods!


<!-- --- -->

<!-- ## Recommended procedure for explanatory models  -->

<!-- $~$ -->

<!-- Before you start:  -->
<!-- \vspace{2mm} -->

<!-- 1. **Think about a suitable model**. This includes the model family (e.g., linear model), but also potential variables that are relevant using _a priori_ knowledge. -->
<!-- \vspace{2mm} -->

<!-- 2. Declare a strategy what you do if _e.g._ modelling assumptions are not met or in the presence of collinearity. -->
<!--     * What kind of variable transformations would you try, in which order, and why? -->
<!--     * What model simplifications will be considered it it is not possible to fit the intended model? -->
<!--     * How will you deal with outliers? -->
<!--     * How will you treat missing values in the data? -->
<!--     * How will you treat collinear covariates? -->
<!--     * ...  -->

<!-- $~$ -->

<!-- It is advisable to write your strategy down as a ``protocol'' before doing any analyses.  -->

<!-- --- -->


<!-- 3. Analyze the data following your ``protocol'': -->

<!-- $~$ -->

<!-- * Fit the model and check if modelling assumptions are met. -->

<!-- * If modelling assumptions are not met, **adapt the model** as outlined in your protocol.  -->

<!-- *  Interpret the model coefficients (effect sizes) and the $p$-values properly.  -->


<!-- $~$ -->


<!-- 4. After the analysis that was specified in the ``protocol'':  -->

<!-- * Any additional analyses, which you did not specify in advance, are purely exploratory. -->


---

Which methods are suitable for explanation (inference)?
\vspace{-2mm}

\centering
![](../../ISLR/Figures/Chapter2/2.7.png)

---

### Bottomline

$~$

* _Less flexible_ models tend to have _better interpretability_ and are therefore better suited _for explanation_ than more flexible models.

\vspace{2mm}

* The _more flexible_ a model, the better it tends to perform _for prediction_.

\vspace{2mm}

* Some models can be used for both, prediction and explanation, but then the approach how to build the model (e.g., how to select variables) should still depend on the aim.



<!-- # One more thing: The replication crisis -->

<!-- \vspace{2mm} -->

<!-- * In science, the aim of models is most often _explanation_. -->

<!-- * _Replication crisis_: https://en.wikipedia.org/wiki/Replication_crisis -->


<!-- $~$ -->

<!-- Do not overinterpret $p$-values. From @goodman2016 : -->

<!-- $~$ -->

<!-- \begin{quote} -->
<!-- Fisher used ``significance'' merely {\bf to indicate that an observation was worth following up, with refutation of the null hypothesis justified only if further experiments ``rarely failed'' to achieve significance.}  -->
<!-- This is in stark contrast to the modern practice of making claims based on a single demonstration of statistical significance. -->
<!-- \end{quote} -->

<!-- --- -->



<!-- Please read some recent literature: -->

<!-- * @wasserstein.lazar2016: The ASA's statement on $p$-values https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108 -->
<!-- * @goodman2008: https://www.sciencedirect.com/science/article/abs/pii/S0037196308000620 -->
<!-- * @nuzzo2014: https://www.nature.com/news/scientific-method-statistical-errors-1.14700 -->

<!-- $~$ -->

<!-- These things are **extremely relevant** for you if you are going to analyse (scientific) data in the future. -->



<!-- --- -->

<!-- # After TMA4268 - what is next? -->

<!-- What are the statistical challenges we have not covered? -->

<!-- Do you want to learn more about the methods we have looked at in this course? And also methods that are more tailored towards specific types of data? Then we have many statistics courses that you may choose from. -->

<!-- An overview of statistics courses is kindly put together by Mette Langaas: -->
<!-- <https://folk.ntnu.no/mettela/Talks/3klinfo20190325.html> -->



---

$~$

Thank you for attending this course  - good luck for the exam!


---

# References


