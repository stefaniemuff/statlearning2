\beamer@slide {acknowledgements}{3}
\beamer@slide {learning-material-for-this-module}{4}
\beamer@slide {what-will-you-learn}{5}
\beamer@slide {introduction-time-line}{6}
\beamer@slide {ai-machine-learning-and-statistics}{9}
\beamer@slide {ai}{10}
\beamer@slide {machine-learning}{11}
\beamer@slide {deep-learning}{12}
\beamer@slide {feedforward-networks}{15}
\beamer@slide {the-single-hidden-layer-feedforward-network}{16}
\beamer@slide {special-case-linear-activation-function-for-the-hidden-layer}{19}
\beamer@slide {multilayer-neural-networks}{20}
\beamer@slide {outcome-encoding}{22}
\beamer@slide {example-mnist-dataset}{23}
\beamer@slide {neural-network-parts}{25}
\beamer@slide {output-layer-activation}{26}
\beamer@slide {hidden-layer-activation}{27}
\beamer@slide {universal-approximation-property}{29}
\beamer@slide {network-architecture}{31}
\beamer@slide {loss-function-method}{32}
\beamer@slide {optimizors}{34}
\beamer@slide {gradient-descent}{35}
\beamer@slide {finding-optimal-weights-gradient-descent-algorithm}{36}
\beamer@slide {full-vs.-stochastic-gradient-descent-sgd}{37}
\beamer@slide {mini-batch-stochastic-gradient-descent-sgd}{38}
\beamer@slide {backpropagation-algorithm}{40}
\beamer@slide {backpropagation-algorithm-1}{41}
\beamer@slide {regularization}{43}
\beamer@slide {regularization-in-neural-networks}{44}
\beamer@slide {early-stopping}{45}
\beamer@slide {dropout}{46}
\beamer@slide {dropout-1}{47}
\beamer@slide {ways-to-avoid-overfitting}{48}
\beamer@slide {how-to-fit-those-models}{49}
\beamer@slide {an-example}{50}
\beamer@slide {boston-house-prices}{50}
\beamer@slide {boston-example-using-keras}{55}
\beamer@slide {neural-networks}{56}
\beamer@slide {today}{57}
\beamer@slide {convolutional-neural-networks-cnns}{58}
\beamer@slide {idea-of-cnns-recognize-features-and-patterns.}{59}
\beamer@slide {elements-of-a-cnn}{60}
\beamer@slide {convolution-layers}{61}
\beamer@slide {pooling-layers}{64}
\beamer@slide {data-augmentation}{66}
\beamer@slide {an-interactive-node-link-visualization-of-convolutional-neural-networks}{67}
\beamer@slide {examples}{68}
\beamer@slide {recurrent-neural-networks-rnns---motivation}{69}
\beamer@slide {problems-with-other-vanilla-nn}{70}
\beamer@slide {recurrent-neural-networks-rnns}{71}
\beamer@slide {eq:chain}{74}
\beamer@slide {fitting-the-weights-in-an-rnn}{75}
\beamer@slide {example-of-an-rnn-time-series-forecasting}{78}
\beamer@slide {recurrent-neural-network}{81}
\beamer@slide {when-to-use-deep-learning}{82}
\beamer@slide {example-the-hitters-data-set}{83}
\beamer@slide {double-descent}{86}
\beamer@slide {double-descent-1}{87}
\beamer@slide {double-descent---some-consideration}{91}
\beamer@slide {dl-in-medicine}{92}
\beamer@slide {dlml-in-ecology}{93}
\beamer@slide {references-and-further-reading}{94}
\beamer@slide {acknowledgements-1}{95}
\beamer@slide {refs}{95}
