> library(keras)
> mnist <- dataset_mnist()
/Users/mettela/.virtualenvs/r-tensorflow/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
  from ._conv import register_converters as _register_converters
Using TensorFlow backend.
> #Train
> train_images <- mnist$train$x
> train_labels <- mnist$train$y
> 
> # Test data
> test_images <- mnist$test$x
> test_labels <- mnist$test$y
> 
> network <- keras_model_sequential() %>%
+   layer_dense(units = 512, activation = "relu", input_shape = c(28*28)) %>%
+   layer_dense(units = 10, activation = "softmax")
> 
> network %>% compile(
+   optimizer = "rmsprop",
+   loss = "categorical_crossentropy",
+   metrics = c("accuracy")
+ )
> 
> train_images <- array_reshape(train_images, c(60000, 28 * 28))
> train_images <- train_images / 255
> train_labels <- to_categorical(train_labels)
> 
> test_images <- array_reshape(test_images, c(10000, 28 * 28))
> test_images <- test_images / 255
> test_labels <- to_categorical(test_labels)
> 
> fitted<- network %>% fit(train_images, train_labels,
+                          epochs = 30, batch_size = 128,
+                          validation_split = 0.2
+ )
Train on 48000 samples, validate on 12000 samples
Epoch 1/30
2019-03-21 18:38:24.694388: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
48000/48000 [==============================] - 5s 107us/step - loss: 0.2890 - acc: 0.9161 - val_loss: 0.1439 - val_acc: 0.9601
Epoch 2/30
48000/48000 [==============================] - 5s 94us/step - loss: 0.1170 - acc: 0.9651 - val_loss: 0.1050 - val_acc: 0.9677
Epoch 3/30
48000/48000 [==============================] - 4s 93us/step - loss: 0.0767 - acc: 0.9774 - val_loss: 0.0996 - val_acc: 0.9683
Epoch 4/30
48000/48000 [==============================] - 4s 92us/step - loss: 0.0551 - acc: 0.9836 - val_loss: 0.0867 - val_acc: 0.9757
Epoch 5/30
48000/48000 [==============================] - 4s 92us/step - loss: 0.0405 - acc: 0.9878 - val_loss: 0.0800 - val_acc: 0.9774
Epoch 6/30
48000/48000 [==============================] - 5s 97us/step - loss: 0.0312 - acc: 0.9902 - val_loss: 0.0840 - val_acc: 0.9750
Epoch 7/30
48000/48000 [==============================] - 4s 88us/step - loss: 0.0236 - acc: 0.9935 - val_loss: 0.0827 - val_acc: 0.9769
Epoch 8/30
48000/48000 [==============================] - 4s 93us/step - loss: 0.0176 - acc: 0.9951 - val_loss: 0.0796 - val_acc: 0.9788
Epoch 9/30
48000/48000 [==============================] - 4s 93us/step - loss: 0.0136 - acc: 0.9961 - val_loss: 0.0918 - val_acc: 0.9767
Epoch 10/30
48000/48000 [==============================] - 4s 93us/step - loss: 0.0101 - acc: 0.9974 - val_loss: 0.1171 - val_acc: 0.9716
Epoch 11/30
48000/48000 [==============================] - 4s 92us/step - loss: 0.0082 - acc: 0.9977 - val_loss: 0.0859 - val_acc: 0.9791
Epoch 12/30
48000/48000 [==============================] - 5s 94us/step - loss: 0.0057 - acc: 0.9984 - val_loss: 0.0860 - val_acc: 0.9805
Epoch 13/30
48000/48000 [==============================] - 4s 91us/step - loss: 0.0047 - acc: 0.9988 - val_loss: 0.0980 - val_acc: 0.9783
Epoch 14/30
48000/48000 [==============================] - 4s 90us/step - loss: 0.0035 - acc: 0.9991 - val_loss: 0.0966 - val_acc: 0.9796
Epoch 15/30
48000/48000 [==============================] - 4s 86us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 0.0962 - val_acc: 0.9794
Epoch 16/30
48000/48000 [==============================] - 4s 90us/step - loss: 0.0020 - acc: 0.9997 - val_loss: 0.1001 - val_acc: 0.9806
Epoch 17/30
48000/48000 [==============================] - 4s 89us/step - loss: 0.0018 - acc: 0.9995 - val_loss: 0.0999 - val_acc: 0.9802
Epoch 18/30
48000/48000 [==============================] - 4s 89us/step - loss: 0.0013 - acc: 0.9997 - val_loss: 0.1036 - val_acc: 0.9809
Epoch 19/30
48000/48000 [==============================] - 4s 89us/step - loss: 0.0010 - acc: 0.9998 - val_loss: 0.1067 - val_acc: 0.9813
Epoch 20/30
48000/48000 [==============================] - 4s 90us/step - loss: 7.1080e-04 - acc: 0.9999 - val_loss: 0.1088 - val_acc: 0.9814
Epoch 21/30
48000/48000 [==============================] - 4s 86us/step - loss: 6.1771e-04 - acc: 0.9999 - val_loss: 0.1126 - val_acc: 0.9809
Epoch 22/30
48000/48000 [==============================] - 4s 87us/step - loss: 6.0087e-04 - acc: 0.9998 - val_loss: 0.1204 - val_acc: 0.9797
Epoch 23/30
48000/48000 [==============================] - 4s 92us/step - loss: 3.4698e-04 - acc: 0.9999 - val_loss: 0.1268 - val_acc: 0.9797
Epoch 24/30
48000/48000 [==============================] - 4s 85us/step - loss: 3.5965e-04 - acc: 0.9999 - val_loss: 0.1191 - val_acc: 0.9811
Epoch 25/30
48000/48000 [==============================] - 4s 86us/step - loss: 3.5031e-04 - acc: 0.9999 - val_loss: 0.1175 - val_acc: 0.9816
Epoch 26/30
48000/48000 [==============================] - 4s 90us/step - loss: 3.8867e-04 - acc: 0.9998 - val_loss: 0.1208 - val_acc: 0.9813
Epoch 27/30
48000/48000 [==============================] - 4s 90us/step - loss: 2.5784e-04 - acc: 0.9999 - val_loss: 0.1233 - val_acc: 0.9812
Epoch 28/30
48000/48000 [==============================] - 4s 90us/step - loss: 8.4604e-05 - acc: 1.0000 - val_loss: 0.1319 - val_acc: 0.9806
Epoch 29/30
48000/48000 [==============================] - 4s 88us/step - loss: 2.0800e-04 - acc: 0.9999 - val_loss: 0.1270 - val_acc: 0.9822
Epoch 30/30
48000/48000 [==============================] - 5s 95us/step - loss: 4.7520e-05 - acc: 1.0000 - val_loss: 0.1343 - val_acc: 0.9814
> 
> network %>% evaluate(test_images,test_labels)
10000/10000 [==============================] - 1s 61us/step
$loss
[1] 0.1225657

$acc
[1] 0.9818

> network %>% predict_classes(test_images)
   [1] 7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4 9 6 6 5 4 0 7 4 0 1 3 1 3 4 7 2 7 1 2 1 1 7 4 2 3 5 1 2 4 4 6 3 5 5 6 0 4 1 9 5 7 8 9 3 7 4 6 4 3 0 7 0 2 9 1 7 3 2 9 7 7 6 2 7
  [85] 8 4 7 3 6 1 3 6 9 3 1 4 1 7 6 9 6 0 5 4 9 9 2 1 9 4 8 7 3 9 7 9 4 4 9 2 5 4 7 6 7 9 0 5 8 5 6 6 5 7 8 1 0 1 6 4 6 7 3 1 7 1 8 2 0 2 9 9 5 5 1 5 6 0 3 4 4 6 5 4 6 5 4 5
 [169] 1 4 4 7 2 3 2 7 1 8 1 8 1 8 5 0 8 9 2 5 0 1 1 1 0 9 0 3 1 6 4 2 3 6 1 1 1 3 9 5 2 9 4 5 9 3 9 0 3 6 5 5 7 2 2 7 1 2 8 4 1 7 3 3 8 8 7 9 2 2 4 1 5 9 8 7 2 3 0 6 4 2 4 1
 [253] 9 5 7 7 2 8 2 6 8 5 7 7 9 1 8 1 8 0 3 0 1 9 9 4 1 8 2 1 2 9 7 5 9 2 6 4 1 5 8 2 9 2 0 4 0 0 2 8 4 7 1 2 4 0 2 7 4 3 3 0 0 3 1 9 6 5 2 5 9 7 9 3 0 4 2 0 7 1 1 2 1 5 3 3
 [337] 9 7 8 6 3 6 1 3 8 1 0 5 1 3 1 5 5 6 1 8 5 1 7 9 4 6 2 2 5 0 6 5 6 3 7 2 0 8 8 5 4 1 1 4 0 3 3 7 6 1 6 2 1 9 2 8 6 1 9 5 2 5 4 4 2 8 3 8 2 4 5 0 3 1 7 7 5 7 9 7 1 9 2 1
 [421] 4 2 9 2 0 4 9 1 4 8 1 8 4 5 9 8 8 3 7 6 0 0 3 0 2 0 6 4 9 3 3 3 2 3 9 1 2 6 8 0 5 6 6 6 3 8 8 2 7 5 8 9 6 1 8 4 1 2 5 9 1 9 7 5 4 0 8 9 9 1 0 5 2 3 7 2 9 4 0 6 3 9 5 2
 [505] 1 3 1 3 6 5 7 4 2 2 6 3 2 6 5 4 8 9 7 1 3 0 3 8 3 1 9 3 4 4 6 4 2 1 8 2 5 4 8 8 4 0 0 2 3 2 7 7 0 8 7 4 4 7 9 6 9 0 9 8 0 4 6 0 6 3 5 4 8 3 3 9 3 3 3 7 8 0 2 2 1 7 0 6
 [589] 5 4 3 8 0 9 6 3 8 0 9 9 6 8 6 8 5 7 8 6 0 2 4 0 2 2 3 1 9 7 5 8 0 8 4 6 2 6 7 9 3 2 9 8 2 2 9 2 7 3 5 9 1 8 0 2 0 5 2 1 3 7 6 7 1 2 5 8 0 3 7 1 4 0 9 1 8 6 7 7 4 3 4 9
 [673] 1 9 5 1 7 3 9 7 6 9 1 3 3 8 3 3 6 7 2 4 5 8 5 1 1 4 4 3 1 0 7 7 0 7 9 9 4 8 5 5 4 0 8 2 1 0 8 4 8 0 4 0 6 1 7 3 2 6 7 2 6 9 3 1 4 6 2 5 9 2 0 6 2 1 7 3 4 1 0 5 4 3 1 1
 [757] 7 4 9 9 4 8 4 0 2 4 5 1 1 6 4 7 1 9 4 2 4 1 5 5 3 8 3 1 4 5 6 8 9 4 1 5 3 8 0 3 2 5 1 2 8 3 4 4 0 8 8 3 3 1 7 3 5 9 6 3 2 6 1 3 6 0 7 2 1 7 1 4 2 4 2 1 7 9 6 1 1 2 4 8
 [841] 1 7 7 4 8 0 7 3 1 3 1 0 7 7 0 3 5 5 2 7 6 6 9 2 8 3 5 2 2 5 6 0 8 2 9 2 8 8 8 8 7 4 9 3 0 6 6 3 2 1 3 2 2 9 3 0 0 5 7 8 3 4 4 6 0 2 9 1 4 7 4 7 3 9 8 8 4 7 1 2 1 2 2 3
 [925] 2 3 2 3 9 1 7 4 0 3 5 5 8 6 3 2 6 7 6 6 3 2 7 9 1 1 7 4 6 4 9 5 1 3 3 4 7 8 9 1 1 0 9 1 4 4 5 4 0 6 2 2 3 1 5 1 2 0 3 8 1 2 6 7 1 6 2 3 9 0 1 2 2 0 8 9
 [ reached getOption("max.print") -- omitted 9000 entries ]
