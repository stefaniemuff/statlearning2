---
title: 'Module 3: Recommended Exercises - Solution'
author:
- Kenneth Aase, Emma Skarstein, Daesoo Lee, Stefanie Muff
- Department of Mathematical Sciences, NTNU
date: "January 26, 2023"
output:
  pdf_document:
    toc: no
    toc_depth: '2'
  html_document:
    df_print: paged
    toc: no
    toc_depth: '2'
subtitle: TMA4268 Statistical Learning V2023
urlcolor: blue
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,
                      tidy = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      strip.white = TRUE,
                      prompt = FALSE,
                      cache = TRUE,
                      size = "scriptsize")

```

<!-- rmarkdown::render("RecEx3-sol.Rmd","all",encoding="UTF-8") -->
<!-- rmarkdown::render("RecEx3-sol.Rmd","html_document",encoding="UTF-8") -->
<!-- rmarkdown::render("RecEx3-sol.Rmd","pdf_document",encoding="UTF-8") -->


---


# Problem 1 


## a)

```{r}
library(ISLR)
Auto <- subset(Auto, select = -name)
#Auto$origin = factor(Auto$origin)
summary(Auto)
str(Auto)

Auto$origin <- factor(Auto$origin)
```

```{r,warning=FALSE, message=FALSE}
library(GGally)
ggpairs(Auto,
        lower = list(continuous = wrap("points", size = 0.1))) + # resize points
  theme(text = element_text(size = 7)) # change text size
```

## b)

Correlation matrix for the `Auto` data set where we omit the column `origin`:
```{r}
cor(subset(Auto, select = -origin))
```

## c)

```{r}
fit.lm <- lm(mpg ~ ., data = Auto)
summary(fit.lm)
#kable(broom::tidy(fit.lm))
```

i. The F-statistic in the very last line of the summary output shows that it is extremely unlikely that we would see such a result if none of the variables had any explanatory power -- the $p$-value is $<2.2e-16$! So we can be sure that the set of predictors is explaining the response to a quite large degree. This is also confirmed by both a quite high $R^2$ and $R^2_{\text adjust}$ (both around 0.82).
ii. Yes, there is very strong evidence for a relationship between the weight of a car and the response. The $p$-value is extremely small. The interpretation is that, if a car weights 1000 kg more, we have a decrease of $1000 \cdot (-6.710\cdot 10^{-3}) = -6.71$ miles per gallon (mpg), that is, the car can drive 6.71 miles less far per gallon of fuel.
iii. The estimated coefficient $\hat{\beta}_\text{year}= 0.77$ suggests that a newer car model has higher `mpg` compared to an older one. A one year newer model should thus, in average, be able to drive $0.77$ miles longer per gallon fuel compared to the older car.


## d)

Remember that if we want to test whether a factor variable with more than two levels should be removed from the model, we actually have to test whether all slope coefficients that are associated with the respective binary dummy variables are zero simultaneously. Look again at equation (3.30) in the course book (p.85). 

Consequently, we have to test here whether $\beta_\text{origin2} = \beta_\text{origin3} = 0$ at the same time, and for this we need the $F$-test, which is implemented in the `anova()` function in R:

```{r}
anova(fit.lm)
```

The $p$-value associated with `origin` is now the one we can look at. Clearly, $p$ is very small, thus the origin of the car has an influence on the response `mpg`.

## e)

```{r, warning = FALSE, fig.width = 5, fig.height = 4, fig.align = "center", out.width = '60%', cache = FALSE}
library(ggfortify)
autoplot(fit.lm, smooth.colour = NA)
```

* In the residual vs fitted plot (the so-called Tukey-Anscombe plot) there is evidence of non-linearity.
* Observation $14$ has an unusually high leverage. This does not necessarily need to be a problem, but it would be wise to double-check that this observation is not an outlier.

## f)

```{r}
set.seed(2332)
n <- 100

par(mfrow = c(2, 3))
for (i in 1:6){
  sim <- rnorm(n)
  qqnorm(sim, pch = 1, frame = FALSE)
  qqline(sim, col = "blue", lwd = 1)
}
```

## g)


```{r}
fit.lm1 <- lm(mpg ~ displacement + weight + year * origin, data = Auto)
summary(fit.lm1)
```

Again, if we want to find evidence whether the interaction term between `year` and `origin` is needed, we are actually testing whether two slope coefficients are zero at the same time ($\beta_\text{year:origin1} = \beta_\text{year:origin2} = 0$). Consequently, we need the $F$-test, which is implemented in the `anova()` function:

```{r}
anova(fit.lm1)
```

There is very strong evidence that the year-effect depends on the origin of the car, as can be seen by the $F$-test that is given by the anova table ($p = 0.0001057$). For European (`2`) and Japanese (`3`) cars, it seems that the fuel consumption (`mpg`) has a steeper slope for `year`: $\hat{\beta}_{year} = 0.615$ for the reference category `1` (American), whereas $\hat{\beta}_\text{year} = 0.615 + 0.519$ and $\hat{\beta}_\text{year} = 0.615 + 0.356$ for category `2` (European) and `3` (Japanese), respectively. This means that the fuel consumption is reduced faster by cars produced outside America (note that `mgp` is "miles per gallon", thus a higher value means that the car consumes less fuel, as it can drive further per gallon).

Note: For a full understanding of interaction terms, you really do need both the `summary()` and the `anova()` tables.


## h)

We try three different transformations and look at the residual plots:

```{r, fig.width = 5, fig.height = 4, fig.align = "center", out.width = '50%', cache = FALSE}
# try 3 predictor transformations

Auto$sqrtmpg <- sqrt(Auto$mpg)

fit.lm3 <- lm(sqrtmpg ~ displacement + weight + year + origin, data = Auto)
autoplot(fit.lm3, smooth.colour = NA)

fit.lm4 <- lm(mpg ~ displacement + I(log(weight)) + year + origin, data = Auto)
autoplot(fit.lm4, smooth.colour = NA)

fit.lm5 <- lm(mpg ~ displacement + I(weight^2) + year + origin, data = Auto)
autoplot(fit.lm5, smooth.colour = NA)
```



# Problem 2

## a)

\begin{align}
E(\hat{\boldsymbol\beta})&=E((X^T X)^{-1}X^T Y)=(X^TX)^{-1}X^T E(Y) =(X^TX)^{-1}X^T E(X \boldsymbol\beta +\varepsilon) \\
&=(X^TX)^{-1}X^T (X \boldsymbol\beta +0)=(X^TX)^{-1}(X^T X) \boldsymbol\beta = I \boldsymbol\beta = \boldsymbol\beta
\end{align}

\begin{align}
Cov(\hat{\boldsymbol\beta})&=Cov((X^T X)^{-1}X^T Y)=(X^TX)^{-1}X^T Cov(Y)((X^TX)^{-1}X^T)^T \\
&=(X^TX)^{-1}X^T \sigma^2  I ((X^TX)^{-1}X^T)^T\\
&=\sigma^2 (X^TX)^{-1} \\
\end{align}

We need to assume that $Y$ is multivariate normal. As $\hat{\boldsymbol\beta}$ is a linear transformation of a multivariate normal vector $Y$, $\hat{\boldsymbol\beta}$ is also multivariate normal. 

All components of a multivariate normal vector are themselves univariate normal. This means that $\hat{\beta}_j$ is normally distributed with expected value given by the $\beta_j$ and the variance given by the $j$'th diagonal element of $\sigma^2 (X^T X)^{-1}$.

## b) 

Fix covariates `X` and repeat the following procedure many times (`nsim = 1000`)

* collect $Y$

* create CI using $\hat{\beta}$ and $\hat{\sigma}$

95 % of the times the CI contains the true $\beta.$

```{r}
# CI for beta_j
beta0 <- 1
beta1 <- 3
true_beta <- c(beta0, beta1) # vector of model coefficients
true_sd <- 1 # choosing true sd
nobs <- 100
X <- runif(nobs, 0, 1)
Xmat <- model.matrix(~X, data = data.frame(X)) # Design Matrix

# Count how many times the true value is within the confidence interval
ci_int <- ci_x <- 0
nsim <- 1000
for (i in 1:nsim){
  y <- rnorm(n = nobs, mean = Xmat %*% true_beta, sd = rep(true_sd, nobs))
  mod <- lm(y ~ x, data = data.frame(y = y, x = X))
  ci <- confint(mod)
  ci_int[i] <- ifelse(true_beta[1] >= ci[1, 1] && true_beta[1] <= ci[1, 2], 1, 0)
  ci_x[i] <- ifelse(true_beta[2] >= ci[2, 1] && true_beta[2] <= ci[2, 2], 1, 0)
}

c(mean(ci_int), mean(ci_x))
```

## c) 

We apply the same idea from b), but now we fix X and $x_0$ and

* collect $Y$

* create PI using $\hat{\beta}$ and $\hat{\sigma}$

* simulate $Y_0$

95 % of the times the PI contains $Y_0.$

```{r}
# PI for Y_0
beta0 <- 1
beta1 <- 3
true_beta <- c(beta0, beta1) # vector of model coefficients
true_sd <- 1 # choosing true sd
X <- runif(100, 0, 1)
Xmat <- model.matrix(~X, data = data.frame(X)) # Design Matrix

x0 <- c(1, 0.4)

# simulating and fitting models many times
pi_y0 <- 0
nsim <- 1000
for (i in 1:nsim){
  y <- rnorm(n = 100, mean = Xmat %*% true_beta, sd = rep(true_sd, 100))
  mod <- lm(y ~ x, data = data.frame(y = y, x = X))
  y0 <- rnorm(n = 1, mean = x0 %*% true_beta, sd = true_sd)
  pi <- predict(mod, newdata = data.frame(x = x0[2]), interval = "predict")[2:3]
  pi_y0[i] <- ifelse(y0 >= pi[1] && y0 <= pi[2], 1, 0)
}

mean(pi_y0)
```

## d) 


### Confidence Interval for ${\boldsymbol x}_0^T\boldsymbol{\beta}$

This corresponds to a CI for $\mu_0 = E(y_0) = \boldsymbol{x}_0^T\boldsymbol{\beta}$ of an observation $y_0$ at $\boldsymbol{x}_0.$

First, by using a) we have that 

$E(\boldsymbol{x}_0^T\hat{\boldsymbol{\beta}}) = \boldsymbol{x}_0^T E(\hat{\boldsymbol{\beta}}) = \boldsymbol{x}_0^T \boldsymbol{\beta}$

and

$\operatorname{Var}(\boldsymbol{x}_0^T\hat{\boldsymbol{\beta}}) = \boldsymbol{x}_0^T \operatorname{Var}(\hat{\boldsymbol{\beta}}) \boldsymbol{x}_0 = \sigma^2\boldsymbol{x}_0^T(X^TX)^{-1} \boldsymbol{x}_0.$

Consequently, 

$$
\boldsymbol {x}_0^T\hat{\boldsymbol{\beta}} \sim N(\boldsymbol{x}_0^T\boldsymbol{\beta}, \sigma^2\boldsymbol{x}_0^T(X^TX)^{-1} \boldsymbol{x}_0)
$$
or equivalently

$$
\frac{\boldsymbol {x}_0^T\hat{\boldsymbol{\beta}}-\mu_0}{\sigma\sqrt{\boldsymbol{x}_0^T(X^TX)^{-1} \boldsymbol{x}_0}} \sim N(0, 1).
$$

By substituting $\sigma^2$ with an estimator $\hat{\sigma ^2},$ the last expression follows a $t-$distribution with $n-p$ degrees of freedom.
Now we can construct a confidence interval with $1-\alpha$ level (in our case $\alpha = 0.05$) as follows

$$
P\Big( -t_{n-p}(1-\alpha/2)\le 
\frac{\boldsymbol {x}_0^T\hat{\boldsymbol{\beta}}-\mu_0}{\hat{\sigma}\sqrt{\boldsymbol{x}_0^T(X^TX)^{-1} \boldsymbol{x}_0}}
\le t_{n-p}(1-\alpha/2) \Big)
= 1-\alpha
$$
or

$$
P\Big( \boldsymbol {x}_0^T\hat{\boldsymbol{\beta}}-t_{n-p}(1-\alpha/2)\hat{\sigma}\sqrt{\boldsymbol{x}_0^T(X^TX)^{-1} \boldsymbol{x}_0}\le 
\mu_0 
\le \boldsymbol {x}_0^T\hat{\boldsymbol{\beta}}+t_{n-p}(1-\alpha/2)\hat{\sigma}\sqrt{\boldsymbol{x}_0^T(X^TX)^{-1} \boldsymbol{x}_0} \Big)
= 1-\alpha
$$

For $\alpha = 0.05$ we have the following $95\%$ CI

$$
\Big[ \boldsymbol {x}_0^T\hat{\boldsymbol{\beta}}-t_{n-p}(1-0.05/2)\hat{\sigma}\sqrt{\boldsymbol{x}_0^T(X^TX)^{-1} \boldsymbol{x}_0}, 
\boldsymbol {x}_0^T\hat{\boldsymbol{\beta}}+t_{n-p}(1-0.05/2)\hat{\sigma}\sqrt{\boldsymbol{x}_0^T(X^TX)^{-1} \boldsymbol{x}_0} \Big]
$$

### Prediction interval 

To construct the PI we can look at the distribution of the prediction error $\hat{\varepsilon}_0 = y_0-\hat{y}_0 = y_0 - {\boldsymbol x}_0^T\boldsymbol{\beta}.$

We have that 
$$
E(\hat{\varepsilon}_0) =y_0 - E(\hat{y}_0) = y_0 - E(\boldsymbol{x}_0^T\hat{\boldsymbol{\beta}}) = y_0-y_0 = 0
$$ 

and by assuming that $y_0$ and $\hat{y_0}$ are independent and $y_0\sim N(\boldsymbol{x}_0^T \boldsymbol{\beta},\sigma^2)$
we have that 

$$
\operatorname{Var}(\hat{\varepsilon}_0) = \operatorname{Var}(y_0)  + \operatorname{Var}(\hat{y}_0) 
=  \sigma^2 + \sigma^2\boldsymbol{x}_0^T(X^TX)^{-1} \boldsymbol{x}_0
= \sigma^2(1+\boldsymbol{x}_0^T(X^TX)^{-1} \boldsymbol{x}_0)
$$

and 

$$
\hat{\varepsilon}_0 = y_0 - \boldsymbol{x}_0^T\hat{\boldsymbol{\beta}}\sim N(0, \sigma^2(1+\boldsymbol{x}_0^T(X^TX)^{-1} \boldsymbol{x}_0)).
$$

Following the same logic as for the before and by substituting  the estimate of variance $\hat{\sigma ^2}$ we can construct a prediction interval with $1-\alpha$ level as follows

$$
P\Big( -t_{n-p}(1-\alpha/2)\le 
\frac{y_0-\boldsymbol {x}_0^T\hat{\boldsymbol{\beta}}}{\hat{\sigma}\sqrt{1+\boldsymbol{x}_0^T(X^TX)^{-1} \boldsymbol{x}_0}}
\le t_{n-p}(1-\alpha/2) \Big)
= 1-\alpha
$$

or

$$
P\Big( \boldsymbol {x}_0^T\hat{\boldsymbol{\beta}}-t_{n-p}(1-\alpha/2)\hat{\sigma}\sqrt{1+\boldsymbol{x}_0^T(X^TX)^{-1} \boldsymbol{x}_0}\le 
y_0 
\le \boldsymbol {x}_0^T\hat{\boldsymbol{\beta}}+t_{n-p}(1-\alpha/2)\hat{\sigma}\sqrt{1+\boldsymbol{x}_0^T(X^TX)^{-1} \boldsymbol{x}_0} \Big)
= 1-\alpha
$$

For $\alpha = 0.05$ we have the following $95\%$ PI

$$
\Big[ \boldsymbol {x}_0^T\hat{\boldsymbol{\beta}}-t_{n-p}(1-0.05/2)\hat{\sigma}\sqrt{1+\boldsymbol{x}_0^T(X^TX)^{-1} \boldsymbol{x}_0}, 
\boldsymbol {x}_0^T\hat{\boldsymbol{\beta}}+t_{n-p}(1-0.05/2)\hat{\sigma}\sqrt{1+\boldsymbol{x}_0^T(X^TX)^{-1} \boldsymbol{x}_0} \Big]
$$

Observe now that the two intervals are similar, but the prediction interval is by construction wider. Specifically, it can be significantly wider in applications where the error variance $\hat{\sigma ^2}$ is large.

The connection between CI for $\boldsymbol{\beta}$, ${\boldsymbol x}_0^T\boldsymbol{\beta}$ and PI for $y$ at ${\boldsymbol x}_0$: The first is CI for a parameter, the second is CI for the expected regression line at the point $\boldsymbol{x}_0$ (when you only have one covariate, this may be more intuitive), and the last is the PI for the response $y_0$. The difference between the two latter is that $y$ are the observations, and ${\boldsymbol x}_0^T\boldsymbol{\beta}$ is the expected value of the observations and hence a function of the model parameters (NOT an observation).

## e)

We have a model on the form $Y=X \beta + \varepsilon$ where $\varepsilon$ is the error. The error of the model is unknown and unobserved, but we can estimate it by what we call the residuals. The residuals are given by the difference between the true response and the predicted value 
$$\hat{\varepsilon}=Y-\hat{Y}=(I-\underbrace{X(X^TX)^{-1}X^\top}_{=H})Y ,$$
where $H$ is called the "hat matrix".

Properties of raw residuals: Normally distributed with mean 0 and covariance $Cov(\hat{\varepsilon})=\sigma^2 (I-H).$ This means that the residuals may have different variance (depending on $X$) and may also be correlated.

In a model check, we want to check that our errors are independent, homoscedastic (same variance for each observation) and not dependent on the covariates. As we don't know the true error, we use the residuals as predictors, but as mentioned, the residuals may have different variances and may be correlated. This is why we don't want to use the raw residuals for model check.

To amend our problem we need to try to fix the residuals so that they at least have equal variances. We do that by working with standardized or studentized residuals. 

